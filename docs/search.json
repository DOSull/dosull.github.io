[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Note: some posts show code that has been superseded. Code ran as written at last modified dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat three letters\n\n\nWord games all the way to the ends of the earth\n\n\n\ngeospatial\n\n\nr\n\n\ntutorial\n\n\nnetworks\n\n\nggplot\n\n\nlife\n\n\n\nMaps of all the possible air routes where the origin and destination\nairport IATA codes would differ by only one letter, and of the ones that\nactually do. Also, grobs. \n\n\n\n\n\nJun 27, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA population based binary space partition\n\n\nOf course there’s a point, I’m just not sure what\n\n\n\ngeospatial\n\n\nr\n\n\ntutorial\n\n\naotearoa\n\n\npopulation\n\n\ncartography\n\n\n\nA process for building a hierarchical partition of a gridded population dataset\n\n\n\n\n\nJun 20, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nHow to cut a cake into four equal slices\n\n\nIf the cake is a country made of people\n\n\n\ngeospatial\n\n\nr\n\n\ntutorial\n\n\naotearoa\n\n\n\n\n\n\n\n\n\nJun 12, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nNorth-south or east-west islands?\n\n\nAnd why both are wrong\n\n\n\ngeospatial\n\n\nr\n\n\ntutorial\n\n\nstuff\n\n\naotearoa\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLook ma! (Almost) no javascript!\n\n\nI gave a talk at the local python meetup\n\n\n\ngeospatial\n\n\npython\n\n\nmaps\n\n\ntiling\n\n\ncartography\n\n\nweaving\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nMay 22, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nNine XKCD bad map projections: ranked!\n\n\nAlso: reverse-engineered\n\n\n\ngeospatial\n\n\nr\n\n\ntutorial\n\n\nstuff\n\n\ncartography\n\n\nxkcd\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMostly tiles, but also glyphs\n\n\nAnd tiles as glyphs\n\n\n\npython\n\n\ncartography\n\n\nvisualization\n\n\ntiling\n\n\naotearoa\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nImagine setting up a Jupyter hub\n\n\nIt’s easy if you try\n\n\n\npython\n\n\ntraining\n\n\naotearoa\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMapWeaver: tiled and woven multivariate maps without code\n\n\n\n\n\n\nmaps\n\n\ntmap\n\n\ntiling\n\n\nweaving\n\n\npython\n\n\n\nHow can I make this better? And: is this anything?!\n\n\n\n\n\nMar 9, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMarimo notebooks\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\n\nReactive notebooks in python!\n\n\n\n\n\nFeb 18, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nInto the (LiDAR) void\n\n\n\n\n\n\naotearoa\n\n\nmaps\n\n\ntmap\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nlife\n\n\n\nOr: who’s afraid of the Tararuas?\n\n\n\n\n\nJan 30, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nRandom points on the globe revisited\n\n\n\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nlife\n\n\n\nOf Christmas gifts and rabbit holes\n\n\n\n\n\nJan 14, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA welcome (re)emergence of systems thinking\n\n\n\n\n\n\ncomplexity\n\n\naotearoa\n\n\nconferences\n\n\nsystems\n\n\n\nAnd it’s not before time!\n\n\n\n\n\nDec 1, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nALGIM 2024\n\n\n\n\n\n\nlocal government\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\n\nOn geospatial in local government\n\n\n\n\n\nNov 22, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\ntmap vs. ggplot2 for mapping\n\n\n\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\ntmap\n\n\nggplot\n\n\n\nI think I prefer…\n\n\n\n\n\nNov 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of DuckDB\n\n\n\n\n\n\ngeospatial\n\n\nduckdb\n\n\nR\n\n\ntutorial\n\n\n\nInsert duck-related joke here\n\n\n\n\n\nOct 25, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nIn praise of GeoPackages\n\n\n\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\n\nShapefiles… who needs ’em?\n\n\n\n\n\nOct 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGiscience 2025\n\n\n\n\n\n\nconferences\n\n\ngeospatial\n\n\naotearoa\n\n\n\nCall for papers\n\n\n\n\n\nOct 10, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGeoCart’2024\n\n\n\n\n\n\ncartography\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\ntime-space\n\n\n\nReflections on the meeting\n\n\n\n\n\nAug 30, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nUseful utilities for NetLogo\n\n\n\n\n\n\nnetlogo\n\n\nsimulation\n\n\n\nFor when you are missing R/python\n\n\n\n\n\nAug 15, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nThe model zoo\n\n\n\n\n\n\nnetlogo\n\n\nbooks\n\n\nsimulation\n\n\n\nKeeping up with NetLogo\n\n\n\n\n\nAug 2, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments with R interpolators\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nMessing around with interpolating via a triangulation to apply coordinate transformations\n\n\n\n\n\nOct 21, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nSpiral origami\n\n\n\n\n\n\njavascript\n\n\norigami\n\n\nstuff\n\n\n\nThings to make and do\n\n\n\n\n\nAug 3, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nOK COVID, you win\n\n\n\n\n\n\nvisualization\n\n\ncovid\n\n\nR\n\n\nmaps\n\n\n\nThe end of the line\n\n\n\n\n\nMar 9, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLow level handling of sf objects\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\n\nGoddamnit, floating point!\n\n\n\n\n\nDec 8, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLocations of interest in 2021 delta outbreak\n\n\n\n\n\n\nmaps\n\n\nvisualization\n\n\ncovid\n\n\n\nWhen everything went pear-shaped\n\n\n\n\n\nOct 30, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nInverse distance weighted (IDW) interpolation using spatstat\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nSo many packages so little time\n\n\n\n\n\nOct 22, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nKernel density estimation in R spatial\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nHere’s one way to do kernel density estimation in R spatial\n\n\n\n\n\nOct 21, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nUniform random points on the globe\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nThe earth’s a sphere: who knew?\n\n\n\n\n\nOct 20, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the Dulux colours\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\nstuff\n\n\nmaps\n\n\naotearoa\n\n\n\nThe ultimate in categorical mapping\n\n\n\n\n\nSep 23, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nAffine transformations of sf objects\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nManipulating simple features in sf is sorta simple, sorta not…\n\n\n\n\n\nAug 12, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nWhat three chords\n\n\n\n\n\n\njavascript\n\n\ngeospatial\n\n\nstuff\n\n\n\nSeriously, WTC?!\n\n\n\n\n\nJul 3, 2020\n\n\nDavid O’Sullivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Most talks are in web formats, and may include links that are now broken. Others are PDFs. Others are missing, but are listed because they give a sense of development over time.\n\n\n\nDate\nTitle\nNotes\n\n\n\n\n2025June\nIt’s turtles all the way down: Simple models - complex outcomes\nCafé Complexité, Te Herenga Waka, 5 June.\n\n\n2025May\nLook ma! (Almost) no javascript!\nPython NZ Wellington Meetup, Sharesies Offices, Wellington, 15 May. Video\n\n\n2024August\nTime-space mapping in mountainous terrain\nGeoCart 2024, National Library, Wellington, 21-23 August\n\n\n2024April\nA spatially explicit agent-based model of on-farm environmental interventions\nAnnual Meeting of the American Association of Geographers, Honolulu, Hawai’i, United States\n\n\n2024March\n30 Day Map Challenge 2023\nMaptime! Wellington\n\n\n2023September\nFrom geographical information science via spatial data science to geographical computing\nFourth Spatial Data Science Symposium, University of Canterbury Hub, 7 September\n\n\n2023August\nComputing Geographically: Bridging Giscience and Geography\nUniversity of Canterbury, School of Earth and Environment, Research Seminar, 10 August\n\n\n2022November\nTiled & woven thematic maps\nRegional GIS Forum, Palmerston North, 11 November\n\n\n2022August\nComputing geographically: rethinking space and place in giscience\nKeynote at New Zealand Geospatial Research Colloquium, 29-30 August\n\n\n2022August\nTiled & woven thematic maps\nGeoCart 2022, National Library, Wellington, 24-26 August\n\n\n2022July\nR for geospatial\nSpatial Literacy User Group, Te Herenga Waka - Victoria University of Wellington\n\n\n2021November\nWeaving maps of multivariate data\nState of New Zealand Cartography, Special Meeting of the New Zealand Cartographic Society, Wellington\n\n\n2021September\nMapping the Dulux Colours of New Zealand Using R\nMaptime! Aotearoa, online\n\n\n2020November\nSpatially-explicit models for exploring COVID-19 lockdown strategies\nNew Zealand Geographical Society, Wellington\n\n\n2020November\nComputing geographically: rethinking giscience as geography\nUniversity of Utah, Department of Geography, Geography Awareness Week Colloquium\n\n\n2019September\nA spatial simulation model to explore the potential impact of gene drives as a control on invasive wasps\nGeocomputation 2019, Queenstown, 18-21 September\n\n\n2019August\nTheoretical geography: definitely harder than physics!\nVvoIP_Physics_Debates symposium\n\n\n2019July\nAvoiding the YAAWN syndrome\nAgents for Theory: From Cases to General Principles, Theory Development through Agent-based Modeling, International Workshop held at Herrenhäuser Palace, Hanover, Germany\n\n\n2018May\nSome translation required, or: A city is not a network either!\nInaugural Brian Coffey lecture and workshop in Geographical Information Science University of Washington, Tacoma\n\n\n2018April\nComputing with many spaces: Generalizing projections for the digital geohumanities and GIScience\n(with Luke Bergmann who presented) 114th Annual Meeting of the American Association of Geographers, New Orleans, LA\n\n\n2018March\nReimagining GIScience for relational spaces\nUniversity of Colorado Boulder, Department of Geography Colloquium Series\n\n\n2017December\nBridging GIScience and Geographical Thought\nGeographic Data Science Lab, University of Liverpool\n\n\n2017September\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nGraduate webinar series on Agent-based models, University of Minnesota. These are the same slides as a talk at Stanford a couple of years earlier…\n\n\n2017September\nSome translation required, or: A city is not a network either!\nInternational Symposium on The Future of Urban Network Research, University of Ghent, Belgium\n\n\n2017April\n‘Same only different’: rethinking the practice of digital urban geographies\n113th Meeting of the American Association of Geographers, Boston, MA\n\n\n2016November\nSimple spatial models: Building blocks for a process-based GIS?\nGeolunch Series, Geospatial Innovation Facility (GIF), University of California, Berkeley\n\n\n2016September\nSearching for common ground (again)\n(with Jim Thatcher and Luke Bergmann) Presented at 9th International Conference on Geographical Information Science (GIScience 2016), Montreal, Canada\n\n\n2016September\nSimple simulation models as a complexity ‘pattern language’\nLightning talk at Rethinking the ABCs: Agent-Based Models and Complexity Science in the age of Big Data, CyberGIS, and Sensor Networks pre-conference workshop at GIScience 2016, Montreal, Canada\n\n\n2016March\nSpatiality, maps, mathematics and critical human geography\nUniversity of Uppsala, Department of Social and Economic Geography\n\n\n2016January\nFuture GIS\n(with Matt Wilson) University of British Columbia, Department of Geography\n\n\n2015December\nThinking with and about models in geography\nUniversity of California, Davis. Geography Graduate Group seminar series.\n\n\n2015August\n(with Alex Singleton and Seth Spielman) Our town: How socioeconomics shape functional neighborhoods in American cities\nGeocomputation 2015, UT Dallas\n\n\n2015May\nSpatial simulation: Exploring pattern and process\nUNIGIS Salzburg Webinar\n\n\n2015May\nSimple spatial models: Building blocks for process-based GIS?\nStanford University\n\n\n2015May\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nStanford University Libraries’ Center for Interdisciplinary Digital Research\n\n\n2015April\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\n(with George Perry) presented at the 110th Annual Meeting of the Association of American Geographers, Chicago, IL\n\n\n2014November\n‘Play well’: Learning about the world using spatial models\nUniversity of Oregon, Department of Geography and Complexity Science conference\n\n\n2014September\nUsing Personal Names to Explore Cultural, Ethnic and Linguistic Structure in Populations\nUC Berkeley, Department of Demography\n\n\n2014June\nSimple spatial models: building blocks for process-based GIS?\nInstitute of Australian Geographers – New Zealand Geographical Society Joint Conference, University of Melbourne\n\n\n2013August\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at SIRC NZ 2013, University of Otago, Dunedin, New Zealand. Extended abstract available here\n\n\n2013April\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at the 109th Annual Meeting of the Association of American Geographers, Los Angeles, CA\n\n\n2013February\nTowards a ‘pattern language’ for spatial simulation models\nDepartment of Geography, University of California Santa Barbara\n\n\n2012May\nNaming networks and population structure\nDepartment of Urban Engineering, University of Tokyo\n\n\n2012May\nSpatial Simulation: Exploring Pattern and Process – A Work in progress\nCentre for Spatial Information Science, University of Tokyo\n\n\n2012February\nAgent-based models: what are they good for? Or: did Schelling really need an ABM?\nPresented at the 108th Annual Meeting of the Association of American Geographers, New York, NY\n\n\n2011September\nModel Histories: The Generative Properties of Agent-Based Modelling\nPresented by James Millington (also with George Perry) at Annual Conference of the Royal Geographical Society with the Institute of British Geographers, Royal Geographical Society, London\n\n\n2011April\nNaming networks and population structure\n(with Pablo Mateos) presented at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA\n\n\n2011April\nDo physicists have geography envy?\nwith Steve Manson (who presented) at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA. This paper was eventually published in much different form in the Annals of the American Association of Geographers.\n\n\n2009December\nSimulating long distance dispersal processes in spatially heterogeneous landscapes\n(with George Perry) presented at Geocomputation 2009, University of New South Wales, Sydney, Australia. A version of this work was published in Ecological Informatics."
  },
  {
    "objectID": "portfolio/04-antarctica.html",
    "href": "portfolio/04-antarctica.html",
    "title": "Science impacts in Antarctica",
    "section": "",
    "text": "This work aims to understand the potential for damaging environmental impacts in Antarctica. You can follow the work as it develops at this website.\nThe approach I’m taking attempts to estimate ‘desire lines’ for human movement. Here’s a typical output:\n\nThis work is funded through Manaaki Whenua Landcare Research and Te Pūnaha Matatini."
  },
  {
    "objectID": "portfolio/01-1-wasp-control-model.html",
    "href": "portfolio/01-1-wasp-control-model.html",
    "title": "A model for gene drive control of wasps",
    "section": "",
    "text": "Over a number of years, I developed a model of the potential for gene-drives as a control on invasive wasp populations in New Zealand. This included a two year pause for COVID when the original paper was disappointingly rejected after revisions.\nAnyway, the model looks like this:\n\nThe general conclusion is that while the gene drive mechanism we explored would substantially reduce wasp populations this outcome would come with substantial downsides:\n\nIt would take years. This would make it highly likely that the modified genotype would make it back to the wasps home range in Eurasia, where while annoying (they are wasps after all!) they are an important part of ecosystems.\nIf control was discontinued before complete eradication, the wild population would rapidly recover, making accurate monitoring of the wild-GM population mix essential.\nEradication could only be achieved by releasing truly vast numbers of genetically modified wasps—many more than there are wild wasps out there! This result is summarised in the figure below\n\n\nwhere only very high intensity releases of GM wasps over many years are successful in eradicating wild wasps. This is Figure 5 from the paper explaining the inner workings and conclusions from the model:\n\nLester PJ, D O’Sullivan and GLW Perry. 2023. Gene drives for invasive wasp control: Extinction is unlikely, with suppression dependent on dispersal and growth rates. Ecological Applications 33(7) e2912. doi: 10.1002/eap.2912\n\nI presented an early version of the model at Geocomputation 2019 which is fondly remembered as the last conference before COVID reared its ugly head.\nAll the materials including the model are available here."
  },
  {
    "objectID": "portfolio/03-time-space-mapping.html",
    "href": "portfolio/03-time-space-mapping.html",
    "title": "Time-space mapping",
    "section": "",
    "text": "Another ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThis is one aspect of a wider project aimed at developing maps where the geometry is based not on ‘location’ but on the relationships — whether of time, money, people, or anything else of interest between places. Space conceived not as a fixed absolute background to things but as a living moving thing that both shapes and is shaped by unfolding events.\nTime-space mapping is a particular subset of this wider project focused on relations of estimated travel time between locations in a mountainous environment. There’s more detail on what we’re up to in this presentation I gave at GeoCart 2024.\nYou’ll find a video of dynamic time-space in the presentation and there’s another in my diary entry about GeoCart. Meanwhile, here’s a series of maps of estimated hiking times from a given starting location on a series of idealised conical mountains of increasing steepness."
  },
  {
    "objectID": "portfolio/00-weaving-and-tiling-maps.html",
    "href": "portfolio/00-weaving-and-tiling-maps.html",
    "title": "Tiled and woven maps",
    "section": "",
    "text": "This is an ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThe idea is to map many different variables by combining them by ‘weaving’ or ‘tiling’. Here’s an example\n\nDifferent strands in the woven pattern represent different socioeconomic (or other) variables in the mapped area, and are coloured in the usual ‘choroplethic’ manner. We’re still figuring out the details, but some of the resulting maps are quite striking. Here’s a tiled (not woven) one\n\nExplanations of exactly what the heck is going on here are at the project repo, where you’ll also find links to presentations, example notebooks, and the code to make maps like these yourself!"
  },
  {
    "objectID": "portfolio/02-moving-the-middle.html",
    "href": "portfolio/02-moving-the-middle.html",
    "title": "Agent modeling of land management",
    "section": "",
    "text": "This is part of a large MBIE funded project ‘Moving the middle’.\nThe part of the research I am most involved in is developing an agent-based model of how land managers (mostly farmers) make changes in landuse and farm management practices. Other parts of the wider project are examining the motivations and drivers of farmer behaviour at an individual level, whereas the model explores how individual actions might scale up across landscapes.\nSlides from a presentation I gave about the model can be viewed here.\nAnd here is a more recent screenshot of the model in action."
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "I offer training in any of the areas listed below. Contact me for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing spatial data science\n\n\nA suite of short courses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial analysis and modelling\n\n\nGeographic information analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeographical computing\n\n\nPython programming for geospatial\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "my-skills.html",
    "href": "my-skills.html",
    "title": "My skills",
    "section": "",
    "text": "For more information or advice on any of these contact me.\n\nWriting\nI have written 3 books and numerous articles, reviews, and book chapters.\n\n\nPresenting\nI have taught for over 20 years and can put together a mean slide show (I like to think…). These days, I use web-compatible formats, primarily revealjs or quarto. See my list of talks, or even the slides from a class for examples.\n\n\nAnalytics\nI work all the time in R and Python developing workflows to tidy, reformat, analyse, and visualize data, especially spatial data.\n\n\nVisualization\nI am a trained cartographer. I haven’t done a lot of publication cartography per se, but have regularly worked on visual analytics or maps of complex data in research projects.\n\n\nCoding\nI have been programming for as long a I can remember. The more or less full list of languages, with those I have taught in bold would be: (Apple) BASIC, PASCAL, Fortran, 6502 assembler, C/C++ (a little), Java (Masters and PhD work), NetLogo, Visual Basic (yeuch!), Python, R, and JavaScript (also HTML and CSS if you can call those programming languages).\nThese days, I use python and R all the time.\nI am not a software developer, but can write well organised and documented code.\nI love NetLogo, but freely admit that it is somewhat niche (surprisingly powerful though, see this geographical COVID model I built in lockdown).\nI have other languages in me: Julia and Rust seem particularly interesting, but I may wait for more mature geospatial stacks to emerge before tackling them.\n\n\nSimulation modelling\nSee above re NetLogo. I’ve also written a book about this.\n\n\nWeb\nMost of my web stuff is linked from these pages, so just have a look around. The most complicated web-thing I’ve built is probably this New Zealand 2018 commute visualization. I am not a web developer, but am happy to roll up my sleeves and figure stuff out if called on!\n\n\nProject management\nBefore academia I was a project engineer leading build, test, commissioning and maintenance of production line laser-scanning inspection equipment, working with major multinationals such as IBM, Kodak, Dow Chemical, and Sony. In that role I learned a great deal about project and people management."
  },
  {
    "objectID": "training/01-spatial-analysis.html",
    "href": "training/01-spatial-analysis.html",
    "title": "Spatial analysis and modelling",
    "section": "",
    "text": "Go to the materials\nThese materials outline a one semester (36 contact hours) class in spatial analysis and modelling that was last taught at Victoria University of Wellington as GISC 422 in the second half of 2023. The materials cover many of the topics introduced in my book Geographic Information Analysis a recognised classic text in the field.\nI am still in the process of cleaning the materials up for conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to spatial analysis in R but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "training/00-spatial-data-science.html",
    "href": "training/00-spatial-data-science.html",
    "title": "Introducing spatial data science",
    "section": "",
    "text": "View the slides\nThese materials aim to help GIS users (even those with limited experience) transition to a more data science-centric approach. There is a particular emphasis on open source tools and on doing geospatial analysis in code using R as both are becoming increasingly important across the industry.\nThe materials are organised by theme and short courses to tailored to any or a selection of the themes can easily be arranged.\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "title": "Uniform random points on the globe",
    "section": "",
    "text": "There isn’t as much land near the poles, so how do you make uniform randomly distributed points in lat-lng coordinate space. Here’s how!\nNeeded libraries are the usual suspects plus rnaturalearth for basemap data\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\nn &lt;- 2500\nThe key thing to realise here is that random uniform numbers in both latitude and longitude will not be evenly distributed on Earth’s surface, because the meridians converge toward the poles. We can make two datasets to show this. First a naive set of randomly located points:\npts_naive &lt;- data.frame(lon = runif(n) * 360 - 180,\n                        lat = runif(n) * 180 - 90,\n                        type = \"naive\")"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "title": "Uniform random points on the globe",
    "section": "Here comes the science…",
    "text": "Here comes the science…\nAnd now a set where inserting a cosine correction ensures that the distribution of latitudes is appropriately more dense close to the equator:\n\npts_even &lt;- data.frame(lon = runif(n) * 360 - 180,\n                       lat = acos(runif(n) * 2 - 1) * 180 / pi - 90,\n                       type = \"even\")\n\n\nCompare the latitude distributions\nWe can make up a combined data table and directly compare the distribution of the latitudes with a nice density plot. The increased representation of points in the mid-latitudes with the cosine correction is clear.\n\npts &lt;- bind_rows(pts_naive, pts_even)\nggplot(pts) +\n  geom_density(aes(y = lat, fill = type), alpha = 0.5, lwd = 0) +\n  scale_fill_viridis_d()"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "title": "Uniform random points on the globe",
    "section": "Make a map",
    "text": "Make a map\nUse an equal-area projection to clearly see the problem geographically.\n\nw &lt;- ne_countries(returnclass = \"sf\") |&gt;\n  st_transform(\"+proj=hammer\")\n\npts_sf &lt;- pts |&gt;\n  st_as_sf(coords = 1:2, crs = 4326)\n\nggplot(w) + \n  geom_sf(fill = \"#cccccc\", colour = \"white\", lwd = 0.35) +\n  geom_sf(data = pts_sf, aes(colour = type), alpha = 0.35) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe naively distributed points are clearly denser at the poles than they should be, where the cosine term in the ‘even’ points generation method makes them evenly distributed over the globe."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html",
    "href": "posts/2025-06-13-population-quadrants/index.html",
    "title": "How to cut a cake into four equal slices",
    "section": "",
    "text": "This journey into the heart of darkness post was inspired by a LinkedIn post from Alasdair Rae linking to a map he made cutting the UK into four equal population slices. The post led to a surprising amount of complicated mathematical discussion on math reddit1 concerning whether such a dissection should always be possible, under what conditions, and if the dissection could be extended to more slices. I think the conclusion there is that if the surface (considering the population map as a surface) is everywhere continuous—roughly speaking it varies smoothly from place to place—then such a dissection should always be possible.\nThis conclusion surprised me a little.2 Here’s my line of thinking: I can see that there should always be a line in any given direction that halves the population. Draw a line at one side of the map area. Slide it across the map area to the other side. At the start all of the population is on one side of the line and at the end it is all on the other side of the line. If the distribution is continuous, then by the intermediate value theorem(IVT) from calculus, the line must at some point on its journey have been at a point where it cut the population exactly in half. So, pick a direction, find the line in that direction that bisects the population. The problem is that there is no guarantee that the bisector at 90° in combination with the first one will divide the population into equal quarters. It might if you ‘get lucky’, but there’s no prior reason why it would.\nAt this point, the mathematicians redditors invoke IVT and argue that you can choose the second line at an angle so that you do get equal quarters and then slide the crossover point and angle around until you find the place and orientation where the two lines do quadrisect3 the population.\nAnyway, that’s all by way of background. Let’s see if we can find that uh… sweet spot for Aotearoa New Zealand!\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n1library(scales)\n2library(patchwork)\n3library(spatstat)\n\n\n\n1\n\nFor nicer numeric labels on some axes.\n\n2\n\nFor nice multipanel plots.\n\n3\n\nFor a weighted median function."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#data",
    "href": "posts/2025-06-13-population-quadrants/index.html#data",
    "title": "How to cut a cake into four equal slices",
    "section": "Data",
    "text": "Data\nI used gridded population data for New Zealand at 250m resolution. The exact dataset I used was the early release data sitting around on my hard drive from when I made this map. You can get current releases of the same data here. I converted the gridded data to an x, y, population CSV file because that’s all I need for the process I followed. I also adjusted the coordinates by centring them on their mean centre. This is to keep the coordinate values in check when we start rotating points around an origin.\n\n\nCode\npop_grid &lt;- read.csv(\"nz-pop-grid-250m.csv\")\ncx &lt;- mean(pop_grid$x)\ncy &lt;- mean(pop_grid$y)\npop_grid &lt;- pop_grid |&gt;\n  mutate(x = x - cx, y = y - cy)\n\nnz &lt;- st_read(\"nz.gpkg\") |&gt;\n  mutate(geom = geom - c(cx, cy))\n\n\nAnd here is a basemap to get everyone oriented.\n\n\nCode\nbasemap &lt;- ggplot(nz) + \n  geom_sf(lwd = 0) +\n  geom_tile(data = pop_grid, \n            aes(x = x, y = y, fill = pop_250m_grid)) +\n  scale_fill_distiller(palette = \"Reds\", direction = -1) +\n  theme_void() +\n  theme(legend.position.inside = c(0.2, 0.7),\n        legend.position = \"inside\")\nbasemap\n\n\n\n\n\n\n\n\nFigure 1: Base map showing distribution of Aotearoa New Zealand population.\n\n\n\n\n\nThe Chatham Islands are included here, although, since this problem is all about medians not means, it’s unlikely that outlier population makes much difference to these calculations. In any case, to a first order approximation everyone is in Auckland—that concentration of population at the narrowest point of the northern isthmus of Te Ika-a-Māui.4 Not really, but Auckland does loom large in what follows, as it does in cartograms of New Zealand population."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#a-concept-of-a-plan-of-an-algorithm",
    "href": "posts/2025-06-13-population-quadrants/index.html#a-concept-of-a-plan-of-an-algorithm",
    "title": "How to cut a cake into four equal slices",
    "section": "A concept of a plan of an algorithm",
    "text": "A concept of a plan of an algorithm\nI briefly contemplated taking an approach similar to that outlined in the mathematical background above, but decided against it. Partly this was because building it would involve (broadly) two steps:\n\nA function to find a bisector;\nGiven a bisector, a function to find another bisector at 90°, that, given the first one, quadrisects the population. This would likely involve a complicated iterative procedure sliding and rotating the pair of bisectors around until the sweet spot is found.\n\nMy short-circuiting brain kicked in after item (1) to say, “well, if you’ve found a bisector, just find all the bisectors5 and then test to see if any pair at 90° meet the criteria.” This would in no sense be a complete or perfect solution, but it would at least start to give some sense of how sensitive to a precise choice of locations and angles the problem is.6 Also, it would avoid having to deal with (2) which sounds, well… tricky.\nSo the plan now was\n\nA function to find a bisector; and\nMake a bunch of bisectors and test to see if any pairs at 90° are quadrisectors.7\n\nThis is kind of an accept-reject algorithm. It’s obviously inefficient, since we generate many incorrect candidate solutions, but if I have a reasonably quick way to find bisectors the cost of those incorrect solutions shouldn’t be too high.\nConcept of a plan in place, let’s go!"
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#a-population-bisector-function",
    "href": "posts/2025-06-13-population-quadrants/index.html#a-population-bisector-function",
    "title": "How to cut a cake into four equal slices",
    "section": "A population bisector function",
    "text": "A population bisector function\nHere comes the science…\nI could search for the bisector at a given angle, by sliding a line across a map and moving it forward or backwards depending on the population on either side of it, until I found the desired spot. That would certainly be computationally satisfying, but it’s possible to derive the line directly using a weighted median function. The weighted median of a set of numbers that have an associated weight, is the value at which the total weight associated with numbers above and below the median is equal. spatstat provides a convenient implementation, so we don’t need to do that part.\nWe know the slope of the desired straight line. So we rotate our x, y coordinates such that the bisector would lie flat along the horizontal axis and determine the weighted median in the vertical direction. This is the offset from the origin of the bisector perpendicular to its direction of slope. Some mathematical manipulation based on the standard form equation of a straight line \\(Ax+By+C=0\\) given that we know the line’s slope, and its \\(x\\)- and \\(y\\)-intercepts (based on the calculated perpendicular offset) allows us to get the \\(A\\), \\(B\\), and \\(C\\) coefficients we need. If the desired angle of the bisector is either horizontal or vertical no rotation is needed and we can handle these special cases easily.\nSo here’s code for all that, along with a couple of convenience functions to return the slope and \\(y\\) intercept of the straight line.\n\n\nCode\nrotation_matrix &lt;- function(a) {\n  matrix(c(cos(a), -sin(a), sin(a), cos(a)), 2, 2, byrow = TRUE)\n}\n\nbisector &lt;- function(pts, angle) {\n  if (angle == 0) {\n    median_y &lt;- weighted.median(pts$y, pts$pop_250m_grid)\n    A &lt;- 0; B &lt;- 1\n1    C &lt;- -weighted.median(pts$y, pts$pop_250m_grid)\n  } else if (angle == 90) {\n    A &lt;- 1; B &lt;- 0\n2    C &lt;- -weighted.median(pts$x, pts$pop_250m_grid)\n  } else {\n    a &lt;- angle * pi / 180\n3    pts_r &lt;- rotation_matrix(-a) %*%\n             matrix(c(pts$x, pts$y), nrow = 2, byrow = TRUE)\n    median_y &lt;- weighted.median(pts_r[2, ], pts$pop_250m_grid)\n4    A &lt;-  median_y / cos(a)\n    B &lt;- -median_y / sin(a)\n    C &lt;- -A * B\n  }\n  list(A = A, B = B, C = C)\n}\n\nget_slope &lt;- function(straight_line) {\n  -straight_line$A / straight_line$B\n}\n\nget_intercept &lt;- function(straight_line) {\n  -straight_line$C / straight_line$B\n}\n\n5get_ggline &lt;- function(straight_line, ...) {\n6  if (straight_line$B == 0) {\n    geom_vline(aes(xintercept = -straight_line$C / straight_line$A), ...)\n  } else {\n    geom_abline(aes(intercept = get_intercept(straight_line), \n                    slope = get_slope(straight_line)), ...)\n  }\n}\n\n\n\n1\n\nFor the horizontal bisector we want the weighted median of the y coordinates.\n\n2\n\nAnd for the vertical bisector, the weighted median of the x coordinates.\n\n3\n\nRotate coordinates by the negative of the line’s slope angle so we can find the weighted median perpendicular to the line.\n\n4\n\nYou’ll have to trust me on the calculation for A, B, and C, but keep in mind that since \\(Ax+By+C=0\\), the gradient of the line is given by \\(-A/B\\) and by inspection that’s true of these calculations, so that part checks out!\n\n5\n\n... means we can pass additional plot options in from the calling code.\n\n6\n\nWhen B == 0 the line is vertical so we need a geom_vline\n\n\n\n\nFor a given bisector we can determine which cells are either side of it by checking if \\(Ax+By+C\\) is positive or negative. Given that result, we can also easily determine the population either side of such a line.\n\n\nCode\n1get_cells_one_side_of_line &lt;- function(pts, sl, above = TRUE) {\n  if (above) {\n    pts |&gt; mutate(chosen = sl$A * x + sl$B * y + sl$C &gt; 0) |&gt;\n      pull(chosen)\n  } else {\n    pts |&gt; mutate(chosen = sl$A * x + sl$B * y + sl$C &lt;= 0) |&gt;\n      pull(chosen)\n  }\n}\n\nget_pop_one_side_of_line &lt;- function(pts, sl, above = TRUE) {\n  sum(pts$pop_250m_grid[get_cells_one_side_of_line(pts, sl, above)])\n}\n\n\n\n1\n\nget_cells_one_side_of_line reports a logical TRUE/FALSE vector for which cells are on the requested side of the line. We do it this way to make it easier to combine the intersection of lines to determine quadrant populations later.\n\n\n\n\n\nSanity check\nOK. So let’s see if all that has worked by running an example for a line at 30°\n\n\nCode\nx30 &lt;- bisector(pop_grid, 30)\nbasemap + get_ggline(x30, linetype = \"dashed\", lwd = 0.5) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nFigure 2: A map to sanity check the methods developed so far.\n\n\n\n\n\nSo we get a line at 30° as required. What are the populations either side of it?\n\n\nCode\nc(get_pop_one_side_of_line(pop_grid, x30),\n  get_pop_one_side_of_line(pop_grid, x30, FALSE))\n\n\n[1] 2542977 2543049\n\n\nNot precisely equal, but pretty close. And of course, because we are not subdividing population if the line passes through a raster cell, but assigning all of the population of a cell to whichever side of the line its centre falls in, we wouldn’t expect an exact answer."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#but-i-wanted-quadrisection",
    "href": "posts/2025-06-13-population-quadrants/index.html#but-i-wanted-quadrisection",
    "title": "How to cut a cake into four equal slices",
    "section": "But I wanted quadrisection",
    "text": "But I wanted quadrisection\nIt has just occurred to me that they called this quartering in medieval times, as in “hung, drawn and quartered”… but let’s not dwell on that, and move on.\nTo get quadrant populations we use two lines and get the various combinations of population above/below each.\n\n\nCode\nget_quadrant_pops &lt;- function(pts, sl1, sl2) {\n1  above1 &lt;- get_cells_one_side_of_line(pts, sl1)\n  above2 &lt;- get_cells_one_side_of_line(pts, sl2)\n2  c(sum(pts$pop_250m_grid[ above1 &  above2]),\n    sum(pts$pop_250m_grid[ above1 & !above2]),\n    sum(pts$pop_250m_grid[!above1 & !above2]),\n    sum(pts$pop_250m_grid[!above1 &  above2]))\n}\n\n\n\n1\n\nGet logical vectors of which cells are above the two lines.\n\n2\n\nAnd then we can combine them to get all the pairwise combinations above/below the lines and calculate the populations.\n\n\n\n\nA function to plot the quadrant populations at a range of angles is convenient. Details in the cell below if you are interested.\n\n\nCode\nplot_range &lt;- function(angles) {\n  bisectors      = lapply(angles     , bisector, pts = pop_grid)\n  perp_bisectors = lapply(angles + 90, bisector, pts = pop_grid)\n  df &lt;- data.frame(angle = angles)\n1  df[, c(\"pop1\", \"pop2\", \"pop3\", \"pop4\")] &lt;-\n        mapply(get_quadrant_pops, bisectors, perp_bisectors,\n               MoreArgs = list(pts = pop_grid)) |&gt; t()\n\n  ggplot(df |&gt; select(angle, pop1:pop4) |&gt; \n               pivot_longer(cols = pop1:pop4)) +\n    geom_line(aes(x = angle, y = value, group = name), \n              lwd = 0.2, alpha = 0.35) +\n    geom_point(aes(x = angle, y = value, colour = name), \n               size = 0.5) +\n    scale_colour_brewer(palette = \"Set1\", name = \"Quadrant\") +\n    ylab(\"Estimated population\") +\n    scale_x_continuous(breaks = angles[seq(1, length(angles), 10)],\n                       minor_breaks = angles) +\n    scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n    theme_minimal()\n}\n\n\n\n1\n\nAssigning the four numbers produced by get_quadrant_pops in this way seems to be the fastest approach, and preferable to a for loop. If you are writing for loops in R or Python, you are losing.\n\n\n\n\nAnd now we can plot the quadrant populations across a range of angles from 0 to 90°.\n\n\nCode\ng1 &lt;- plot_range(-1:90) \ng1 + annotate(\"rect\", xmin = c(-1, 76.5, 79), \n                      xmax = c(1, 78.5, 81), \n                      ymin = -Inf, ymax = Inf, \n              fill = \"goldenrod\", alpha = 0.3, lwd = 0)\n\n\n\n\n\n\n\n\nFigure 3: Variation in quadrant populations as the orientation of the first bisector is varied.\n\n\n\n\n\nAs expected these are actually two pairs of equal sums given that the two lines we are using are bisectors, so we expect diagonally opposite quadrants to have equal population, but not necessarily all four to be equal. There are three highlighted orientations where all four populations seem to converge, at around 0°, 77°, and 80°.\n\n\nCode\ng2 &lt;- plot_range(seq(-0.5,  0.5, 0.025)) + guides(colour = \"none\")\ng3 &lt;- plot_range(seq(  77,   78, 0.025)) + guides(colour = \"none\")\ng4 &lt;- plot_range(seq(79.5, 80.5, 0.025)) + guides(colour = \"none\")\n(g2 + g3 + plot_layout(axes = \"collect\")) / g4\n\n\n\n\n\n\n\n\nFigure 4: A closer look at the quadrant populations at angles where they appear equal.\n\n\n\n\n\nProbably for a continuous, infinitely divisible population surface these would all be solutions to our quartering problem, but if we have a closer look (below) we can see that only the solution close to 80° actually converges for our discretised data. The other two cases have (I assume) some population grid cells jumping back and forward across our lines as we shift them slightly. On a continuous population surface I assume the behaviour at these angles would be more like the smooth variation we see in the 80° case.8"
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#so-finally",
    "href": "posts/2025-06-13-population-quadrants/index.html#so-finally",
    "title": "How to cut a cake into four equal slices",
    "section": "So finally…",
    "text": "So finally…\nThe zoomed in view around 80° suggests that bisectors at angles of 79.975° and 169.975° are pretty much the ‘answer’.\nHere’s a map, and the estimated populations in each quadrant, which, given the discretisation of our population data are very close to equal.\n\n\nCode\nxs &lt;- c(79.975, 169.975) |&gt; lapply(bisector, pts = pop_grid)\nx_df &lt;- data.frame(\n  slope     = xs |&gt; sapply(get_slope), \n  intercept = xs |&gt; sapply(get_intercept))\n\nbasemap + \n  geom_abline(data = x_df, \n              aes(slope = slope, intercept = intercept),\n              linetype = \"dashed\", lwd = 0.5) +\n  guides(fill = \"none\")\n\nget_quadrant_pops(pop_grid, xs[[1]], xs[[2]])\n\n\n[1] 1271354 1271442 1271630 1271600\n\n\n\n\n\n\n\n\nFigure 5: The best fit equal population quadrants.\n\n\n\n\n\nIt’s unsurprising as I hinted at the start, that one of the bisectors passes through Auckland, with over a third of the country’s population living there. Going anti-clockwise, starting from the northeast quadrant, these four ‘regions’ might be called Part of Auckland and Waikato-Bay of Plenty, Part of Auckland and Northland, Most of Te Waipounamu and Taranaki, and What’s Left of Te Waipounamu and Te Ika-a-Māui."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#final-thoughts",
    "href": "posts/2025-06-13-population-quadrants/index.html#final-thoughts",
    "title": "How to cut a cake into four equal slices",
    "section": "Final thoughts",
    "text": "Final thoughts\nWorking on this problem was instructive.\nPerhaps most telling is how the conclusion that a pure mathematical approach leads to—that there is always a precise solution— depends heavily on an assumption that’s difficult to realise in practice. There’s no such thing as a continuous distribution of population, so the idealised mathematics only plays out approximately in practice. Something to keep in mind in many problem settings. Even so, the mathematics is useful: it tells us that there should be a solution even if that seems unlikely given real world data.\nTo find that ‘precise’ solution, we’d have to do the dissections in a way that yielded continuously varying estimates of the populations either side of a line as it is infinitesimally shifted around. For example, we could use an areal interpolation method, assigning the proportion of population in a grid cell based on the fraction of its area on each side of the line. This seems likely to be quite a lot slower than my admittedly approximate approach.\nA further thought is that lurking in here are almost certainly the ingredients for a treemap/quadtree approach to cartograms. This is not an entirely new idea. For example in this paper\n\nSlingsby A, J Wood, and J Dykes. 2010. Treemap Cartography for showing Spatial and Temporal Traffic Patterns. Journal of Maps 6(1) 135-146. doi: 10.4113/jom.2010.1071\n\nspatialised treemaps are applied to visualize complex data.\nI am envisaging bisecting horizontally, then bisecting each half vertically, then each quarter horizontally, and so on. The resulting rectangular regions would be of equal population, and could be rescaled to be of equal size after each cut. Something like that. The idea definitely needs work, but I think it has potential.\nBut that’s another puzzle for another time."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#footnotes",
    "href": "posts/2025-06-13-population-quadrants/index.html#footnotes",
    "title": "How to cut a cake into four equal slices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, that’s a thing.↩︎\nNotwithstanding the counterintuitive wonders of the ham sandwich theorem invoked by many of the math redditors.↩︎\nYes, it’s a word, I checked.↩︎\nBack in the day, this made a lot of sense as an overland shortcut between a harbour on the Pacific and another on the Tasman. Now as a place to concentrate your population it’s not so obviously a good choice.↩︎\nAt some angular resolution.↩︎\nI’d call it an agile approach if I were being kind. A little more seriously, I’d call it getting a feel for the problem before bothering to tackle it in full.↩︎\nI’m less certain this is a word.↩︎\nProbably… I can’t be absolutely certain of this. We’d have to look more closely to investigate which grid cells are ‘jumping around’ at these values to confirm this assumption.↩︎"
  },
  {
    "objectID": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "href": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "title": "Spiral origami",
    "section": "",
    "text": "I made an observable notebook to generate crease patterns for the flat-foldable origami ‘whirpool spirals’ designed by Tomoko Fuse and presented in chapter 3 of her amazing book Spiral: Origami|Art|Design. You’ll find more details in the notebook.\n\nThe picture is an example of the folds it produces, or at any rate of the things you can fold by cutting out and following the crease patterns it produces. You’ll have to do the folding yourself, I’m afraid.\nIf you’d like an idea of how the fold works without going to all the trouble of actually making one, then visit Amanda Ghassaei’s Origami Simulator and navigate to Examples - Tessellations - Whirlpool Spiral."
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "title": "Affine transformations of sf objects",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "title": "Affine transformations of sf objects",
    "section": "Packages",
    "text": "Packages\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(wk)\n\nsf::sf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "title": "Affine transformations of sf objects",
    "section": "A simple square",
    "text": "A simple square\nJust to get things set up let’s make a simple square.\n\nsquare &lt;- (st_polygon(list(matrix(c(-1, -1, 1, -1, 1, 1, -1, 1, -1, -1), \n                                 5, 2, byrow = TRUE))) * 0.5 + c(1, 0)) |&gt;\n  st_sfc()\n\ntm_shape(square) + \n  tm_borders(col = \"red\") + \n  tm_grid()"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "title": "Affine transformations of sf objects",
    "section": "Simple transformations",
    "text": "Simple transformations\nIn the code above, we made a polygon and multipled it by 0.5, then added c(1,0) to it. This had the effect of scaling it by 0.5 andthen translating it by the vector \\[\\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\]\nThese unlikely looking operations are perfectly valid, although they feel a bit ‘off’.\nEven more unlikely is that you can multiply an sf object by a matrix…\n\nang &lt;- pi / 6\nmat &lt;- matrix(c(cos(ang), -sin(ang), \n                sin(ang),  cos(ang)), 2, 2, byrow = TRUE)\n(square * mat) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThis is very handy… but probably also a bad idea! Because you have to post-multiply by the matrix, the sense of many affine transformations is reversed and construction of the matrix is not ‘by the book’. Usually the affine transformation matrix \\(\\mathbf{A}\\) for an anti-clockwise rotation by angle \\(\\theta\\) around the origin, would be\n\\[\n\\mathbf{A} =\n\\left[\\begin{array}{cc}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{array}\\right]\n\\]\nHere, because we are post-multiplying the rotation will be in the other direction… and to rotate anti-clockwise, you use the \\(-\\mathbf{A}=\\mathbf{A}^T\\)\n\\[\n-\\mathbf{A} =\n\\left[\\begin{array}{cc}\n-\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & -\\cos\\theta\n\\end{array}\\right] =\n\\left[\\begin{array}{cc}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{array}\\right] = \\mathbf{A}^\\mathrm{T}\n\\]\nThis means that if you are doing any serious affine transforming of sf shapes at a low-level in R spatial, I recommend either writing some wrapper functions that generate and apply the necessary matrices on the fly, or, probably better yet, using the wk package which has proper support for affine transformations."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html",
    "title": "A population based binary space partition",
    "section": "",
    "text": "In a recent post I developed some R code to quadrisect the population of Aotearoa New Zealand. At the end of that post I airily suggested\nThe thought was something like, bisect the population data (by population), then bisect each half, then bisect each quarter and so on. The bisection procedure using weighted medians was thoroughly explored in the earlier post.\nNow, as promised, here’s that iterative bisection process, which yields something like a population quadtree. Each leaf of the tree accounts for some power-of-two fraction of the population (in the example below 1/1024th). If we had the location of every individual in the data as a point, the result is roughly what we would get if we were adding those points to a binary space partition data structure.\nIt would be nice to do this as a quad-tree where at each step we subdivide the population into 4 more or less equal groups, but that would require allowing the lines subdividing the space to be at arbitrary angles at each step and would demand much more complicated coding than what follows. Sticking to halving the population at each step means we can work with only east-west or north-south bisectors and simplifies matters greatly. I’ll set the more complicated quad-tree approach to one side for now.\nAs an indication of where we are going, here’s the end result, showing approximately equal population rectangular areas around Christchurch."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#preliminaries",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#preliminaries",
    "title": "A population based binary space partition",
    "section": "Preliminaries",
    "text": "Preliminaries\nAs usual, we need some libraries and data.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(spatstat)  # for weighted median function\nlibrary(data.tree) # to build a tree\n\npop &lt;- read.csv(\"nz-pop-grid-250m.csv\") |&gt;\n1  dplyr::filter(x &lt; 2.2e6)\nnz &lt;- st_read(\"nz.gpkg\")\n\n\n\n1\n\nThis is to remove the far-flung Chatham Islands, which in this case detract from the pattern of the quadtree.\n\n\n\n\nA useful helper functions is get_rectangle() to return a sf polygon based on x and y limits.\n\n\nCode\n1get_rectangle &lt;- function(xmin, ymin, xmax, ymax) {\n  st_polygon(list(matrix(c(xmin, xmin, xmax, xmax, xmin,\n                           ymin, ymax, ymax, ymin, ymin), nc = 2)))\n}\n\n\n\n1\n\nA convenience function to get an sf polygon given x and y limits."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#iterative-population-bisection",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#iterative-population-bisection",
    "title": "A population based binary space partition",
    "section": "Iterative population bisection",
    "text": "Iterative population bisection\nCentral to this whole exercise is a function that given a dataframe of x, y, and population data, along with a bounding extent, returns two rectangles within the extent, each containing half the population. This is accomplished using a weighted median as in the previous post. Because we are working only with north-south or east-west subdivisions there isn’t any especially complicated geometry needed to handle this.\n\n\nCode\nsplit_population &lt;- function(pop_pts, bounds) {\n1  w &lt;- bounds[1]; e &lt;- bounds[3];\n  s &lt;- bounds[2]; n &lt;- bounds[4];\n  width &lt;- e - w; height &lt;- n - s;\n2  xy &lt;- pop_pts |&gt; filter(x &gt; w, x &lt;= e, y &gt; s, y &lt;= n)\n  bbs &lt;- list()\n3  if (height &gt; width) { # cut east-west at median y\n    my &lt;- weighted.median(xy$y, xy$pop_250m_grid)\n    south &lt;- xy |&gt; filter(y &lt;= my)\n4    x1 &lt;- min(south$x) - 125; x2 &lt;- max(south$x) + 125;\n    y1 &lt;- min(south$y) - 125; y2 &lt;- my;\n    bbs[[1]] &lt;- get_rectangle(x1, y1, x2, my)\n    north &lt;- xy |&gt; filter(y &gt; my)\n    x1 &lt;- min(north$x) - 125; x2 &lt;- max(north$x) + 125;\n    y1 &lt;- my; y2 &lt;- max(north$y) + 125; \n    bbs[[2]] &lt;- get_rectangle(x1, my, x2, y2)\n  } else { # cut north-south at median x\n    mx &lt;- weighted.median(xy$x, xy$pop_250m_grid)\n    west &lt;- xy |&gt; filter(x &lt;= mx)\n    x1 &lt;- min(west$x) - 125; x2 &lt;- mx;\n    y1 &lt;- min(west$y) - 125; y2 &lt;- max(west$y) + 125;\n    bbs[[1]] &lt;- get_rectangle(x1, y1, mx, y2)\n    east &lt;- xy |&gt; filter(x &gt; mx)\n    x1 &lt;- mx; x2 &lt;- max(east$x) + 125;\n    y1 &lt;- min(east$y) - 125; y2 &lt;- max(east$y) + 125;\n    bbs[[2]] &lt;- get_rectangle(mx, y1, x2, y2)\n  }\n  bbs\n}\n\n\n\n1\n\nUnpack the bounding box to west, east, bottom, and top values, and use these to get width w and height h.\n\n2\n\nWe are only interested in the population inside the bounding box.\n\n3\n\nIf the area is taller than it is wide, then cut east-west at the weighted median y coordinate, otherwise cut north-south at the weighted median x coordinate.\n\n4\n\nHere, and elsewhere the 125 offsets account for the fact that the grid cells are 250m, but we using their central coordinates.\n\n\n\n\nThe bisection runs east-west if the extent of the bounding area is longer north-south than east-west, and will run north-south otherwise. This is to promote ‘squarer’ rectangles as outputs, although as we’ll see the vagaries of population distribution mean that plenty of long skinny rectangles make it through the process.\nUsing this splitter function it is straightforward to iteratively subdivide the data into halves by population to some requested depth. I found an R package data.tree that means I can store the results of an iterative splitting process like this as a tree, or more accurately as a linked set of ‘nodes’ with associated attributes. As is often the case in R, when a data structure that is not tabular appears the syntax is a little weird1, so figuring out how to use it took some experimentation. The Node$new() and using $ to invoke methods on objects was new to me, but I figured it out in the end.\nHaving struggled a bit to get data.tree working,2 the reward is nice compact code, which much more clearly expresses the intent to create a tree structure than some earlier sprawling and rather kludge-y list-based code.\n\n\nCode\nbsp &lt;- Node$new(\n  \"X\", bb = get_rectangle(min(pop$x), min(pop$y), \n1                          max(pop$x), max(pop$y)))\n\nfor (level in 1:10) {\n2  leaves &lt;- bsp$leaves\n  for (i in 1:length(leaves)) {\n    leaf &lt;- leaves[[i]]\n    id &lt;- leaf$name\n    bbs &lt;- split_population(pop, leaf$bb |&gt; st_bbox())\n    leaf$AddChildNode(Node$new(str_c(id, 1), bb = bbs[[1]]))\n    leaf$AddChildNode(Node$new(str_c(id, 2), bb = bbs[[2]]))\n  }\n}\n\n\n\n1\n\nThe ‘root’ node which we call ‘X’ which has as its ‘bb’ attribute, a bounding box of all the population data.\n\n2\n\nThen, we repeatedly split the ‘leaf’ nodes of the tree in two, adding them to each leaf as child nodes.\n\n\n\n\nA minor difficulty once the tree has been constructed is to retrieve the sf geometries intact. Because sf geometries are thinly wrapped R matrices and lists, they are prone to ‘reverting to type’ when you include them in R lists or other containers. Using data.tree the polygons did indeed revert to matrices, but it’s not difficult to retrieve them as polygons, which we do in the code below to assemble final simple feature datasets for mapping.\n\n\nCode\n1all_levels &lt;- bsp$Get(\"bb\") |&gt;\n2  lapply(list) |&gt;\n3  lapply(st_polygon) |&gt;\n4  st_as_sfc() |&gt;\n  as.data.frame() |&gt; \n  st_sf(crs = 2193) |&gt; \n5  mutate(level = bsp$Get(\"level\"),\n         id = bsp$Get(\"name\"))\n\n6top_level &lt;- all_levels |&gt; filter(level == 11)\n\n\n\n1\n\nThis is data.tree‘s syntax for retrieving the ’bb’ attribute of each node in the tree.\n\n2\n\nWrap the matrix we get back in a list.\n\n3\n\nAnd convert to an sf polygon.\n\n4\n\nThen we apply a relatively well-work st_as_sfc() |&gt; as.data.frame() |&gt; st_sf() sequence to convert to a simple features dataset.\n\n5\n\nThen we can append the binary space partition’s level and id attributes to our dataframe.\n\n6\n\nIt’s convenient to separate out the highest level of detail rectangles as a separate layer."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#some-maps-of-the-hierarchy",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#some-maps-of-the-hierarchy",
    "title": "A population based binary space partition",
    "section": "Some maps of the hierarchy",
    "text": "Some maps of the hierarchy\nSubdivision completed we can make a map of the final level of the tree, with 1024 rectangles. These are of roughly equal populations of around 5000.\n\n\nCode\nggplot(nz) + \n  geom_sf(fill = \"grey\", lwd = 0) +\n  geom_sf(data = top_level, fill = \"#ff101040\", \n          colour = \"white\", lwd = 0.15) +\n  guides(fill = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 2: All 1024 cells at level 11 of the partition.\n\n\n\n\n\nIt’s also interesting to see the stages along the way to the final subdivision, which is conveniently done using a facet_wrap based on the level attribute in the all_levels dataframe\n\n\nCode\nggplot(nz) + \n  geom_sf(fill = \"grey\", lwd = 0) +\n  geom_sf(data = all_levels |&gt; filter(level &gt; 1), \n          fill = \"#ff101040\", colour = \"white\", lwd = 0.1) + \n  facet_wrap( ~ level, ncol = 5) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 3: The sequence of bisections along the way to 1024 rectangles."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#a-close-up-look",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#a-close-up-look",
    "title": "A population based binary space partition",
    "section": "A close up look",
    "text": "A close up look\nIt’s also useful to look at a zoomed in local areas. As I’ve noted previously a big chunk of the country’s population is in Auckland, so it’s perhaps most interesting to look there. What emerges is a picture over an extended area3 of relatively evenly distributed population everywhere that is inhabitated (all the pink areas of the map), except for a concentration in a tiny area of the downtown centre. The result in terms of the space partition is relatively equal-sized areas across much of the populated suburban sprawl.4\n\n\nCode\nbb &lt;- c(1.735e6, 5.895e6, 1.785e6, 5.935e6)\nggplot() +\n  geom_sf(data = nz, fill = \"white\", lwd = 0) + \n  geom_tile(data = pop, aes(x = x, y = y, fill = pop_250m_grid)) +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +\n  geom_sf(data = nz, fill = NA, colour = \"#1e90ff80\", lwd = 0.3) + \n  geom_sf(data = all_levels |&gt; filter(level == 11), \n          fill = \"#00000030\", colour = \"white\", lwd = 0.2) +\n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)], expand = FALSE) +\n  guides(fill = \"none\") +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA, linewidth = 0.5), \n        panel.background = element_rect(fill = \"#1e90ff30\"))\n\n\n\n\n\n\n\n\nFigure 4: A zoomed in view around Auckland of the final layer of the partition."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#and-a-zoomable-view",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#and-a-zoomable-view",
    "title": "A population based binary space partition",
    "section": "And a zoomable view",
    "text": "And a zoomable view\nThis doesn’t really need any further comment. The hover shows the id variable which encodes the sequence of west-east or north-south choices by which any given rectangle was arrived at. See if you can find X1111111111 and X2222222222. It’s trickier to find X1212121212.5\n\n\nCode\ntmap_mode(\"view\")\ntm_shape(top_level) +\n  tm_polygons(fill = \"#ff101040\", border.col = \"white\", \n              popup.vars = FALSE, id = \"id\") +\n  tm_view(use_WebGL = FALSE)"
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#but-what-is-it-good-for",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#but-what-is-it-good-for",
    "title": "A population based binary space partition",
    "section": "But what is it good for?",
    "text": "But what is it good for?\nI’m honestly not sure about this! I learned about data.tree while making it, which is good.\nFor me, the next back-burner project in this line of thought is to think about how to equalize the size of the rectangles perhaps en route to cartograms. I find the rectilinear cartograms in early editions of Kidron and Segal’s The State of the World Atlas quite compelling. See this page for some spreads.6 In any case, perhaps this iterative bisection technique could be used as part of an automated workflow to make them.\nOr perhaps there is a population-centric web map zooming mechanism somewhere to be found in all this?"
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#footnotes",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#footnotes",
    "title": "A population based binary space partition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI should probably say weirder, because, let’s face it, R’s syntax is weird from the get-go.↩︎\nWhile using data.tree was a little confusing, it was not as bad as second guessing what R lists will do when you append sf polygons to them.↩︎\nThe extent of the mapped area is 50 by 40km.↩︎\nEchoes here of Austin Mithchell’s phrase ‘the quarter acre paradise’, which sounds rather belittling to me, but was apparently well received in New Zealand at the time.↩︎\nTo save it taking too much of your time: it’s near Levin, not far from where my Tararuas adventure unfolded.↩︎\nThe internet tells me that edition of the atlas is from 1981: Kidron M and R Segal. 1981. The State of the World Atlas. Pan Books, London. Many more recent editions have come and gone. The latest seems to be this one, although rectilinear cartograms are no longer quite so heavily featured.↩︎"
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "",
    "text": "I’ve just spent much of a not especially nice Saturday (weather-wise) tidying up some loose ends on the Computing Geographically website. In particular, I’ve been updating the R code snippets that make many of the figures. I started out intending to update the tmap v3 code to v4, since the latter is now recommended by the developer. It has, for example, been adopted in the latest edition of the excellent Geocomputation with R.\nA little way into the process, I realised that since doing last year’s 30 Day Map Challenge (see my efforts here) I use ggplot2 a lot more for my everyday mapping work than I do tmap. That’s in spite of having taught tmap for several years, a choice I made because its learning curve is less steep than ggplot2’s. Anyway, long story short, migrating the code on my book website from tmap v3 to v4 turned into more of a mixed bag: migrating some code to tmap v4, and some to ggplot2.\nIn this post I discuss some things to consider if you are choosing which of these two excellent packages—that’s important: they are both excellent packages—to use in various situations."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Same only different",
    "text": "Same only different\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tmaptools)\nlibrary(tmap)\nlibrary(htmlwidgets)\nlibrary(cols4all)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(dplyr)\n\n\nWhat the packages have in common is that they implement the idea of a grammar of graphics (hence ggplot2’s name) where we progressively add layers to a graphic, specifying for each layer how data attributes are scaled to give values of :visual variables (colour, line width, line style, symbol size and so on). There’s a graphic (in German) showing this idea here.\nSo the packages are similar at a high-level. However, I’m not going to delve into details of the differences between these packages. For that, you should explore the extensive online resources available. The best places to start are hard to pick. Maybe here for tmap and here for geospatial stuff in ggplot2. Just be aware that tmap is undergoing a major upheaval as it transitions to v4 and while older code should still work, you’ll see a lot of warnings. Meanwhile, ggplot2 has a habit of changing things without preserving backwards compatibility, so it’s advisable to be wary of any code snippets more than 3 or 4 years old when you are looking for help.\nRather than the details, I want to explore the packages ‘in use’ by looking at four broad aspects of mapping that speak to the advantages of a package specifically designed for mapping (tmap) over a more general purpose visualization tool (ggplot2), which nevertheless holds it own. Those four things are choropleth maps, raster data, web maps, and ‘map junk’ (north arrows and the like).\n\nChoropleth maps: tmap’s killer app\nMaking ‘proper’ choropleth maps in ggplot2 is no fun at all. See my Day 13 experience in the 2023 30 Day Map Challenge. The difficulty is that the central tenet of classified choropleth mapping is controlling how you relate data to colour fills. It’s not that ggplot2 won’t allow you fine control over this aspect of your choropleth map, it’s just that it really, really wants you to map your data linearly, and continuously to a colour ramp. That’s a reasonable design decision for a general scientific visualization tool. It’s just not what cartographers do in choropleth mapping.\nTo illustrate, here’s an old dataset I often use: TB cases in Auckland City in 2006 by Census Area Unit:\n\nak &lt;- st_read(\"ak-tb.gpkg\")\n\nAnd here’s the most basic ggplot2 choropleth map of the TB_RATE variable (which is cases per 100,000 population):\n\nggplot() +\n1  geom_sf(data = ak, aes(fill = TB_RATE))\n\n\n1\n\naes(fill = ...) specifies which variable maps to the colour fill\n\n\n\n\n\n\n\n\n\n\n\nA few things stand out here (to my eye at least):\n\nyou get a graticule in latitude-longitude even if the data are in a projected coordinate system (to be clear, the map is in the projected coordinates);\nthe default colour ramp goes from dark for low values to light for high values; and\nthe default colour ramp is black-to-blue, which might be preferable to the default muddy browns we’ve become accustomed to, but is an unusual choice for a map.\n\nOf course we can fix all these things:\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_continuous_c4a_seq(\n2    palette = \"brewer.reds\") +\n3  coord_sf(datum = st_crs(ak))\n\n\n1\n\nscale_fill_continuous_c4a_seq() is in the cols4all package which includes a very wide range of palettes\n\n2\n\nUse Brewer palette ‘Reds’\n\n3\n\nGet the coordinate system of the data using st_crs() and apply to the coordinate frame using coord_sf()\n\n\n\n\n\n\n\n\n\n\n\nNote that I am using a cols4all scale, because this is the preferred colour palettes package for tmap, and provides a wide array of options. For casual mapping, we likely don’t care about the grid, and we can get rid of that too using theme_void():\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"brewer.reds\") +\n  theme_void()\n\n\n\n\n\n\n\n\nSomething like the above, which is three lines of code after the introductory ggplot() call, is what I use most of the time. And the map is good enough, especially if it’s temporary and a stepping stone on the way to some other analysis.\nWhat does the same map look like in tmap v4?\n\ntm_shape(ak) +\n  tm_polygons(\n1    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(\n2      values = \"brewer.reds\")) +\n  tm_layout(\n    frame = FALSE, \n3    legend.frame = FALSE)\n\n\n1\n\nVariable name for fill in quotes\n\n2\n\nBrewer ‘Reds’ again\n\n3\n\nRemove annoying frames around map and legend\n\n\n\n\n\n\n\n\n\n\n\nEssentially the same map, up to minor aesthetic details. These are easily tweaked in either package, so we won’t worry about them too much here.\nWhere tmap wins out is if you want to experiment with classic choropleth map classification schemes, for example:\n\nm1 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\")) +\n1  tm_layout(title = \"Pretty\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\nm2 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n2      values = \"brewer.reds\", style = \"equal\", n = 6)) +\n  tm_layout(\n3    title = \"Equal intervals\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE) \n\nm3 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n4      values = \"brewer.reds\", style = \"quantile\", n = 6)) +\n  tm_layout(\n    title = \"Quantiles\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE)\n\nm4 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n5      values = \"brewer.reds\", style = \"sd\")) +\n  tm_layout(title = \"Std. Dev.\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\ntmap_arrange(m1, m2, m3, m4, ncol = 2)\n\n\n1\n\nThe default classification style is \"pretty\", i.e. human-friendly round numbers\n\n2\n\nstyle = \"equal\" to get equal intervals\n\n3\n\nadd a title to show the classification style\n\n4\n\nstyle = \"quantile\" for quantiles\n\n5\n\nstyle = \"sd\" for standard deviation breaks\n\n\n\n\n\n\n\n\n\n\n\nForget the minor issue with aligning these maps exactly, the point here is that these are all classified choropleth maps, with the classification method specified by the style parameter (more options are available than shown here). This is an established way to make choropleth colours easier to parse, or put another way, to make it easier to highlight the specific features of the data you want readers to focus on.\nThe magic here is that behind the scenes tmap is using the classInt package, and if we want to make similar maps in ggplot2 we have to do that work ourselves:\n\nlibrary(classInt)\n\nclass_breaks &lt;- ak$TB_RATE |&gt;\n  classIntervals(6, \"quantile\", digits = 1)\nbrks &lt;- round(class_breaks$brks, 1)\n\nggplot() + \n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_binned_c4a_seq(\n    palette = \"brewer.reds\", breaks = brks) + \n  theme_void()\n\n\n1\n\nNote the change to a binned scale, controlled by the breaks\n\n\n\n\n\n\n\n\n\n\n\nThe above is the minimal ggplot2 version of a quantile map I can come up with. If you want the nice class labels then you have to make those labels yourself and do more work on the legend. You can see an example of this in my Day 13 map in the 2023 30 Day Map Challenge.\nOverall, if you are making a lot of classified choropleth maps then tmap is your friend.\n\n\nRaster data: tmap’s other killer app\ntmap is comfortable dealing with raster data. Here’s a simple example:\n\ndem &lt;- rast(\"dem.tif\")\n\ntm_shape(dem) + \n1  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_intervals(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n1\n\ntm_raster() ‘announces’ that these data are raster so that some visual variables might be interpreted differently, e.g. col is now what fill is on a polygon layer, where col is interpreted as linework or outline colour\n\n\n\n\n\n\n\n\n\n\n\nNice! Note that the same tm_scale_intervals() function is used here to specify colours in this case as it was to specify fill colours in the choropleth map case. We can also have a continuous colour ramp, and layer on top a hillshade. First make the hillshade using some terra functions:\n\nslope &lt;- dem |&gt; terrain(\"slope\", unit = \"radians\")\naspect &lt;- dem |&gt; terrain(\"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, \n                   angle = 35, direction = 135)\n\nThen just add it as another layer also using the col aesthetic with a semi-transparent grey palette.\n\ntm_shape(dem) + \n  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_shape(hillshade) +\n  tm_raster(\n1    col = \"hillshade\",\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col_alpha = 0.35\n  ) +\n  tm_layout(\n    legend.frame = FALSE, legend.show = FALSE)\n\n\n1\n\ntmap is OK about adding a second layer using the col visual variable\n\n\n\n\n\n\n\n\n\n\n\nNicer! If I have one issue here it’s with using the parameter name values for the palette name, which takes a little bit of getting used to. Of course, the rationale for this is that the palette specifies where the colour visual variable is to get its values which in this context, are colours.\nggplot2 doesn’t really deal in rasters. We can make similar maps pretty easily, by converting to an x, y, attribute dataframe:\n\ndem_df &lt;- dem |&gt; as.data.frame(xy = TRUE)\nggplot(dem_df) +\n1  geom_raster(aes(x = x, y = y, fill = dem)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"hcl.terrain2\", name = \"Elevation\") +\n2  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(\n3    panel.border = element_rect(fill = NA, linewidth = 1))\n\n\n1\n\nNote that for ggplot2 the colours are still a fill here\n\n2\n\nexpand = FALSE prevents a margin around the data\n\n3\n\nggplot2’s theming system has a lot going on…\n\n\n\n\n\n\n\n\n\n\n\nYou can probably tell I’ve done a fair bit of this kind of thing: I didn’t just magic that expand = FALSE option and the panel.border thing out of nowhere.\nAnyway, while this approach is fine for a small raster like this one, even here the x-y dataframe equivalent to the raster has 35,000 or so rows, so this won’t scale well.\nAnd we haven’t even got into the headaches involved if you want to layer something else on top of a raster dataset where you need to use the fill aesthetic again, like say… a hillshade. Let’s just say the ggnewscale package is your friend. I leave that as an exercise for the reader…\nAgain… I think it’s clear that tmap is a better choice if you are making a lot of maps of raster data.\n\n\nWeb maps: tmap’s oth… OK, this is just getting silly\nWhen it comes to web maps ggplot2 doesn’t even try. tmap on the other hand has its ‘mode switch’ option. Set the mode to view and you are making web maps!\n\n1tmap_mode(\"view\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\", \n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"), \n    fill_alpha = 0.5)\n\n\n1\n\nNow you’re making web maps! Use tmap_mode(\"plot\") to switch back\n\n\n\n\n\n\n\n\n\n \nSo, yeah, if you are making bona fide web maps, tmap every time. Switch back to plot mode, and tmap also does a good job of allowing you to use a web map as a basemap layer:\n\ntmap_mode(\"plot\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"),\n    fill_alpha = 0.5) +\n1  tm_basemap(server = \"OpenStreetMap\") +\n  tm_layout(frame = FALSE, legend.frame = FALSE)\n\n\n1\n\nAdds a static web map as a basemap\n\n\n\n\n\n\n\n\n\n\n\nggplot2 can get the same effect using the ggspatial::annotation_maptile() function (you will also need to have the prettymapr package installed).\n\nggplot() + \n1  annotation_map_tile(zoomin = 1) +\n  geom_sf(data = ak, aes(fill = TB_RATE), alpha = 0.5) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  theme_void()\n\n\n1\n\nannotation_map_tile() is a function from ggspatial\n\n\n\n\n\n\n\n\n\n\n\nOverall though, another win for tmap.\n\n\nMap junk: whatever\nAs you might guess from the title of this section, I am not much concerned with north arrows and scalebars and such-like. tmap has them built in.\n\ntm_shape(ak) + \n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(values = \"brewer.reds\")) +\n  tm_compass() +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n\n\n\n\n\n\nTo get the same things in ggplot2 you need the ggspatial package, and they’re perfectly serviceable. I am about as excited about this as I sound.\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n1  annotation_north_arrow(location = \"br\") +\n2  annotation_scale(location = \"bl\") +\n  theme_void() + \n  theme(panel.border = element_rect(fill = NA))\n\n\n1\n\nAnother ggspatial function\n\n2\n\nAnd another\n\n\n\n\n\n\n\n\n\n\n\nNeedless to say, both tmap and ggplot2 offer lots of flexibility for adding titles and subtitles and positioning all these elements around the map wherever you want them.\nAll in all, this one is probably a tie, assuming you’ve installed ggspatial."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Final thoughts",
    "text": "Final thoughts\nIn brief then: tmap is better at:\n\nclassified choropleth maps;\ndealing with geospatial rasters; and\nweb maps,\n\nand about equal on\n\nweb map derived basemaps; and\nmap junk;\n\nThe latter two assuming that you have installed ggspatial. The funny thing is, I still find myself using ggplot2 more! For me the counterpoints to the above are:\n\nunclassified choropleths are often fine for a quick look-see at your data;\nI don’t make many raster-based maps, and ggplot2::geom_tile is often good enough for my purposes;\nWhere I previously used web maps in tmap to get an overview of my data, now I tend to use QGIS. If I really need a web map, I will certainly use tmap; and\nI am not a fan of map junk (hence: junk). Don’t get me wrong, I’ll but a north arrow and scalebar on a map if necessary, I just don’t make many maps like that.\n\nIf your mix of use-cases is different, especially if choropleth maps, raster data, and web maps loom larger in your world than they do in mine, then you might wind up making a different choice.\nI think the reason I end up gravitating to ggplot2 is that it is the entry point into a much wider visualization world. The idiom it has popularised, which tmap has reworked for mapping, of layering a series of aesthetics each linking a data variable to a visual variable (that so-called grammar of graphics) is more cleanly implemented in ggplot2, as you’d expect it to be. And the exact same semantics apply to scatterplots, boxplots, violin plots, histograms, bar charts, and so on. The ggplot2 ecosystem is sprawling and extremely powerful, and making maps is just one small corner of it.\nBecause of that, I am always making charts using ggplot(data) + geom_*() and it feels very natural to make maps the same way.\nHaving said that, for R coded maps that cover all the mapping bases, you should probably also get to grips with tmap. I use both regularly, even if I do use ggplot2 a little more regularly!"
  },
  {
    "objectID": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "href": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "title": "Giscience 2025",
    "section": "",
    "text": "The first Call for Submissions to GIScience 2025 has just come out (deadline for proceedings papers Jan 31 2025; for other submissions Apr 4 2025).\nWe’re excited in Aotearoa New Zealand to be hosting this one, even if I think it should be called giscience 2025 (note the non-capitalisation). I’m on the program committee so look forward (a little nervously) to a flood of submissions. It’s nice for a change that it won’t be us Aotearoans making the long trip.\nThe last big geospatial conference in NZ was Geocomputation 2019 which was greatly enjoyed by all who made it, and this one will be even better, I’m certain of it!"
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html",
    "href": "posts/2025-27-06-iata-codes/index.html",
    "title": "What three letters",
    "section": "",
    "text": "Last year some time I found myself on a flight from Dubai (DXB) to Dublin (DUB) and of course my pattern detecting brain noted the off-by-one-letter difference in the International Air Transport Association (IATA) codes of the two airports. It got in my head at the time and between watching bad movies and dozing and wondering if the flight would ever end (it was the third leg of a WLG - AKL - DXB - DUB endurance test) I found myself idly wondering, just how many off-by-one-letter flights there are. Are they rare or commonplace? Where are they? How many could there possibly be?\nSo, for all those similarly afflicted by such questions, here’s a post with more than you ever wanted to know about this deeply trivial matter. As always, there are coding tricks to be learned along the way. The code I think is less interesting can also be viewed by clicking on the ▸ Code links."
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#libraries-and-data",
    "href": "posts/2025-27-06-iata-codes/index.html#libraries-and-data",
    "title": "What three letters",
    "section": "Libraries and data",
    "text": "Libraries and data\nThe most interesting libraries we need are geosphere, which does all manner of great circle calculations. It could use an update to more fully integrate with sf, but that said is pretty great. stringdist does the needed calculation of differences between strings.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n1library(patchwork)\n2library(geosphere)\n3library(stringdist)\n4library(smoothr)\n\n\n\n1\n\nFor assembling multi-panel figures.\n\n2\n\nFor making great circle arcs.\n\n3\n\nFor determining similarities between strings.\n\n4\n\nFor densifying points along meridians to make a ‘globe’ polygon.\n\n\n\n\nWe need some base world data, and I’ve also made a ‘globe’ so we can colour in the background sensibly in a non-rectangular projection. I’ve chosen 30°E for the central meridian of my world maps because1 that allows all the off-by-one routes that exist to be shown without breaks.\n\n\nCode\n1c_meridian &lt;- 30\nprojection &lt;- str_glue(\"+proj=natearth lon_0={c_meridian}\")\n\nworld &lt;- spData::world\nworld_p &lt;- world |&gt; \n2  st_break_antimeridian(lon_0 = c_meridian) |&gt;\n  st_transform(projection)\n\nglobe &lt;- st_polygon(\n  list(matrix(c(c(-1, -1, 1,  1, -1) * 179.999 + c_meridian, \n                c(-1,  1, 1, -1, -1) * 90), ncol = 2))) |&gt;\n  st_sfc() |&gt;\n  as.data.frame() |&gt;\n  st_sf(crs = 4326) |&gt;\n3  densify(100)\nglobe_p &lt;- globe |&gt;\n  st_transform(projection)\n\n\n\n1\n\nUsing a central meridian 30°E gives a better centred final map.\n\n2\n\nBreak things based on a central meridian 30°E\n\n3\n\nFor rounded ‘edges’ to the globe when projected."
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#aviation-data",
    "href": "posts/2025-27-06-iata-codes/index.html#aviation-data",
    "title": "What three letters",
    "section": "Aviation data",
    "text": "Aviation data\nVia ourairports.com2 we can get pretty comprehensive data on airports, large and small. So much data in fact (some 83,210 airports) that I’ve limited the focus for this post to a mere 482 ‘large’ airports that also have an IATA code.\n\n\nCode\nairports &lt;- read.csv(\"https://davidmegginson.github.io/ourairports-data/airports.csv\") |&gt; \n  filter(type %in% c(\"large_airport\"), iata_code != \"\") |&gt;\n  select(name, iata_code, longitude_deg, latitude_deg) |&gt;\n  rename(x = longitude_deg, y = latitude_deg)"
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#all-possible-off-by-one-routes",
    "href": "posts/2025-27-06-iata-codes/index.html#all-possible-off-by-one-routes",
    "title": "What three letters",
    "section": "All possible off-by-one routes",
    "text": "All possible off-by-one routes\nFinding the best way to get all the two airport combinations proved trickier than I expected, at least it did for as long as I persisted with trying to do it ‘tidily’. I also took a detout via igraph which allows for a nice graph-based approach, round-tripping airport-pairs via a simple undirected graph. That’s appropriate in some respects, but really overkill for this problem. Knowing when to give up is half the battle, and switching to using base R’s combn is simple, give or take a bit of matrix conversion to get the results into a dataframe.\nAfter that it’s a simple matter to filter for routes with a Hamming distance between codes as calculated by stringdist of exactly one, and then join coordinate data for the two airports.\n\n\nCode\nroutes &lt;- combn(airports$iata_code, 2) |&gt; \n  matrix(ncol = 2, byrow = TRUE, \n         dimnames = list(NULL, paste(\"iata\", 1:2, sep = \"\"))) |&gt; \n  as.data.frame() |&gt;\n  filter(stringdist(iata1, iata2, method = \"hamming\") == 1) |&gt;\n  left_join(airports, by = join_by(iata1 == iata_code)) |&gt;\n  rename(name1 = name, x1 = x, y1 = y) |&gt;\n  left_join(airports, by = join_by(iata2 == iata_code)) |&gt;\n  rename(name2 = name, x2 = x, y2 = y) |&gt;\n  select(iata1, iata2, name1, name2, x1, y1, x2, y2)\n\n\n\nGreat circle routes\nWhile flights might very well not follow great circle routes in all cases, they are the most appropriate general way to represent them on a world map, which is where geoshphere’s functionality comes in.\nThe tricky part here was to break the great circle linestrings into multistrings at my chosen antimeridian (at 150°W). While sf::st_break_antimeridian does this correctly, for reasons I think associated with the various necessary format conversions I wound up with duplicate copies of every flight that crossed the anti-meridian, in addition to a multilinestring broken at the meridian. Fixing this necessitated some additional steps.\n\n\nCode\nroutes_sf &lt;- gcIntermediate(\n    routes |&gt; select(x1:y1), routes |&gt; select(x2:y2), \n    n = 100, addStartEnd = TRUE, breakAtDateLine = TRUE, sp = TRUE) |&gt;\n  st_as_sf() |&gt;\n  st_set_crs(4326) |&gt;\n  bind_cols(routes) |&gt;\n  st_break_antimeridian(lon_0 = c_meridian) |&gt;\n1  mutate(route = str_c(iata1, \"-\", iata2))\n\n2routes_multi &lt;- routes_sf |&gt;\n  group_by(route) |&gt;\n  summarise()\n\n3routes_sf &lt;- routes_sf |&gt;\n  st_drop_geometry() |&gt;\n  distinct(iata1, iata2, .keep_all = TRUE) |&gt;\n  left_join(routes_multi, by = join_by(route)) |&gt;\n  st_as_sf()\n\n\n\n1\n\nAdd a \"AKL-WLG\" style route attribute to help in identifying duplicate routes.\n\n2\n\nGroup on the route attribute and summarise to create multilinestrings.\n\n3\n\nJoin the multilinestrings back to the data."
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#a-map-of-all-the-potential-off-by-one-routes",
    "href": "posts/2025-27-06-iata-codes/index.html#a-map-of-all-the-potential-off-by-one-routes",
    "title": "What three letters",
    "section": "A map of all the potential off-by-one routes",
    "text": "A map of all the potential off-by-one routes\n\n\nCode\nggplot() +\n  geom_sf(data = globe_p, fill = \"#ddeeff\", colour = NA) +\n  geom_sf(data = world_p, fill = \"#dddddd\", colour = NA) +\n  geom_sf(data = routes_sf, lwd = 0.05, colour = \"#666666\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 1: All possible routes between major airports differing by one letter in their IATA codes.\n\n\n\n\n\nJust to emphasise: this is not a map of actually existing off-by-one routes, it’s a map of all the possible such routes between our 482 large airports. There are 782 of them. That’s from a possible 116,402 routes, or about 0.67%. This is more than half as many again as we’d expect if the codes were random. There are up to 26-cubed or 17,576 codes. Any given code can connect to as many as 17,575 other codes, and of these only 75 can be off-by-one (there are 25 possible different letters in each of the three available positions). That’s 75 / 17,575, which is only 0.43%\nOf course the codes aren’t random, they generally bear some relation to the names of actual places3 and we’d therefore expect the distribution of letters in the codes to be uneven. We can confirm this very easily:\n\n\nCode\nletter_freqs = data.frame(\n  Letter = LETTERS,\n  Count = airports |&gt; \n    pull(iata_code) |&gt;\n    str_c(collapse = \"\") |&gt;\n    str_count(LETTERS)\n)\nggplot(letter_freqs) +\n  geom_col(aes(x = Letter, y = Count), fill = \"darkgrey\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 2: Counts of letters in the IATA codes of 482 airports\n\n\n\n\n\nI checked for the 9000 or so airports that have codes and the pattern is not so different, with A and S still the most favoured letters."
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#what-about-actually-existing-routes",
    "href": "posts/2025-27-06-iata-codes/index.html#what-about-actually-existing-routes",
    "title": "What three letters",
    "section": "What about actually existing routes?",
    "text": "What about actually existing routes?\nAmong other reasons, actually existing routes are different from all possible ones because the longest scheduled flights max out at a bit over 15,000km4\nSadly, it’s difficult to source open data on regularly scheduled flights, because it’s commercially valuable information. The best I could do was from openflights.org, but as they say “The third-party that OpenFlights uses for route data ceased providing updates in June 2014”.5 Anyway, we can filter out routes from our off-by-one dataset that don’t appear in this list.\n\n\nCode\nsched &lt;- read.csv(\n  \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\", \n  header = FALSE) |&gt;\n  select(3, 5) |&gt; \n  rename(iata1 = V3, iata2 = V5) |&gt;\n  mutate(ab = str_c(iata1, \"-\", iata2),\n         ba = str_c(iata2, \"-\", iata1))\n\nroutes_sf_actual &lt;- routes_sf |&gt;\n  mutate(ab = str_c(iata1, \"-\", iata2)) |&gt;\n  filter((ab %in% sched$ab) | (ab %in% sched$ba)) \n\n\nAnd we can map those.\n\n\nCode\nggplot() +\n  geom_sf(data = globe_p, fill = \"#ddeeff\", colour = NA) +\n  geom_sf(data = world_p, fill = \"#dddddd\", colour = NA) +\n  geom_sf(data = routes_sf, lwd = 0.05, colour = \"#666666\") +\n  geom_sf(data = routes_sf_actual, lwd = 0.5, colour = \"red\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 3: Actually existing routes off-by-one in their IATA codes highlighted in red.\n\n\n\n\n\nThe longest of these routes is Melbourne (MEL) to Delhi (DEL) from where you could get an onward connection to Helsinki (HEL). Surprisingly New Zealand’s only entrant in this list is the relatively local Auckland (AKL) to Adelaide (ADL). Amusingly, the shortest off-by-one route at 324km is between King Abdulaziz International Airport (JED) and Prince Mohammad Bin Abdulaziz Airport (MED) both in Saudi Arabia, and keeping airport naming rights in the family.\n\nMore local maps\nIn lieu of a zoomable web map6 below are four more localised views.\n\n\nCode\nairport_labels &lt;- airports |&gt;\n  filter(iata_code %in% c(routes_sf_actual$iata1, \n                          routes_sf_actual$iata2)) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\"), crs = 4326)\n\nget_proj_string &lt;- function(bounds) {\n  lon_0 &lt;- (bounds[1] + bounds[3]) / 2\n  lats &lt;- quantile(bounds[c(2, 4)], c(0.25, 0.5, 0.75))\n  lat_1 &lt;- lats[1]\n1  lat_0 &lt;- lats[2]\n  lat_2 &lt;- lats[3]\n  str_glue(\"+proj=aea +lat_1={lat_1} +lat_2={lat_2} +lon_0={lon_0}\")\n}\n\nget_ll_bbox &lt;- function(bounds) {\n  st_polygon(list(\n    matrix(c(bounds[1], bounds[1], bounds[3], bounds[3], bounds[1],\n             bounds[2], bounds[4], bounds[4], bounds[2], bounds[2]), ncol = 2))) |&gt;\n    st_sfc(crs = 4326) |&gt; \n    as.data.frame() |&gt;\n    st_sf() |&gt;\n    densify(100)  \n}\n\nget_ll_limited_gdf &lt;- function(gdf, ll_bbox, crs) {\n  gdf |&gt; \n    st_filter(ll_bbox, \n              .predicate = st_is_within_distance, \n2              dist = 1e6) |&gt;\n    st_transform(crs) |&gt;\n    st_make_valid()\n}\n\nget_regional_map &lt;- function(limits) {\n  proj     &lt;- get_proj_string(limits$lims)\n  bb       &lt;- get_ll_bbox(limits$lims)\n  world_p  &lt;- get_ll_limited_gdf(world, bb, proj)\n  routes_p &lt;- routes_sf |&gt; st_transform(proj)\n  actual_p &lt;- routes_sf_actual |&gt; st_transform(proj)\n  labels_p &lt;- get_ll_limited_gdf(airport_labels, bb, proj)\n  plot_lims &lt;- bb |&gt;\n    st_transform(proj) |&gt; \n    st_bbox()\n  ggplotGrob(   # return complex plot as grob: less prone to memory issues?\n    ggplot() +\n      geom_sf(data = world_p, fill = \"#d0d0d0\", colour = \"white\", lwd = 0.25) +\n      geom_sf(data = routes_p, lwd = 0.04, colour = \"#666666\") +\n      geom_sf(data = actual_p, lwd = 0.35, colour = \"red\") +\n      geom_sf_label(data = labels_p, aes(label = iata_code), size = 1.5) +\n      coord_sf(xlim = plot_lims[c(1, 3)], \n               ylim = plot_lims[c(2, 4)], expand = FALSE) +\n      ggtitle(limits$region) +\n      theme_void() +\n      theme(panel.border = element_rect(fill = NA, linewidth = 0.5),\n            plot.title = element_text(size = 10))\n    )\n}\n\nregional_limits &lt;- list(\n  list(region = \"North America\", lims = c(-115, 15, -65, 55)),\n  list(region = \"Europe\",        lims = c( -15, 30,  45, 70)),\n  list(region = \"Middle East\",   lims = c(  30, 10,  80, 50)),\n  list(region = \"East Asia\",     lims = c(  80,  0, 130, 40))\n)\n\nwrap_plots(lapply(regional_limits, get_regional_map))\n\n\n\n1\n\nNo lat_0 is required for this projection, but might be if I change it some time.\n\n2\n\nIt’s necessary to ‘cast the net wide’ to make sure all countries within a window in lat-lon are on the map in the projected space, which is quite different than the lat-lon bounding box.\n\n\n\n\n\n\n\n\n\n\nFigure 4: Regionally focused maps with IATA codes shown."
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#finally-are-the-off-by-one-routes-clustered",
    "href": "posts/2025-27-06-iata-codes/index.html#finally-are-the-off-by-one-routes-clustered",
    "title": "What three letters",
    "section": "Finally: are the off-by-one routes clustered?",
    "text": "Finally: are the off-by-one routes clustered?\nIt’s not easy to know exactly how to approach this question. But we might expect, due to geographical patterns in naming and language that there would be some clustering of the actual off-by-one routes. A rough and ready approach to testing this idea is applied in the code cell below, based on the lengths in km of the various subsets of flights. The idea is that that shorter length routes might be more common among off-by-one routes than in scheduled flights in general.\n\n\nCode\nrandom_xy &lt;- world |&gt; \n  filter(continent != \"Antarctica\") |&gt;\n  st_sample(250) |&gt;\n  st_coordinates()\npairs &lt;- combn(1:250, 2) |&gt; t()\n\nsched_xy &lt;- sched |&gt;\n  inner_join(airports, by = join_by(iata1 == iata_code)) |&gt;\n  rename(name1 = name, x1 = x, y1 = y) |&gt;\n  inner_join(airports, by = join_by(iata2 == iata_code)) |&gt;\n  rename(name2 = name, x2 = x, y2 = y)\n\ndf &lt;- list(\n  random = distGeo(random_xy[pairs[, 1], ], random_xy[pairs[, 2], ]),\n  scheduled = distGeo(sched_xy |&gt; select(x1, y1),\n                      sched_xy |&gt; select(x2, y2)),\n  possible = distGeo(routes_sf |&gt; st_drop_geometry() |&gt; select(x1, y1),\n                     routes_sf |&gt; st_drop_geometry() |&gt; select(x2, y2)),\n  actual = distGeo(routes_sf_actual |&gt; st_drop_geometry() |&gt;select(x1, y1),\n                   routes_sf_actual |&gt; st_drop_geometry() |&gt; select(x2, y2))) |&gt;\n  stack() |&gt;\n  as.data.frame() |&gt; \n  rename(Distance = values, Type = ind) |&gt;\n  mutate(Distance = Distance / 1e3,\n         Type = ordered(Type, levels = c(\"random\", \"scheduled\", \n                                         \"possible\", \"actual\"),\n                              labels = c(\"Random\", \"Scheduled\", \n                                         \"Possible off-by-one\",\n                                         \"Actual off-by-one\")))\n\nggplot(df) +\n  geom_histogram(aes(x = Distance), binwidth = 2000, \n                 fill = \"grey\", colour = \"black\", linewidth = 0.25) +\n  facet_wrap( ~ Type, scales = \"free_y\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 5: A comparison of the flight length distributions of each category of flight.\n\n\n\n\n\nLooking at these distributions there’s no reason to think there’s any geographical patterning, in spite of Canada’s attempt to tip the scales with its weird Y– codes.\nCompletely random flights with origins and destinations randomly located on land (excluding Antarctica) and our all possible off-by-one flights are similarly distributed, although the latter has a spike in shorter (sub-5000 km) flights most likely due to higher densities of within-region flights. A more sophisticate ‘null’ model would likely place random airports based on population densities, but that seems a bit like overkill in this context! Both these distributions have an irrelevant tail of flights longer than 15,000 km which simply don’t exist (at least not yet).\nMeanwhile, the lengths of actually existing off-by-one flights look very much like a random sample from the lengths of all scheduled flights.\nBased on this evidence it would be hard to make a claim that there’s any geographical pattern to where you are most likely to find yourself on one of these flights. A code after all is just a code, especially when, like IATA codes it has developed in a fairly ad hoc manner with a need for consistency locking in many early decisions.^[Unlike, for example, the numbers assigned to highways in the United States or roads in the United Kingdom according to explicitly geographical schemes).\nSo next time, if ever, you find yourself on one of these relatively rare flights, do your best to enjoy it. But there’s really no need to add this to your bucket list. Saying which, MEL-DEL-HEL seems like it could be… fun?"
  },
  {
    "objectID": "posts/2025-27-06-iata-codes/index.html#footnotes",
    "href": "posts/2025-27-06-iata-codes/index.html#footnotes",
    "title": "What three letters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpoilers…↩︎\nAlbeit hosted by David Megginson, who appears to be FlightGear flight simulator enthusiast.↩︎\nCanada’s weird system stretches this claim pretty far.↩︎\nI regret to say I’ve been on a couple of these, a side-effect of being from Ireland and living in New Zealand. I can’t recommend such super-long flights in economy unless it’s in an A380. But can I just say? Those things are awesome.↩︎\nOh well… this is a fun little project but not one worth spending any money on.↩︎\nWhich comes with its own challenges with respect to meridians and projections, etc.↩︎"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html",
    "href": "posts/2021-10-21-kde/kde.html",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#packages",
    "href": "posts/2021-10-21-kde/kde.html#packages",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#data",
    "href": "posts/2021-10-21-kde/kde.html#data",
    "title": "Kernel density estimation in R spatial",
    "section": "Data",
    "text": "Data\nThe data are some point data (Airbnb listings from here) and some polygon data (NZ census Statistical Area 2 data).\n\nLoad the data\n\npolys &lt;- st_read(\"sa2.gpkg\")\n\nReading layer `sa2' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/sa2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 78 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1735096 ymin: 5419590 xmax: 1759041 ymax: 5443768\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\npts &lt;- st_read(\"abb.gpkg\")\n\nReading layer `abb' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1254 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1742685 ymin: 5420357 xmax: 1755385 ymax: 5442630\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAnd have a look\n\ntm_shape(polys) +\n  tm_polygons() + \n  tm_shape(pts) + \n  tm_dots()"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "href": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "title": "Kernel density estimation in R spatial",
    "section": "spatstat for density estimation",
    "text": "spatstat for density estimation\nThe best way I know to do density estimation in the R ecosystem is using the spatstat library’s specialisation of base R’s density function. That means converting the point data to a spatstat planar point pattern (ppp) object, which involves a couple of steps.\n\npts.ppp &lt;- pts$geom %&gt;% \n  as.ppp()\n\nA point pattern also needs a ‘window’, which we’ll make from the polygons.\n\npts.ppp$window &lt;- polys %&gt;%\n  st_union() %&gt;%       # combine all the polygons into a single shape\n  as.owin()            # convert to spatstat owin - again maptools...\n\n\nNow the kernel density\nWe need some bounding box info to manage the density estimation resolution\n\nbb &lt;- st_bbox(polys)\ncellsize &lt;- 100\nheight &lt;- (bb$ymax - bb$ymin) / cellsize\nwidth &lt;- (bb$xmax - bb$xmin) / cellsize\n\nNow we specify the size of the raster we want with dimyx (note the order, y then x) using height and width.\nWe can convert this directly to a raster, but have to supply a CRS which we pull from the original points input dataset. At the time of writing (August 2021) you’ll get a complaint about the New Zealand Geodetic Datum 2000 because recent changes in how projections and datums are handled are still working themselves out.\n\nkde &lt;- density(pts.ppp, sigma = 500, dimyx = c(height, width)) %&gt;%\n  raster() \ncrs(kde) = st_crs(pts)$wkt  # a ppp has no CRS information so add it\n\n\n\nLet’s see what we got\nWe can map this using tmap.\n\ntm_shape(kde) +\n  tm_raster(palette =  \"Reds\")\n\n\n\n\n\n\n\n\n\n\nA fallback sanity check\nTo give us an alternative view of the data, let’s just count points in polygons\n\npolys$n &lt;- polys %&gt;%\n  st_contains(pts) %&gt;%\n  lengths()\n\nAnd map the result\n\ntm_shape(polys) +\n  tm_polygons(col = \"n\", palette = \"Reds\", title = \"Points in polygons\")\n\n\n\n\n\n\n\n\n\n\nAggregate the density surface pixels to polygons\nThis isn’t at all necessary, but is also useful to know. This is also a relatively slow operation. Note that we add together the density estimates in the pixels contained by each polygon.\n\nsummed_densities &lt;- raster::extract(kde, polys, fun = sum)\n\nAppend this to the polygons and rescale so the result is an estimate of the original count. We multiply by cellsize^2 because each cell contains an estimate of the per sq metre (in this case, but per sq distance unit in general) density, so multiplying by the area of the cells gives an estimated count.\n\npolys$estimated_count = summed_densities[, 1] * cellsize ^ 2\n\nAnd now we can make another map\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\",\n              title = \"500m KDE summed\")\n\n\n\n\n\n\n\n\nSpot the deliberate mistake?!\nSomething doesn’t seem quite right! What’s with the large numbers in the large rural area to the west of the city? Thing is, you shouldn’t really map count data like this, but should instead convert to densities. If we include that option in the tm_polygons function, then order is restored.\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\", convert2density = TRUE,\n              title = \"500m KDE estimate\")\n\n\n\n\n\n\n\n\nReally, this should be done with the earlier map of points in polygons too, so let’s show all three side by side. tmap_arrange is nice for this, although it has trouble making legend title font sizes match, unless you do some creative renaming. I’ve also multiplied the KDE result by 1,000,000 to convert the density to listings per sq. km, and we can see that the three maps are comparable.\n\nm1 &lt;- tm_shape(kde * 1000000) + \n  tm_raster(palette = \"Reds\", title = \"500m KDE\")\nm2 &lt;- tm_shape(polys) + \n  tm_fill(col = \"n\", palette = \"Blues\", convert2density = TRUE,\n              title = \"Point density\")\nm3 &lt;- tm_shape(polys) + \n  tm_fill(col = \"estimated_count\", palette = \"Greens\", convert2density = TRUE,\n              title = \"KDE summed\")\ntmap_arrange(m1, m2, m3, nrow = 1)"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html",
    "href": "posts/2025-06-01-nsew-island/index.html",
    "title": "North-south or east-west islands?",
    "section": "",
    "text": "Rather obviously, the proper names for Aotearoa New Zealand’s two largest islands are Te Ika-a-Maui and Te Waipounamu. It’s not just that those are the names given by the indigenous peoples of the whenua (and also official names), it’s that the extremely boring ‘North Island’ and ‘South Island’ are wrong. Or at any rate, not entirely right.\nThat’s a bold claim. Let me explain with the help of some uh… in-depth spatial analyis.\nCode\nlibrary(sf)\nlibrary(units)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nnz &lt;- st_read(\"nz-islands.gpkg\")\nSo here’s a usefully labelled map of the three big islands along with quite a few smaller offshore islands.1\nCode\nggplot(nz) + \n  geom_sf(aes(fill = north), lwd = 0) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  theme_void()\nI considered colouring Te Ika-a-Māui blue and Te Waipounamu red, for the respective colours of their allegedly strongest sports teams. But I’m not much invested in rugby, and Wellington are clearly the best cricket team,2 so I’ve inverted what many might consider the ‘natural’ colours for the two islands. This is also a nice echo of one of my favourite spreads from Chris McDowall and Tim Denee’s wonderful We Are Here:\nAnyway, if you look at that north-south map, it’s not entirely clear that east-west isn’t just as accurate a binary. If we randomly sample the islands, we get a sense of how strongly aligned north/east and south/west designations are.\nCode\npts &lt;- nz |&gt; \n  st_sample(1000) |&gt; \n  st_coordinates() |&gt;\n  as.data.frame()\n\nggplot(nz) + \n  geom_sf(lwd = 0) +\n  geom_point(data = pts, aes(x = X, y = Y), size = 0.25) +\n  geom_smooth(data = pts, aes(x = X, y = Y), method = \"lm\") +\n  coord_sf(datum = 2193) +\n  theme_minimal()\nWe can even put a number on this observation.\ncor(pts)\n\n          X         Y\nX 1.0000000 0.8515348\nY 0.8515348 1.0000000\nWith pretty high accuracy, if a place is in the ‘north’, it is in the ‘east’! There are probably better ways to ‘put a number’ on this observation (contingency tables and whatnot), but a correlation coefficient will do for now.\nTo be serious—for just a paragraph—this correlation between latitudes and longitudes is something to be aware of when doing spatial modelling. In the context, for example, of species distribution models, it’s an open question if latitude or ‘island’ is a better variate to include for this reason, and it’s certainly questionable due to collinearity to include both latitude and longitude in models.3"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#slicing-and-dicing-the-islands",
    "href": "posts/2025-06-01-nsew-island/index.html#slicing-and-dicing-the-islands",
    "title": "North-south or east-west islands?",
    "section": "Slicing and dicing the islands",
    "text": "Slicing and dicing the islands\nTo put areas on this perspective, we can slice the islands at their mutual centroid. First get a bounding box, and a centroid.\n\nbb &lt;- nz |&gt; st_bbox()\ncentroid &lt;- nz |&gt; \n  st_union() |&gt; \n  st_centroid() |&gt; \n  st_coordinates() |&gt;\n  c()\n\nNext make a function to move one bound of a bounding box and turn it into a simple features data set.\n\n1bbox_split &lt;- function(bb, ctr, half = \"N\") {\n  key   &lt;- c(N = \"ymin\", S = \"ymax\", \n             E = \"xmin\", W = \"xmax\")\n  coord &lt;- c(N = 2, S = 2, E = 1, W = 1)\n  bb |&gt; replace(key[half], ctr[coord[half]]) |&gt;\n    st_as_sfc() |&gt;\n    as.data.frame() |&gt;\n    st_sf()\n}\n\n\n1\n\nYes… this function is pretty ugly, but it gets the job done.\n\n\n\n\nAnd now we can split the country in each of these two directions.\n\n1ns_split &lt;- bind_rows(\n  bbox_split(bb, centroid, \"N\") |&gt; mutate(north_c = TRUE),\n  bbox_split(bb, centroid, \"S\") |&gt; mutate(north_c = FALSE))\n\new_split &lt;- bind_rows(\n  bbox_split(bb, centroid, \"E\") |&gt; mutate(east_c = TRUE),\n  bbox_split(bb, centroid, \"W\") |&gt; mutate(east_c = FALSE))\n\nnz_split &lt;- nz |&gt; \n  st_intersection(ns_split) |&gt;\n  st_intersection(ew_split) |&gt;\n2  mutate(ns_correct = north_c == north,\n         ew_correct = east_c == north) |&gt;\n3  group_by(ns_correct, ew_correct) |&gt;\n  summarise() |&gt;\n  mutate(area = st_area(geom) |&gt; set_units(\"km^2\"))\n\n\n1\n\nWe add boolean variables indicating the centroid-based classification of each half.\n\n2\n\nIf the centroid-based and toponym based classification are the same we consider the toponym ‘correct’.\n\n3\n\nDissolving the areas together with group_by |&gt; summarise makes for nicer maps.\n\n\n\n\nAnd we get the calculated areas below.\n\nnz_split |&gt; st_drop_geometry()\n\n# A tibble: 4 × 3\n# Groups:   ns_correct [2]\n  ns_correct ew_correct    area\n* &lt;lgl&gt;      &lt;lgl&gt;       [km^2]\n1 FALSE      FALSE        7169.\n2 FALSE      TRUE         7334.\n3 TRUE       FALSE       11494.\n4 TRUE       TRUE       238720.\n\n\nTo my (slight) disappointment north-south is less wrong than east-west would be, although not by much. Oh well, so much for that idea.\nHere are a couple of maps in case you’re missing them in that blizzard of code. Notably the very tip of Te Ika-a-Māui winds up in a notional ‘west’ island, and to the distress of many, a chunk of Canterbury winds up lumped with the ‘east’ island.4\n\n\nCode\nmap1 &lt;- ggplot(nz_split) +\n  geom_sf(aes(fill = ns_correct), lwd = 0) +\n  scale_fill_manual(breaks = as.logical(0:1),\n                    values = c(\"red\", \"lightgrey\")) +\n  geom_hline(aes(yintercept = centroid[2]), linetype = \"dotted\") +\n  guides(fill = \"none\") +\n  ggtitle('North-South wrong') +\n  theme_void() +\n1  theme(plot.title = element_text(hjust = 0.5))\n\nmap2 &lt;- ggplot(nz_split) +\n  geom_sf(aes(fill = ew_correct), lwd = 0) +\n  scale_fill_manual(breaks = as.logical(0:1),\n                    values = c(\"red\", \"lightgrey\")) +\n  geom_vline(aes(xintercept = centroid[1]), linetype = \"dotted\") +\n  guides(fill = \"none\") +\n  ggtitle('East-West wrong') +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n2map1 + map2\n\n\n\n1\n\nCentre-aligned titles seem better here.\n\n2\n\nUsing the excellent patchwork package here rather than facetted plots."
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#a-better-binary",
    "href": "posts/2025-06-01-nsew-island/index.html#a-better-binary",
    "title": "North-south or east-west islands?",
    "section": "A ‘better’ binary",
    "text": "A ‘better’ binary\nAt this point, I contemplated finding a decision boundary for points sampled from the islands, but I haven’t (yet) lost it completely.5\nA more rough and ready approach involved jiggling geom_abline around a bit until it threaded through the Cook Strait / Te Moana-o-Raukawa.\n\n\nCode\nhulls &lt;- nz |&gt;\n  group_by(north) |&gt;\n  summarise() |&gt;\n  st_convex_hull()\n\ng1 &lt;- ggplot(nz) +\n  geom_sf(lwd = 0) +\n  geom_sf(data = hulls, fill = NA, colour = \"red\") +\n  geom_abline(aes(intercept = 9.6125e6, slope = -2.414214), \n              linetype = \"dashed\") +\n  annotate(\"polygon\", x = c(1.65e6, 1.65e6, 1.75e6, 1.75e6),\n                      y = c(5.4e6,  5.5e6,  5.5e6,  5.4e6), \n                      fill = \"#00000030\", colour = \"black\") +\n  theme_void()\n\ng2 &lt;- ggplot(nz) +\n  geom_sf() +\n  geom_sf(data = hulls, fill = NA, colour = \"red\") +\n  coord_sf(xlim = c(1.65e6, 1.75e6), ylim = c(5.4e6, 5.5e6), \n           expand = FALSE, datum = 2193) +\n  geom_abline(aes(intercept = 9.6125e6, slope = -2.414214), \n              linetype = \"dashed\") +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA))\n\ng1 + g2\n\n\n\n\n\n\n\n\n\nAs it happens, that line is on a bearing close to north by northwest.6\nSo, in conclusion… East Northeast and West Southwest Islands, anyone?\nWell no: Te Ika-a-Māui and Te Waipounamu will do just fine, thanks!"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#addendum",
    "href": "posts/2025-06-01-nsew-island/index.html#addendum",
    "title": "North-south or east-west islands?",
    "section": "Addendum",
    "text": "Addendum\nWere the islands ever to split (politically,7 not seismically, where the splits run in different directions) the equidistance principle would require a line be drawn more like the one I’ve worked out below using Voronoi polygons generated from points along the coastlines of the islands.\n\n1bb &lt;- (nz |&gt; st_bbox() + 5e4 * c(-1, -1, 1, 1)) |&gt;\n  st_as_sfc()\n\nvoronoi_islands &lt;- nz |&gt;\n2  st_cast(\"POINT\") |&gt;\n  st_union() |&gt; \n  st_voronoi() |&gt; \n3  st_cast() |&gt;\n  st_as_sf() |&gt; \n  st_intersection(bb) |&gt;\n  st_join(nz) |&gt;\n4  group_by(north) |&gt;\n  summarise()\n\n\n1\n\nThe raw Voronoi polygons produced by st_voronoi extend well beyond the area of interest, so make a bounding box to clip them. The strange shenanigans with adding to the bounding box is to get properly squared off corners on extended bounding box (st_buffer’s settings don’t seem to allow for this).\n\n2\n\nIt’s necessary to merge points into a single multipoint for the Voronoi function to work properly.\n\n3\n\nIt’s further necessary to go through a number of steps to massage the polygons into a useable simple features dataset.\n\n4\n\nFinally dissolve them into single polygons.\n\n\n\n\nAnd here’s a map:\n\n\nCode\ng1 &lt;- ggplot(voronoi_islands) + \n  geom_sf(aes(fill = north), lwd = 0) + \n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  geom_sf(data = nz, lwd = 0) +\n  guides(fill = \"none\") +\n  theme_void()\n\ng2 &lt;- ggplot(voronoi_islands) + \n  geom_sf(aes(fill = north), lwd = 0) + \n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  geom_sf(data = nz, lwd = 0) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(1.65e6, 1.75e6), ylim = c(5.4e6, 5.5e6), \n           expand = FALSE, datum = 2193) +\n  theme_void()\n\ng1 + g2"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#footnotes",
    "href": "posts/2025-06-01-nsew-island/index.html#footnotes",
    "title": "North-south or east-west islands?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t mention the Chathams. Also, I’m lumping Stewart Island / Rakiura with Te Waipounamu.↩︎\nDon’t mention the football.↩︎\nFor what it’s worth, that observation is what I have to ‘thank’ for all this.↩︎\nIt may be worth noting here that a relatively common pub or Stuff quiz question concerns identifying which of a number of cities in Aotearoa New Zealand is the most easterly/westerly.↩︎\nPerhaps another time… or an exercise for an enthusiastic reader.↩︎\nGreat movie.↩︎\nNot entirely impossible to imagine, see this page.↩︎"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "title": "Low level handling of sf objects",
    "section": "",
    "text": "You can handle sf objects at a low level but it can take a bit of getting used to, and you have to watch out for floating point gotchas.\nknitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "title": "Low level handling of sf objects",
    "section": "Packages",
    "text": "Packages\nEverything here needs just sf and dplyr.\n\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "title": "Low level handling of sf objects",
    "section": "Making polygons",
    "text": "Making polygons\nMy main confusion dealing with polygons in sf sounds dumb, but was easily fixed. Matrices in R get populated by column, by default, where the points in a polygon are in the rows of the matrix (as they would be in a dataframe with x and y attributes). You just have to make sure to populate the matrices in the right order.\nThere’s also the slightly strange fact that you have to wrap a matrix of points in a list to make a polygon.\nSo because of the row-column thing, there’s a tendency to do\n\nmat &lt;- matrix(c(0, 0,\n                1, 0,\n                1, 1,\n                0, 1,\n                0, 0), nrow = 5, ncol = 2)\nsquare &lt;- st_polygon(list(mat))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBut that fails, because the matrix we made was\n\nmat\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    0    0\n[3,]    1    1\n[4,]    0    0\n[5,]    1    0\n\n\nand the first and last rows don’t match (even if they did, it’s not actually a polygon!).\nBut specify that the matrix should be populated byrow and all is well\n\nsquare &lt;- st_polygon(list(matrix(c(0, 0,\n                                   1, 0,\n                                   1, 1,\n                                   0, 1,\n                                   0, 0), nrow = 5, ncol = 2, byrow = TRUE)))\nplot(square)\n\n\n\n\n\n\n\n\nIf you happen to have vectors of the x and y coordinates, then it’s easier.\n\nx &lt;- c(0, 1, 1, 0, 0)\ny &lt;- c(0, 0, 1, 1, 0)\nsquare &lt;- st_polygon(list(matrix(c(x, y), nrow = 5, ncol = 2)))\nplot(square)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "title": "Low level handling of sf objects",
    "section": "Floating point coordinates and their discontents",
    "text": "Floating point coordinates and their discontents\nsf defaults to using floating point calculations which has some annoying side-effects. For example, the code below results in an error\n\nangles &lt;- 0:3 * 2 * pi / 3\nx &lt;- cos(angles)\ny &lt;- sin(angles)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 7, ncol = 2)))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBecause sf defaults to floating point it doesn’t consider the polygon closed due to precision issues that mean R considers sin(0) != sin(2 * pi):\n\nsin(0) == sin(2 * pi)\n\n[1] FALSE\n\n\nThere is no easy way to fix this except to round the coordinates!\n\nx &lt;- round(x, 6)\ny &lt;- round(y, 6)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 4, ncol = 2)))\nplot(triangle)\n\n\n\n\n\n\n\n\nThere’s not a lot you can do about this when you are constructing sf objects. Polygons must be closed, and equality is strictly applied to the opening and closing points. You can’t ask st_polygon to automatically close polygons for you.\nOnce you have polygons to work with, the problem can come back to bite you, but there is a way around it. For example, this works OK:\n\nsquare |&gt; \n  st_difference(triangle) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nBut let’s make two squares that are theoretically adjacent to one another, but happen to have non-integer coordinates (which… is pretty commonplace!)\n\nangles &lt;- seq(1, 7, 2) * 2 * pi / 8\nangles &lt;- c(angles, angles[1])\nx1 &lt;- cos(angles)\ny1 &lt;- sin(angles)\n\ns1 &lt;- st_polygon(list(matrix(c(x1, y1), nrow = 5, ncol = 2)))\nbb &lt;- st_bbox(s1)\ns2 &lt;- s1 + c(bb$xmax - bb$xmin, 0)\n\nplot(s1, xlim = c(-1, 2.1))\nplot(s2, add = TRUE)\n\n\n\n\n\n\n\n\nTwo squares, next to one another as we might hope, but if, for example, we st_union them we get a MULTIPOLYGON.\n\ns3 &lt;- st_union(s1, s2)\ns3\n\nMULTIPOLYGON (((0.7071068 0.7071068, -0.7071068 0.7071068, -0.7071068 -0.7071068, 0.7071068 -0.7071068, 0.7071068 0.7071068)), ((2.12132 0.7071068, 0.7071068 0.7071068, 0.7071068 -0.7071068, 2.12132 -0.7071068, 2.12132 0.7071068)))\n\n\nIf we plot them, they still appear separate\n\nplot(s3)\n\n\n\n\n\n\n\n\nand if we measure the distance between them, turns out they don’t touch at all, but are in fact a miniscule distance apart…\n\ns1 |&gt; st_distance(s2)\n\n             [,1]\n[1,] 3.330669e-16\n\n\nIt’s probably not necessary to point out how silly this is, even if it is strictly correct.\n\nRXKCD::getXKCD(2170)$img\n\n\n\n\n\n\n\n\n[1] \"https://imgs.xkcd.com/comics/coordinate_precision.png\""
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "title": "Low level handling of sf objects",
    "section": "Specifying precision for spatial operations",
    "text": "Specifying precision for spatial operations\nBy contrast if we use rgeos functions the equivalent union operation works as we might expect (although we do have to feed rgeos the old sp types of polygon, which we can do via a call to as(\"Spatial\")…)\n\nrgeos::gUnion(as(s1, \"Spatial\"), as(s2, \"Spatial\")) |&gt;\n  st_as_sfc() |&gt;\n  plot()\n\nError in loadNamespace(x): there is no package called 'rgeos'\n\n\nsf does allow us to effectively emulate the rgeos behaviour, albeit not for simple geometries. When we instead bundle geometries up into feature collections, we can assign them a precision, and this will take care of the kinds of problems we see above:\n\ns1_sfc &lt;- s1 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\ns2_sfc &lt;- s2 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\n\ns1_sfc |&gt;\n  st_union(s2_sfc) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThe first time I looked this up in help, I got it wrong due to careless reading, and, I think, assuming that the number you provide to st_set_precision() was a ‘tolerance’, or, in effect a ‘snap distance’. The help is also a bit roundabout, and directs you to this page, for an explanation of how it works.\nIn effect all coordinates are adjusted by applying a function like this one:\n\nadjust_precision &lt;- function(x, precision) {\n  round(x * precision) / precision\n}\nsqrt(2) |&gt; adjust_precision(1000)\n\n[1] 1.414\n\n\n\nst_snap\nAnother possible fix for the floating point issue is snapping points to the coordinates of another object before applying operations. So this works, although it is not as clean as the st_precision option. On the other hand, it does work on plain geometry objects, not only on those that have been bundled up into collections.\n\ns1 |&gt; st_snap(s2, 1e-8) |&gt;\n  st_union(s2) |&gt;\n  plot()"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "title": "Low level handling of sf objects",
    "section": "In conclusion",
    "text": "In conclusion\nThe tools for making and manipulating geometries at a low level are available in sf but they are not always as simple as you’d like. Of course, most often you are dealing with datasets and that’s where sf comes into its own. Just remember st_set_precision() and you should be able to avoid quite a few headaches…"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html",
    "title": "GeoCart’2024",
    "section": "",
    "text": "Another (even-number year) August coming to a close and another GeoCart’ is over.\nThe passing last year of Igor Drecki inevitably cast a shadow. But importantly, and even under that shadow, it was a fun meeting, as Igor would have wanted. Given the current woes of Wellington’s public sector, attendance was as expected down a little, but there were still enough faces old and new for the meeting to retain its usual friendly buzz, without it being too overwhelming.\nThe organisers of GeoCart’2024 can be proud of what they achieved in putting on a meeting that Igor would have enjoyed. It bore all the hallmarks of Igor’s vision for the meeting. Many keynotes (perhaps too many, but more on that below…) and many opportunities for informal discussion and exchange in the breaks between presentations, over lunch, and at the Icebreaker and Conference Dinner events. The latter in particular was really excellent—I don’t recall ever before having FOUR choices of starter, main, and dessert at a catered sit-down meal like this one: well done Dockside.1"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "title": "GeoCart’2024",
    "section": "Keynotes",
    "text": "Keynotes\nSo many keynotes…\nA two-and-a-half day meeting with six (yes SIX!) ‘all-hands’ presentations seems perhaps a little too much of a good thing. Having said that, all the keynotes were excellent. Like, really excellent. I didn’t necessarily agree with everything everyone said, but I was certainly engaged throughout. In chronological order:\nOcean Mercier kicked things off with ‘Te Taunaha Whenua: Mapping Connections to Place’ emphasising the role that maps and mapping play in defining and making place, and the more than coincidental relationship between those processes and colonisation and (yay!) decolonisation. I loved that Ocean also shared her longtime passion for orienteering— and its slightly goofy (but highly functional) maps. She was even heading off :rogaining one evening of the conference.2\n\n\n\nDaniel Huffman in their presentation ‘Sharing the Emotional Work of Mapping’ spoke to a topic that has been front of mind for me lately: the rollercoaster ride of the uncertainties of taking on projects as they appear, and balancing ambitions for your creative work against the expectations of paying clients. I have to confess to some disappointment that we didn’t get some Huffman eye candy along the way, but instead had to content ourselves with :Madison, Wisconsin’s—admittedly very nice—flag.\n\n\n\nDaniel O’Donohue. After Daniel H’s quietly reflective take on the emotional journey of making maps, next morning podcaster and self-described geospatial evangelist Daniel O gave everyone a good kick in the pants encouraged us all to get busy making millions on the interwebs. Some of this talk leaned a bit too much for cycnical old gen-X-er me into the social media influencer vibe, but more than a couple of people were taking notes, and I expect to see mass-market cartography rocket to the top of Aoteroa’s export earnings charts in the years to come. Seriously though, I did have a couple of conversations with folks prompted by this talk to explore the possibilities for getting paid more for their skills. Being underpaid is a problem in cartography/geospatial,3 so I wish them all the best, and will join them, just as soon as I come up with an idea. Also: we all got socks.4\n\n\n\nDavid Garcia somehow managed to keep the energy going in a talk entitled ‘Revisiting the Social Nature of Mapping’. As in Daniel O’s talk, the memes were flowing, but so were the serious messages echoing both Ocean’s and Daniel H’s talks from the previous day in taking seriously the emotional heft of cartography, and its roles in colonisation and decolonisation. The recently minted Dr. Garcia wears his learning lightly and with great good humour, but his work is deeply serious. For a sense of just how serious, here’s the thesis in full. I have made my own (marginal) contributions to critical GIS and critical cartography, and even managed to inject a little of it into the program at VUW but I still learned a great deal from this talk (and I also laughed a lot…).\n\n\n\nSarah Bell introduced the final day with an emotionally5 nuanced presentation pushing us to carefully consider the many ways that maps can be biased. So far so obvious,6 but this talk added a few more dimensions for the manipulation of an audience, most poignantly sound, in an animated revisiting of Snow’s cholera data. As co-author of a book whose first edition cover featured :the John Snow map, I was fully prepared to yawn my way through that part of the presentation, but this was very compelling.\n\n\n\nAnd finally, Wendy Shaw Toitū Te Whenua - Land Information New Zealand and Secretary of Ngā Pou Taunaha o Aotearoa - New Zealand Geographic Board, on the 100th anniversary of the board, updated us on progress in the ongoing consideration and restoration of Māori placenames across Aotearoa. The quick summary is that much has been done, and that much remains to be done. I scored a couple of the associated maps to add to Daniel O’s socks.\n\n\n\n\nToo much of a good thing?\nSo… I’ll say it again: all the keynotes were excellent. I can’t help wondering even so, if this balance in the meeting foregrounds the accomplishments of the already well-recognised, to the disadvantage of the great work being done by others.\nI am only really say this comparing GeoCart’s complement of keynotes relative to other conferences of similar size, where there is more likely to be one keynote a day, not two. In part the meeting follows this schedule to justify its two-and-a-half day duration so that there is space for two evening social events. There is likely not enough submitted work to make for two parallel tracks of presentations over that length of time. But I do wonder if in a small meeting like this two tracks are really needed. Cutting things back to three keynotes and a single track would make this a non-issue—and nobody would have to miss any of the talks!\nBut really, it’s not actually a problem, especially not if the keynotes can maintain this level of quality. And I know that for Igor a key mission for GeoCart was to connect cartography in Aotearoa New Zealand with the wider world, and keynotes from afar are one important way to do this. And having our visiting keynotes see the quality of the work in Aotearoa as exemplified in the ‘home team’ keynotes doesn’t hurt either.\nOne thing I do know is that finding that many keynote-worthy speakers every two years is a challenge: don’t hesitate to offer suggestions to the NZGS Committee!"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "title": "GeoCart’2024",
    "section": "Other highlights",
    "text": "Other highlights\nAs noted, with two tracks, you can’t catch ’em all, so this is inevitably a partial perspective… but among the highlights for me were:\n\nKarl Baker spoke with great good humour and impressive clarity about the often rather sorry tale of the mishandling of New Zealand’s longest placename in maps. Say it with me: Te Taumata­whaka­tangi­hanga­kōauauo­tamatea­turi­pūkākā­piki­maunga­horo­nuku­pōkai­whenua­kita­na­tahu.7 You can get some insight into the sorry tale of the use and abuse of the name from the wikipedia talk page for the place name, and there’s a concise correct pronunciation :here.\nAndy Tyrell presented a really cool workflow for automating the production of linework style oblique terrain maps like these.\nCraig Devereux’s map of the Roman Empire which won the map contest was a standout and hard to beat.\nJessie Colbert’s presentation of work with Katarzyna Sila-Nowicka and Dan Exeter visualising maps of multiple deprivation was a model of completeness and clarity. I hope to maybe get involved in this work with my ‘dots within dots’ and weaving contributions.\nSam van der Weerden gave a great great talk about Maynard Design’s work on the mapping for Auckland’s cycle network. Really interesting: particularly so was the insight that perhaps as cartographers we should sometimes resist our instinct to differentiate things by colour. After all, no matter what kind of cycle path it is, it’s still a cycle path and should be coloured green, and perhaps the less important differences among types of cycle path should be differentiated by line style not colour!\nDid I mention the dinner at Boatshed?\n\n\nSociety business\nAlso worth mentioning, the New Zealand Cartographic Society after somewhat inexplicably being refused permission to dual list its name in English and te reo Māori way back in 1989 (when doing so would have been well ahead of the curve) is finally en route, remaining bureaucratic hurdles notwithstanding, to adopting a te reo Māori name: Te Rōpū Tuhi Whenua o Aotearoa."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "title": "GeoCart’2024",
    "section": "Finally, permit me a moment of self-promotion",
    "text": "Finally, permit me a moment of self-promotion\nI enjoyed giving a talk on time-space maps of mountainous terrain. Here, as a bonus is an animated time-space map of a loop walk around Taranaki Maunga, which probably should have made it into the talk, but for one reason or another did not."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "title": "GeoCart’2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDisclaimer: I have not been recompensed in any form by Dockside.↩︎\nThat word rogaining is very weird derived from the given names of its ‘inventors’ ROd, GAil, and NEil. The names of new sports aren’t what they used to be: see also pickleball.↩︎\nThis recent episode of the MapScaping podcast is thought-provoking on the subject.↩︎\nI’m wearing mine now.↩︎\nThere’s that word again.↩︎\nMonmonier’s How To Lie With Maps anyone?↩︎\nI’ve gone with the longer not yet official form.↩︎"
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html",
    "title": "Mostly tiles, but also glyphs",
    "section": "",
    "text": "Note: You can take a closer look at any of the maps in this post simply by clicking on them.\nI’ve recently talked a lot (and will again…) about using tiling as an approach to present complex multivariate data in single map views. In this post I look at a relatively simple example of this problem from a project I’ve been working on recently—just three variables—where, it might be that a different approach is warranted, although as I think will become clear, the regularities of tiled patterns are useful regardless of whether the final result is technically (or even remotely) a tiling or not!\nI’m deliberately not going to say what the data represent. Suffice to say we have three variables, shown below for a chunk of New Zealand. From left to right, these are a resource potentially at risk, and two risk factors. The data preparation involved in assembling these risk factors is a whole story in itself, which I might get into some other time. For now, suffice to say, we’ve been careful to stick with colouring these three elements consistently (black, red, blue) as that helps a great deal in keeping some rather complicated data organised in our heads!\nYou’ll see also that we’ve calculated our three facets of the problem in a consistent geographic frame of reference, namely 10km grid squares. That’s 10×10km, i.e. 100km2 and not 10km2 as people so often seem to garble square units.1"
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tilings-by-squares",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tilings-by-squares",
    "title": "Mostly tiles, but also glyphs",
    "section": "Tilings by squares",
    "text": "Tilings by squares\nIt’s fine looking at the three datasets in parallel, but what’s really of interest is where high values of all three variables co-exist, and in given places which, if any of them, are low. Presenting more than one variable at once is exactly what our efforts in map tiling have been all about.\nSince only the red and the blue are risk factors one option using tiles is square tiles cut in half diagonally and choropleth coloured in the same manner as above. This gives us a map like this:\n\nThat seems fine, although… it might cause some unnecessary alarm given how almost the whole mapped area seems at great risk.2 So, can we also show the at-risk variable to offset that effect?\n One option is three variable tilings. You can slice a square into three equal sized elements rather easily. It turns out that if you cut from the centre of a square (or for that matter any regular polygon) to three points spaced at equal distances along its perimeter, then the magic of geometry guarantees the slices will be of equal size.3 Pop over to my handy dandy mapweaver app (I recommend opening that link in a new tab) and you can see what this looks like up close, by selecting the square-slice 3 tiling and setting Offset to 1. Mapping with this tileable unit gives us a map like the one below.\n\nBecause we’ve used the same colours here we can still tell which variable is which, although the tiling’s shapes tend to draw our eye to the cube-like arrangement of one blue tile on the left and one red tile on the right, where these are actually the data values from neighbouring grid cells, and should not strictly be paired with one another!\n We could fix this with ‘insetting’ as shown in the tileable unit to the left, although for me this is a little unsatisfactory as a pattern, and I leave generating such a map as an exercise for the reader.4 Another problem with this tiling is that the different shapes of the three tiles, which should perhaps in theory help us with telling one variable from another are distracting. Nice idea in theory then, but maybe not so great in practice."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#but-squares-are-so-square",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#but-squares-are-so-square",
    "title": "Mostly tiles, but also glyphs",
    "section": "But squares are so square",
    "text": "But squares are so square\nYes, you’re right. Squares are like, totally square. Hexagons are much sexier, even if we’re maybe all a bit over the hexbin moment and they are no longer the basis for the coolest discrete global grid systems.5\nIn the current application where we are interested in three variables, hexagons also have the convenient feature of a number of sides divisible by three so we can slice them into three pieces more nicely than squares. That approach yields a pleasingly cubic map:\n\n We could even make our slices pentagonal and get the best of both worlds. The problem with hexagons in this application is that the repeating hexagon shape doesn’t have a consistent mapping onto the underlying square gridded data, so which underlying data any particular group of tiles represents is unclear, and it’s even possible we are double counting some data and missing others. We could go back and redo all the analysis. It’s worth noting that all these numbers are estimates and subject to all kinds of variation as we move data around from geometry to geometry anyway, so perhaps the hex-square mismatch isn’t as bad as all that. Even so, if we really love the hex output map, then probably we should redo the analysis for hexagonal output areas. For now, I’m going to move on and go back to another way to present these three variables using square tiles cut diagonally."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#back-to-square-two",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#back-to-square-two",
    "title": "Mostly tiles, but also glyphs",
    "section": "Back to square two",
    "text": "Back to square two\nThe map below borrows an idea from the interesting value-by-alpha approach proposed some years ago by Rob Roth and others.6\n\nI’ve never entirely got along with value-by-alpha maps. This is partly because I’m not sure they work especially well on white backgrounds and I’m not cool enough for maps with dark backgrounds, and partly because I struggle to get the alpha (transparency/opacity) channel and all the associated blending modes to play nice. Somehow or other the alpha channel never has quite the effect I hope for and expect it to have in practice.\nThat said, the above map has some merit. I made it by calculating the colours in QGIS. You set the layer symbology to Simple Fill and then edit the expression for the colour to something like the below:\ncolor_rgba(if(\"var2\" &gt; 0, 204, 255),\n           if(\"var2\" &gt; 0, 0, 255),\n           if(\"var2\" &gt; 0, 0, 255),\n           \"var1\" * 255)\nIf you’ve ever used the expression editing in QGIS you’ll know it’s not a lot of fun to work with. Also, if you’re really paying attention, you’ll realise that this isn’t applying a colour ramp to var2 but simply turning it on if var2 is above 0 (which it is across most of the map, if you refer back to the first couple of maps in this post). It’s ‘dampening’ down the red colour by applying an alpha channel based on var1 the at-risk attribute. So this is almost like doing GIS overlay cartographically. The dimmed out reds and blues are of less concern because the resource at-risk is less exposed in those dimmed out areas. It would be possible using a more complex expression than that above to put the reds and blues on colour ramps, although given the transparency effect I think that the approach shown is better."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#just-show-all-the-data",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#just-show-all-the-data",
    "title": "Mostly tiles, but also glyphs",
    "section": "Just show all the data!",
    "text": "Just show all the data!\nI think the last example is interesting, and of the tiling-based approaches shown probably the most successful—although success will ultimately lie in the eyes of the end user of these maps and datasets.\nBut what that last example got me thinking is, “why not just show all the data?” The three variables are on different numerical scales, but if we rescale all three so they range from 0 to 1 then by setting symbology in QGIS to No Symbols and instead using the Diagrams option, we can construct a three variable histogram7 in each of the 10km square cells, after some rather annoying fiddle to get the barcharts to line up properly.8 By sticking with the red-blue-grey colouring it’s easy to tell which bar is which.\nAnyway, after some frustrating experiences with the options in the Diagrams option I wound up with this map:\n\nThis slice of the map doesn’t really do it justice, because what’s interesting about this map (to my tiling-attuned eye) is how the regular arrangement of barchart glyphs9 itself conveys information about the overall pattern. That is clearer when you see the whole map:\n\nWhere there is more ‘ink on the page’ or ‘colour on the screen’ are areas of greater possible concern. At the same time that you can see those overall patterns, it is easy to zoom in on particular areas for a closer look to confirm exactly why a particular area might be of interest. And because this representation is based on barcharts and does not depend on the vagaries of how colour is perceived but on estimation based on the heights of bars, which we are generally better at than reading values based on colours, the underlying data is likely to be conveyed more reliably by this map than some of the others here."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tiles-and-glyphs-the-same-or-different",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tiles-and-glyphs-the-same-or-different",
    "title": "Mostly tiles, but also glyphs",
    "section": "Tiles and glyphs: the same or different?",
    "text": "Tiles and glyphs: the same or different?\nThere isn’t really one ‘right’ map. Any of the maps shown in this post could I think be useful depending on the exact interest of the end user. We’ll find out more about that in the weeks ahead, and it’s entirely possible they will simply want all the underlying data assembled into some kind of interactive web map (perhaps even a GIS…).\nI still think single map summaries of multivariate data have real value, and their design is an ongoing challenge. I really like the tiling approach I’ve been developing over the last couple of years, but in this case, I think that the barchart glyphs are probably a better overall solution. Saying that, I’ve learned from my adventures in tiling that repeating arrangements are an incredibly powerful tool for revealing patterns in geographical data, and it’s that as much as the individual glyphs that is doing a lot of the ‘information transmission’ work here.\nIt is also worth pondering the question of when a tiling becomes an arrangement of glyphs once we allow for ‘insetting’ as shown in the three slice options above. Insetting the repeating tileable unit when the inset distance is small makes it easier to tell which tile is which in the overall map arrangment. But as shown below, as the inset gets wider, the tileable unit in effect becomes a glyph!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the latest version of the weavingspace module10 the option to scale, rotate, and skew tileable units independently of their spacing within the tiling has been enabled, which can support this perspective. This is certainly a new dimension to the approach that I expect to explore further in other projects."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#footnotes",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#footnotes",
    "title": "Mostly tiles, but also glyphs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was delighted to see this bugbear of mine picked up in a recent XKCD comic. Go. There. Now.↩︎\nAlthough, it’s really not: all those areas where there is no blue have very low risk associated with that factor and if we need both to be present then they are OK.↩︎\nA potentially useful tip for children’s parties. As for the geometry, it’s all about how the area of a triangle is given by \\(A=\\frac{1}{2}bh\\).↩︎\n😉↩︎\nThat honour now belongs to Carto’s A5geo pentagon based system in case you were wondering. I am of course using the word ‘coolest’ in the most nerdy and loosest of senses.↩︎\nSee for example this post.↩︎\nNot technically a histogram, more a bar chart, but now I am being nitpicky.↩︎\nBy default QGIS hovers them over the area centroid, which is fine… and makes complete sense if the areas are typical irregular polygons. For this gridded arrangement I wanted the barchart baselines to line up and that involves some more expressions to be entered to offset barcharts from the centroid depending on the values in the data. The barcharts also have to be drawn in Map Units which conjures up an alarming image of giant 3km wide bars rectangles the length and breadth of the country.↩︎\nThis paper provides a good overview of glyph-based visualisation: Borgo R, J Kehrer, DHS Chung, E Maguire, RS Laramee, H Hauser, MO Ward and M Chen. 2013. Glyph-based visualization: foundations, design guidelines, techniques and applications. In 34th Eurographics 2013: Girona, Spain - State of the Art Reports, ed. M Sbert and L Szirmay-Kalos, 39-63. doi: 10.2312/conf/EG2013/stars/039-063.↩︎\n0.0.6.79 at time of writing available on pypi.org↩︎"
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "",
    "text": "If you’ve seen me or my co-conspirator Luke Bergmann present at a cartographic meeting recently, there’s a fair chance you’ve heard us talk about using tiling and/or weaving to make maps of complex multivariate data.1 The idea is to break up map areas into multiple units by tiling so that we have available more than one shape to colour choropleth-style so that we can represent more than one variable in each area.\nPeople have been enthusiastic about the concept and the look of the maps, but (I’m fairly sure) reticent about taking up the idea because doing so demanded a willingness to dive into writing python code to make such maps, or even just to explore the idea. For example, this talk which gives a useful overview of the concepts is liberally sprinkled with snippets of python code. That was intended to make it easier for anyone interested to jump in and have a go at making their own maps. But realistically, it probably discouraged the vast majority!\nAnyway, I recently came across a tool called marimo that enables creation of reactive python notebooks, which are shareable as apps. That got me thinking about providing a way to make tiled maps without writing code, which might get a few more people interested in the idea."
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#a-prototype-web-app-mapweaver",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#a-prototype-web-app-mapweaver",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "A prototype web app: MapWeaver2",
    "text": "A prototype web app: MapWeaver2\nIt’s almost there.\nLoad the app in another tab by clicking the button below. It takes a while to load, so in the meantime keep reading…\nLaunch MapWeaver\nUsing the app you can experiment with different tiling designs on a multivariate dataset: Dan Exeter’s Index of Multiple Deprivation3 for the central Auckland region in 2018.\nYou can choose how many variables you want to include (between 2 and 10), and what kind of tiling or weave pattern you want to use. You can also choose colour ramps to associated with each of the chosen variables. So with a few clicks, you can make this map\n\nor this one\n\nYou can also modify the spacing or rotation of a chosen tiling, or (for a weave) the width of the strands\n\nor experiment with different colour ramps and the resulting map appearance"
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#feedback-welcome",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#feedback-welcome",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "Feedback welcome!",
    "text": "Feedback welcome!\nIt’s proving trickier than I hoped to upload user data to the app (see this issue) so this isn’t the finished product, which will allow you to upload and tile your own data and download the results for further refinement in a GIS or graphics package. Even so, it would still be useful to get any thoughts or reactions from prospective users. Keep in mind:\n\nI know it is slow to load. I don’t think there is much I can do about this in a pyodide Web Assembly app that depends on several python packages beyond the core language. Just be patient… or if you have experience with this problem, and know how to fix it, get in touch and tell me how!\nIt’s worth exploring the available options with spacing set to a high value so that the refresh of the map is faster. Once you have a pattern you are happy with, then adjust the spacing to the desired value.\n\nThat aside, any thoughts just get in touch on LinkedIn or email me! We’re also interested in thoughts about this mapping method, not just this implementation.\nFor more on how MapWeaver does what it does, check out the weavingspace code repository."
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#footnotes",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#footnotes",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNothing at all to do with web map tiles.↩︎\nNothing at all to do with MapTiler↩︎\nSee also Exeter DJ, Zhao J, Crengle S, Lee A, Browne M, 2017, The New Zealand Indices of Multiple Deprivation (IMD): A new suite of indicators for social and health research in Aotearoa, New Zealand PLOS ONE 12(8) e0181260↩︎"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "title": "Experiments with R interpolators",
    "section": "Get input datasets",
    "text": "Get input datasets\n\nThe empirical projection\nThis file contains points on a global grid system, generated using the dggridR package. We can see the points in ‘lat-lon’ space below. Note how because this is a global grid system the points appear to ‘thin out’ towards the poles. This is an artifact of plotting the points in lat-lon, which is also explored in this post.\n\nemp_proj &lt;- read.csv(\"dgg-2432-no-offsets-p4-briesemeister.csv\")\nggplot(emp_proj) +\n  geom_point(aes(x = lon, y = lat), size = 0.05) +\n  coord_equal()\n\n\n\n\n\n\n\n\nInspection of the data shows we have two sets of coordinates lon, lat and x, y.\n\nhead(emp_proj)\n\n  ID dir     lon      lat          x       y\n1  0   .   11.25 58.28253  -428675.9 1520344\n2  1   . -168.75 58.28253 -1197290.8 7794188\n3  2   . -168.75 65.09003 -1120150.7 7234575\n4  3   . -168.75 72.07407 -1048583.9 6613223\n5  4   . -168.75 79.18998  -973499.6 5945572\n6  5   . -168.75 86.38746  -892969.4 5242144\n\n\nThis projection is Briesemeister, which is an oblique form of the Hammer-Aitoff projection. See\n\nBriesemeister W, 1953, “A New Oblique Equal-Area Projection” Geographical Review 43(2) 260\n\nIt’s possible to form this projection with a proj string, but it is not commonly supported in GIS, and who knows proj strings that well?! For the record, this is the string you are looking for:\n+proj=ob_tran +o_proj=hammer +o_lat_p=45 +o_lon_p=-10 +lon_0=0 +R=6371007\n\n\nA sample dataset\nWe also want a set of points to project, and what better than a world map. Note that we can only project points, so this is points along world coastlines, not polygons.\n\npts &lt;- read.csv(\"world_better.csv\") |&gt;\n  dplyr::select(lon, lat)\n\n# sanity check with a map\nggplot(pts) + \n  geom_point(aes(x = lon, y = lat), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "title": "Experiments with R interpolators",
    "section": "Triangles interpolator",
    "text": "Triangles interpolator\nThere are many different ways we can do this kind of interpolation. The simplest is based on triangulation. This method is available in the package interp but also in akima which is much quicker. The output x and y coordinates are formed by interpolating as shown below. x and y are the known locations of the input coordinate, which here are the longitude and latitude in out empirical projection dataset emp_proj. The desired outputs are at the longitude and latitude coordinates in the world maps dataset pts. And we do the interpolation twice, once for the x coordinate and once for the y coordinate in our target projection.\n\nx_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$x,\n                xo = pts$lon, yo = pts$lat)\ny_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$y,\n                xo = pts$lon, yo = pts$lat)\n\nNow make up a results data table and map it. akima puts the result in a column z in its output.\n\nresult &lt;- data.frame(x = x_out$z, y = y_out$z)\nggplot(result) + \n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "title": "Experiments with R interpolators",
    "section": "Apply the empirical projection’s cut region",
    "text": "Apply the empirical projection’s cut region\nWhat are those dots across the southern area of the map? These are points that happen to fall in triangles in the first coordinate system (i.e. lon-lat) where one corner of the triangle lies on a different side of a discontinuity in the projection than the other corners. We should avoid projecting points inside these triangles because they project (as we can see!) unreliably.\nFor the Briesemeister projection we know the precise location of this discontinuity, and have prepared a file delineating the ‘cut’ position. We can use this to remove points from the sample dataset that lie inside triangles that intersect the cut region.\nFirst, here is the discontinuity. Points close to or on this line could end up in very different parts of the projected output and so are ‘unsafe’ to project using our interpolation-based approximation.\n\ncut_sf &lt;- st_read(\"briesemeister-cut.geojson\")\n\nReading layer `briesemeister-cut' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-experiments-with-r-interpolators/briesemeister-cut.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -179.8892 ymin: -82.94613 xmax: -0.3442386 ymax: 44.55223\nGeodetic CRS:  WGS 84\n\nggplot(cut_sf) + \n  geom_sf()\n\n\n\n\n\n\n\n\nNow triangulate the empirical projection data points, and assemble a polygon from all those triangles that are intersected by the discontinuity.\n\n# make the cut region into a sf dataset\nemp_proj_sf &lt;- emp_proj |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326)\n\ntriangles &lt;- emp_proj_sf |&gt;\n  st_union() |&gt;\n  st_triangulate() |&gt;   # triangulation of empirical projection points\n  st_cast() |&gt;\n  st_as_sf() \n\ncut_triangles &lt;- triangles |&gt;\n  st_filter(cut_sf)\n\ncut_region_sf &lt;- cut_triangles |&gt; \n  st_filter(cut_sf) |&gt;\n  st_union() |&gt;       \n  st_as_sf() \n\nWe quite reasonably get a warning that triangulation doesn’t really apply to geographical coordinates, but… akima did the interpolation by triangulating these points and it doesn’t know it’s unsafe (because it’s not a geospatial package). It’s not actually ‘unsafe’ as such in this case, because we aren’t using the triangulation for its metric properties anyway. So… we ignore this warning and plot this to see what we are dealing with\n\nggplot(triangles) +\n  geom_sf(colour = \"grey\") + \n  geom_sf(data = cut_triangles, fill = \"grey\", colour = \"white\") +\n  geom_sf(data = cut_region_sf, fill = \"#00000000\", colour = \"black\") +\n  geom_sf(data = cut_sf, color = \"red\")\n\n\n\n\n\n\n\n\nNow we use st_disjoint to remove points in the data to project that are inside the cut region.\n\npts_to_project_sp &lt;- pts |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326) |&gt;\n  st_filter(cut_region_sf, .predicate = st_disjoint) |&gt;\n  as(\"Spatial\")\n\nThe last step converts the points to the SpatialPointsDataFrame format of the sp package, which akima can also work with:\n\n# we also need the empirical projection data in the sp format\nemp_proj_sp &lt;- emp_proj_sf |&gt;\n  as(\"Spatial\")\n\nx &lt;- akima::interpp(emp_proj_sp, z = c(\"x\"), xo = pts_to_project_sp, linear = TRUE)\ny &lt;- akima::interpp(emp_proj_sp, z = c(\"y\"), xo = pts_to_project_sp, linear = TRUE)\n\nA bit unexpectedly, akima outputs the data to a two column dataframe with the interpolated values in a column with the same name as the input data, so getting the results into a final output table is as below.\n\nresult &lt;- data.frame(x = x$x, y = y$y)\nggplot(result) +\n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()\n\n\n\n\n\n\n\n\nAnd those rogue dots are all gone!"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Computing Geographically argues for the importance of giscience taking geography seriously, and also for geography taking giscience more seriously. Too much GIS work is conducted as if \\((x,y)\\) coordinates were all that is needed to make data geographical; equally, far too many geographers think that those coordinates are all that GISers care about. This book aims to bridge the divide. An accompanying website includes high resolution copies of all the figures, and ‘bonus material’ (mostly code). See: computinggeographically.org.\n Spatial Simulation: Exploring Pattern and Process with George Perry is an advanced introduction to simple spatial simulation models for researchers in fields such as geography, planning, archaeology, ecology and beyond. George and I believe such models have great value across the ‘spatial sciences’ but the literature is scattered, making it a challenge for researchers to get started. We read all those mathematics, physics and statistics papers, so you won’t have to! A library of models in NetLogo accompanies the book, and can be downloaded here or explored at patternandprocess.org.\n Geographic Information Analysis with Dave Unwin, was first published in 2003, with a second edition in 2010, and translations into Mandarin and Korean. It has been a mainstay of my teaching (and that of many others!) since first publication, and offers an excellent, proven introduction to key principles and methods of spatial analysis and modelling."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html",
    "title": "Imagine setting up a Jupyter hub",
    "section": "",
    "text": "I’ve worked with :Jupyter notebooks for many years. Almost—but not quite!—since their inception.\nI was lucky enough to be a professor at Berkeley in 2014 when the technology was emerging in education. People associated closely with the Berkeley data science program pretty much invented this stuff. I developed a ‘connector’ class in geography as a contribution to the nascent Data Science major. The (I believe correct) thinking was that that it shouldn’t be possible to graduate in Data Science alone, and that all students should have some kind of domain expertise in the form of another major in a traditional discipline. To encourage this line of thought students taking ‘data science 101’ (in fact :Data 8 The foundations of data science) are required to take a parallel half-credit ‘connector’ class in some other discipline.\nI developed Geog 88 Data science applications in geography which was was my first experience with notebooks. It was also my first encounter with a very limited (at the time) geopandas. So limited that I had to hack together some code to make waiting for it to plot polygon layers bearable. As I recall the problem was that geopandas was iterating over the polygons in a dataset using a for loop and plotting them one at a time. The fix was to convert a GeoDataFrame to a PatchCollection and plot that instead. Anyway, they’ve fixed that problem now so this is ancient history. That was my first close-up and personal encounter with matplotlib a python module I continue to hate and love in near-equal measure to this day.1"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#my-life-as-a-jupyter-pioneer",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#my-life-as-a-jupyter-pioneer",
    "title": "Imagine setting up a Jupyter hub",
    "section": "",
    "text": "I’ve worked with :Jupyter notebooks for many years. Almost—but not quite!—since their inception.\nI was lucky enough to be a professor at Berkeley in 2014 when the technology was emerging in education. People associated closely with the Berkeley data science program pretty much invented this stuff. I developed a ‘connector’ class in geography as a contribution to the nascent Data Science major. The (I believe correct) thinking was that that it shouldn’t be possible to graduate in Data Science alone, and that all students should have some kind of domain expertise in the form of another major in a traditional discipline. To encourage this line of thought students taking ‘data science 101’ (in fact :Data 8 The foundations of data science) are required to take a parallel half-credit ‘connector’ class in some other discipline.\nI developed Geog 88 Data science applications in geography which was was my first experience with notebooks. It was also my first encounter with a very limited (at the time) geopandas. So limited that I had to hack together some code to make waiting for it to plot polygon layers bearable. As I recall the problem was that geopandas was iterating over the polygons in a dataset using a for loop and plotting them one at a time. The fix was to convert a GeoDataFrame to a PatchCollection and plot that instead. Anyway, they’ve fixed that problem now so this is ancient history. That was my first close-up and personal encounter with matplotlib a python module I continue to hate and love in near-equal measure to this day.1"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#and-so-to-aotearoa",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#and-so-to-aotearoa",
    "title": "Imagine setting up a Jupyter hub",
    "section": "And so to Aotearoa",
    "text": "And so to Aotearoa\nAnyway, in an ideal world, I’d say ‘and the rest is history’ at this point, but… while the trajectory of Jupyter notebooks has been onwards and upwards from there, my personal journey with them has been rather different.\nWe came back to Aotearoa New Zealand in 2018 for uh… reasons (not unrelated to more recent events in the US) to a position as Professor of Geography and Geospatial Science at Victoria University of Wellington. Taking the second part of that moniker (geospatial science) somewhat seriously, I was excited to advocate for using python notebooks in teaching, and… well… basically I hit a brick wall. Turns out that Berkeley was probably definitely five (maybe even more) years ahead of New Zealand universities in uptake and support for Jupyter notebooks and the associated cloud computing needed to make effective use of the technology in the classroom.\n\nIt’s too hard\nAt first when I asked for IT support to set up notebooks in the lab I encountered blank looks. Then after some research, mostly more blank looks, followed by some version of “it’s too hard” in the context of general computing labs where the GIS classes are taught. We briefly explored the free tier of Azure notebooks then on offer to the university through its everything-MS-all-the-time Microsoft subscription but the CPU cores provided through that deal were underpowered. I also contemplated using the Jupyter capability then emerging in the Esri suite, but that was ArcPy, which was and remains a clunky and unwelcoming introduction to the elegant python programming language. A most ‘unpythonic’ place to begin.2 The more friendly ArcGIS API for Python had yet to emerge (ArcGIS Pro had barely emerged at this point.)\nSo… I shelved things for a while, but returned to notebooks when it came to developing a class in Geographical Computing as part of the eventually-doomed postgraduate program in GIS at VUW. Given lab support limitations, I wound up teaching students about conda environments so we could all setup the same python environment on our computers and took it from there. So we never used Jupyter notebooks in the client-(remote) server mode, but instead in client-(local) server mode. It was fine, if a little deflating, given that Berkeley had been delivering notebook based instructions to classes of several hundred or more students five years earlier! On the upside, students did get to learn about virtual environments and all that good stuff."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#anyway-fast-forward-to-today",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#anyway-fast-forward-to-today",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Anyway… fast forward to today",
    "text": "Anyway… fast forward to today\nI still use notebooks all the time. It’s probably not a great habit, but they’re my go to for quickly exploring coding ideas. I even use them as a high level ‘test harness’ of sorts for the weavingspace module.\nAnd now, I’ve been asked to develop python training materials for a client. In that context, I once again have run into organisational IT. In brief, the course is to be delivered via Jupyter notebooks, but IT want the supplier (that would be me) to provide a solution completely outside their infrastructure. In other words, no in-house Jupyter hub or similar, student login credentials to be provided by me, and so on.\nWhich meant I was faced with the question that VUW IT were posed by me back in 2018: can you support Jupyter notebook based instruction?\nAt first I did the obvious thing and explored options at a number of more or less shrink-wrapped solutions such as Google Colab, CoCalc, and SaturnCloud. What I found there was more confusing than helpful. CoCalc is the only one actually focused on teaching/training and its pricing model is hard to unpick. The interface for setting up a course is also less than intuitive. The other two, like most offerings in this space are so focused on the work of machine-learning teams that it’s not at all obvious that they would work for my use-case: a relatively small group of learners working with a limited (and niche, i.e. geospatial) set of python modules. This pattern was repeated at all the options I looked at.\nAnd so,3 I thought it was time to roll up my sleeves and give setting up my very own Jupyter hub a try. How hard could it be? Really really hard I thought. After all, fully grown university IT departments have shuddered at the very thought."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#reader-its-really-not-that-hard",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#reader-its-really-not-that-hard",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Reader, it’s really not that hard!",
    "text": "Reader, it’s really not that hard!\nSeriously though, it really isn’t.\nIt’s so easy that I’m not going to provide tutorial advice, but instead link you to the excellent instructions I followed at The Littlest Jupyter Hub. It took about 45 minutes.\nBut that included registering payment details at Digital Ocean4 and two false starts, where first, I ignored a clear bolded instruction in the setup guide5\n\nand then I forgot to give the server a more memorable name than the default one assigned by Digital Ocean (this isn’t a problem as such, just annoying).\nWhat can I say? It had been a long day poring over complicated cloud computing pricing structures and negotiating 2FA on new services I had only joined to take a look. If you avoid those false starts and already have a cloud computing subscription, you could be up and running in under half an hour.\nIt took a further half hour or so to configure things on the server itself, setting up the default user environment, and most importantly setting up https rather than http access (for this you need an internet domain name on which you can establish subdomains).\nBut really… I have had more problems getting IMAP access to an email account via gmail to work properly."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#the-hardest-parts",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#the-hardest-parts",
    "title": "Imagine setting up a Jupyter hub",
    "section": "The hardest parts",
    "text": "The hardest parts\nThe biggest obstacles to doing this are most likely:\n\nFor https access you need an internet domain on which you can set up a subdomain. This is technically straightforward but you do need a domain you own and have some control over.\nCloud compute pricing plans are pretty much meaningless until you actually sign up. You won’t know what you need until you start using it, and you won’t know what that translates to in terms of actual compute you are paying for until you have some people logged in and using a server. It’s reminiscent of the worst days of mobile phone plans.\n\nThe latter is a real issue. I will have to on-charge the compute costs to the client and we won’t know what those are until after we’re done. We do have a no-cost (but some nuisance) plan B involving the somewhat amazing Github Codespaces, and we might even end up using that because of the cost, or more accurately, the uncertainty around the cost of the Jupyter hub solution.\nBut my main reflection on this experience would be to strongly encourage any organisational IT team to take a look at this. Chances are you are already using cloud compute infrastructure, so what’s the holdup?!"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#footnotes",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#footnotes",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, probably hate &gt; love. If there is a more convoluted API than matplotlib anywhere on the planet, I really don’t want to know about it.↩︎\nYes… I’ve drunk the python koolaid.↩︎\nEgged on, I should acknowledge, by an academic buddy↩︎\nA choice dictated more than anything by them appearing first in the list on the tutorial page: do yourself a favour and give one of the options with free compute hours a try first.↩︎\nRTFM!↩︎"
  },
  {
    "objectID": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "href": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "title": "The model zoo",
    "section": "",
    "text": "I’ve been diligently keeping the so-called ‘model zoo’ associated with our book Spatial Simulation updated for compatibility with new releases of NetLogo. Each time I generally manage to clean up a few parts of the code and slowly complete the ‘Info’ tabs (otherwise known as user documentation).\nThe models are available from this repo and you can get an overview of what they’re all about at this website. Really though, you should buy the book:"
  },
  {
    "objectID": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html",
    "href": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html",
    "title": "Look ma! (Almost) no javascript!",
    "section": "",
    "text": "Last week I gave talk about how I made a web app for our tiled and woven maps work1 to the Wellington Python meetup held at the Sharesies office on the third Thursday of every month.\nThe talk was recorded, although the original was pretty choppy as a result of some weird interactions between google meet, macos workspaces, and full screen browser windows (or something) so the below is a re-recording I made the day after.\nIf you are mostly interested in how you can make a web app in pure python while writing no javascript, then skip to about 19 minutes in where I talk about marimo and more or less competently demonstrate how easy it is to do. I recently posted about marimo when I first encountered it two or three months ago.\nThe app itself is here. Be patient: it takes a little while to load, which is why I wrote almost no javascript. The little I did write is to show a splash screen while the app loads in the background!"
  },
  {
    "objectID": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html#footnotes",
    "href": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html#footnotes",
    "title": "Look ma! (Almost) no javascript!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI should note here as I somehow failed to do in the talk that the tiled and woven maps work is in collaboration with Luke Bergmann at UBC in Canada.↩︎"
  },
  {
    "objectID": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "href": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "title": "Useful utilities for NetLogo",
    "section": "",
    "text": "An intermittent side-project of mine over (github informs me) more than three years has been assembling a collection of useful functions or rather reporters and procedures written in NetLogo to make it just a little bit easier to do a range of fairly standard things in that language.\nDon’t get me wrong, NetLogo is wonderful and I love it dearly. I find it tremendous fun to code in. But any time I start dealing in any serious way with lists and strings or slightly more obscure probability distributions, I find that I am repeating myself rewriting code I’ve written dozens of times before. This probably became most apparent when I was writing the spatial COVID model.\nAnyway… the result has been this extremely intermittent and informal set of netlogo source (.nls) files that you can import into a NetLogo model to get access to more useful list and string handling without having to write an extension.\nNo warranties of fitness are expressed or implied. If you find them useful, let me know!"
  },
  {
    "objectID": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "href": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "title": "Locations of interest in 2021 delta outbreak",
    "section": "",
    "text": "For a time, during the August 2021 COVID outbreak that started in Auckland but eventually led to the end of New Zealand’s attempt to stop COVID completely, I continuously updated the above kepler.gl animated map of ‘locations of interest’. Kepler.gl is a nice tool which I’d recommend for making quick visualizations like this.\n\nThere was something oddly compelling about the mundane details of the reported locations of interest.\nGo to the viewer\nLike most of the country I got pretty fed up doing it in the end, so I stopped… for whatever it’s worth, looking at the timeline on the visualization the delta outbreak did get reigned in eventually, but it it was pretty clear that the mood of the country had changed and lockdowns weren’t going to stick for much longer."
  },
  {
    "objectID": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "href": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "title": "Mapping the Dulux colours",
    "section": "",
    "text": "Dulux have had a range of colours named for places in New Zealand for several years now. Clearly this is an opportunity for mapping too good to be missed. You can thank me later.\n\nGo to the map\nI gave a presentation about how this map was made using R to a Maptime! Wellington audience, and it takes you through the code in enough detail to require no further explanation from this post!"
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "title": "In praise of GeoPackages",
    "section": "",
    "text": "In the shapefiles must die wars I’ve been smugly using GeoPackages for several years now. Here are some reasons why."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "title": "In praise of GeoPackages",
    "section": "One file: no really, it’s just one file!",
    "text": "One file: no really, it’s just one file!\nIf there is one thing above all others to love about GeoPackages it’s this. When teaching newcomers, the standout advantage over shapefiles is simple: there is only one file, and not some number between three and seven (or is it eight? I’m just not sure).\nI’ve lost count of how often I had to disable the increasingly hard to find Hide file extensions option on a baffled student’s computer to reveal the disturbing truth that there were several identical-except-for-the-extension files that together formed a so-called shapefile. Or how when supplying data for lab assignments I had to include instructions about unzipping files to a known folder and so on (a seemingly simple requirement made much more complicated by more recent releases of Windows allowing the user to look inside a .zip file without actually unpacking the contents…)."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "title": "In praise of GeoPackages",
    "section": "Understandable ATTRIBUTE_",
    "text": "Understandable ATTRIBUTE_\nWhat I meant to say was: understandable attribute_names because you can have attribute names longer than 10 characters."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "title": "In praise of GeoPackages",
    "section": "Many layers",
    "text": "Many layers\nI’ve tended to shy away from packaging multiple datasets in GeoPackages as support for this feature has at times been uncertain and confusing.\nNow that I am less beholden to not confusing beginners where ‘one file = one layer’ is a useful rule to live by, I’ve started to look more closely at what’s going on here. It still has the potential to confuse — especially in QGIS’s right-click Export → Save Features As… — but there is untapped potential here for making life easier when it comes to sharing bundles of related data with a minimum of fuss.\nI’ll explain using my go to tool for general data wrangling, R’s sf package.\n\nMulti-layer geopackages in R\nAssuming you have a locally stored simple GeoPackage nz.gpkg, you read it using:\n\nlibrary(sf)\nnz &lt;- st_read(\"nz.gpkg\")\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow, if you’d like to add another dataset to that file, you can specify a layer to put it in. Before doing that, it’s probably best to check what layers are already there using st_layers():\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAs we might expect (and if you’ll excuse the awkward formatting due to the long crs_name) a single layer with the same layer name as the file itself. Say we buffer our data and want to store it back into the same file, then we can do the below, as long as we provide a new layer name to store it in:\n\nnz |&gt; \n  st_buffer(12000) |&gt; \n  st_write(\"nz.gpkg\", layer = \"coast\")\n\nWriting layer `coast' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nAnd now we can see that both layers are present in the file:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n2      coast Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n2 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow if you open this file in R unless you specify the layer you want, you’ll just get the first one:\n\nst_read(\"nz.gpkg\")\n\nMultiple layers are present in data source /Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg, reading layer `nz'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nand one of those warning messages it’s tempting not to read, but really should.\nAnd that’s it really, for multiple vector layers in GeoPackages in R.\n\n\nMulti-layer geopackages in QGIS\nMeanwhile, if you open a two-layer GPKG in QGIS you’ll see this:\n\n\n\n\n\nThat’s pretty clear. What’s unfortunately less clear than in R is the sequence of operations that will safely add a layer to an existing GeoPackage. That’s not quite fair: what is unclear is the warning message you get if you choose an existing .gpkg file as the destination for a dataset you’d like to save. The warning message looks like this:\n\n\n\n\n\nThis seems pretty scary. Before trying this at home I suggest you make a copy of the target geopackage if you are worried about losing your data, but if you steel yourself, and against every instinct hit Replace, then as long as you set a different name in the Layer name option of the Save Vector Layer as… dialog\n\n\n\n\n\nit will be fine, and you’ll end up with an additional layer in the target GeoPackage.\nYou can also manage the component layers of GeoPackages in QGIS’s Browser panel.\n\n\nOther platforms are available\nI should note that similar to R, the Python geopandas module through its read_file() and list_layers() functions, and its GeoDataframe’s to_file() method offers the same functionality as discussed above."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "title": "In praise of GeoPackages",
    "section": "Many formats",
    "text": "Many formats\nSo if you can put many different layers in a GeoPackage, can you mix vector and raster datasets in there too?\nTurns out you can, although, at least for R’s sf this is where things get a bit messy. terra is the package for dealing with raster data, so let’s load that and read in a raster dataset. Before doing that, I’ll clean up nz.gpkg so it only has one layer again:\n\nnz |&gt; st_write(\"nz.gpkg\", layer = \"vector\", delete_dsn = TRUE)\n\nDeleting source `nz.gpkg' using driver `GPKG'\nWriting layer `vector' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nNow load the raster layer\n\nlibrary(terra)\nnz_r &lt;- rast(\"nz.tif\")\nnz_r\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.tif \nname        : layer \nmin value   :     1 \nmax value   :     1 \n\n\nCrowbarring this thing into our GeoPackage is certainly possible, but it’s far from intuitive, and involves invoking some GDAL options.\n\nnz_r |&gt; \n  writeRaster(\"nz.gpkg\", \n              gdal = c(\"RASTER_TABLE=raster\", \"APPEND_SUBDATASET=YES\"))\n\nThe GDAL options are all documented, but applying them using terra::writeRaster is finicky, and clearly this is not for the faint-hearted!\nFurthermore… sf can’t ‘see’ the raster layer:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1     vector Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nI guess if you don’t ‘do’ raster layers then there is no point in being able to see them either ¯\\_(ツ)_/¯. terra is similarly see-no-evil about things and just reads in the raster layer that is in the file without commenting on other layers that might be present:\n\nrast(\"nz.gpkg\")\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.gpkg \nname        : Height \n\n\nQGIS actually does better here. It certainly sees both layers:\n\n\n\n\n\nIt’s worth noting that the QGIS Browser panel makes mixing raster and vector layers into your GeoPackages straightforward.\n\nOverall, I’ve been aware that I can bundle raster and vector layers in GeoPackages like this but haven’t used the capability. In part because I’ve only just figured out how to do it using the R tools(!), but mostly because I prefer to keep raster data in GeoTIFFs and vector data in GeoPackages so I can tell which is which at a glance."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "title": "In praise of GeoPackages",
    "section": "So, are GeoPackages perfect?",
    "text": "So, are GeoPackages perfect?\nOf course not. There’s an argument to be made that every format that perpetuates the simple features paradigm is a bad as every other. I even wrote a book that is — kind of — all about this. It’s one of the mysteries of the evolution of geospatial that topology was embedded in the ‘standard’ formats, until it wasn’t. For what it’s worth, I think we have relational databases to blame for that.\nGeoPackages don’t get us out of floating point geometry hell either.\nThere are also better formats for particular applications. GeoJSON is web-native in a way that GeoPackages never will be, and newer formats such as FlatGeoBuf and GeoParquet, and more recent approaches like Discrete Global Grids certainly have their place.\nBut in a world still dominated by relational DBMS, a geospatial format that is basically a wrapper around SQLite tables was almost certain to emerge eventually, and GeoPackages are that format. They’re vastly preferable to shapefiles, and it’s good to see them slowly (more quickly would be better) replacing them.\nThe shapefile is (almost) dead, long live the GeoPackage!"
  },
  {
    "objectID": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "href": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "title": "OK COVID, you win",
    "section": "",
    "text": "From some time in March 2020 for two years I downloaded the latest reported COVID data for New Zealand, added them to my spreadsheet of the various numbers, and updated a timeline I was keeping in R. The download process got a lot easier when I was introduced to the data downloader at University of Auckland eResearch.\nAs you can see, by the time I stopped things had gone pretty badly off the rails, and even a log scale wasn’t helping much.\nI can honestly say this was when I started to become competent with the ggplot2 package, and for that, as well as the reassurance the daily ritual provided for about a year and a half (we were doing so well…), I am grateful.\n\nAddendum\nI also mapped the progress of the vaccination program for a (much shorter) time. At least these numbers were released weekly and in a much more accessible form. Here’s how the critical ‘second dose’ went:"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "",
    "text": "One of the joys (ahem) of R spatial is moving data around between formats so you can use the best packages for particular jobs. Here’s an example using IDW interpolation in spatstat."
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Libraries",
    "text": "Libraries\nLibraries are the usual suspects plus spatstat (duh) and maptools for some extra conversions. We also need terra for the data prep.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Introduction",
    "text": "Introduction\nAs is often the case there is useful functionality in a package that doesn’t play nice with the core R-spatial packages. spatstat is really great for lots of things, but does not support sf and even needs a bit of persuading to handle sp data. Its implementation of IDW interpolation is nice however, so it’s nice to know how to use it. Whether or not you should ever use IDW is another question altogether, but we can worry about that some other time.\n\nData\nFirst we need a set of values to interpolate. I made a projected version of the R core dataset volcano which is a nice place to start.\n\nmaungawhau &lt;-  rast(\"maungawhau.tif\")\n\n\n\nSome random control points and a study area\nWe can get a dataframe of random points on the surface using terra::spatSample. We’ll make this into a sf object as a starting point because that’s the most likely situation when you want to interpolate data (you will have an sf source).\n\npts &lt;- maungawhau |&gt;\n  spatSample(500, xy = TRUE) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\")) |&gt;\n  st_set_crs(2193) |&gt;\n  st_jitter(5)\n\nWe also need a spatial extent for the interpolation, so let’s just make a convex hull of the points\n\nspatial_extent &lt;- pts |&gt;\n  st_union() |&gt;\n  st_convex_hull() |&gt;\n  st_sf()\n\nAnd just to see where we are at\n\ntm_shape(maungawhau) + \n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\")) +\n  tm_shape(pts) + \n  tm_dots() + \n  tm_shape(spatial_extent) + \n  tm_borders() + \n  tm_layout(legend.frame = FALSE)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Make the data into a spatstat point pattern",
    "text": "Make the data into a spatstat point pattern\nspatstat has its own format for point patterns, including coordinates, marks (the values) and a window or owin (the spatial extent). It’s best to make the window first and then we can make the whole thing all at once. spatstat prefers sp objects, so we go via ‘Spatial’ to get a spatstat::owin object. maptools provides the conversion to an owin.\n\nW &lt;- spatial_extent |&gt;\n  as.owin()\n\nWe also need the control point coordinates\n\nxy &lt;- pts |&gt;\n  st_coordinates() |&gt;\n  as_tibble()\n\nNow we can make a spatstat::ppp point pattern\n\npp &lt;- ppp(x = xy$X, xy$Y, marks = pts$maungawhau, window = W)\nplot(pp)\n\n\n\n\n\n\n\n\nSuccess!\nA previous notebook showed an even quicker way to do this, but where the window will be formed from a bounding box (and where’s the fun in that?)\npts |&gt;\n  as(\"Spatial\") |&gt;\n  as.ppp()"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Interpolation",
    "text": "Interpolation\nIt’s easy from here. power is the inverse power applied to distances, and eps is the resolution in units of the coordinate system.\n\nresult &lt;- idw(pp, power = 2, eps = 10)\nplot(result)\n\n\n\n\n\n\n\n\nThis can be converted back to a terra raster for comparison with the original surface.\n\n# stack the layers so we can 'facet' plot them\ncomparison_raster &lt;- rast(\n  list(maungawhau = maungawhau, \n       interpolation = rast(result) |&gt; resample(maungawhau)\n  ))\n\ntm_shape(comparison_raster) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n            col.free = FALSE,\n            col.legend = tm_legend(\n              title = \"Elevation\", \n              position = tm_pos_out(\"right\", \"center\"), \n              frame = FALSE))\n\n\n\n\n\n\n\n\nLike I said, IDW is not necessarily a great interpolation method!"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "",
    "text": "Warning: sensitive souls who care about geodetic accuracy should probably stop reading now. Liberal use of affine transformation of geometries throughout. You have been warned.\nBefore I get going, huge appreciation to Randall Munroe for the generous terms of use for the XKCD comics.\nFor a long time I thought about teaching a geography / GIS class using XKCD comics as the primary source material, or at the very least as a jumping off point for every lecture. Recently I actually went through every XKCD from the very beginning to the present to assemble a list of comics that would actually be usable in some semi-serious2 way for this purpose. Among the things I learned is that Randall Munroe has become a lot less fixated on sex than he was when he started, and also somewhat less worried about velociraptors.\nAlso, his interest in map projections only seems to have kicked in about halfway through the XKCD time series. Possibly this is related to an ongoing interest in GPS, which will likely be a topic for a future post. #977 Map projections posted in late 2011, is the first sign of an enduring interest in projections, but things don’t really take off until #1500 almost 4 years later, which is the first of several ‘bad map projections’, although not actually called out as such in its title.\nHerewith, my ranking from worst bad projection to best bad projection of the nine examples I found,3 accompanied in a few cases by attempts at recreating them in R (click on the code-fold links to see how). Eventually, perhaps, ongoing work on arbitrary map projections might enable me to automate reverse-engineering any XKCD projection. For now, a combination of perseverance, guess-work, and sheer bloody mindedness will have to suffice.\nCode\nlibrary(sf)\n1library(tmap)\n2library(usmap)\nlibrary(dplyr)\nlibrary(ggplot2)\n3library(stringr)\n4library(smoothr)\n5library(xkcd)\n\n\n\n1\n\nFor its World dataset.\n\n2\n\nFor its US states dataset from usmap::us_map().\n\n3\n\nFor forming proj strings that include values from data.\n\n4\n\nTo densify some shapes.\n\n5\n\nThe less said about this the better.\nFor what it’s worth, by worst bad projection I mean something like ‘least interesting’ and by best, I mean something like ‘most thought-provoking’.4"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-liquid-resize",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-liquid-resize",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#9 Bad Map Projection: Liquid Resize",
    "text": "#9 Bad Map Projection: Liquid Resize\n\n\n\nAlt-text: This map preserves the shapes of Tissot’s indicatrices pretty well, as long as you draw them in before running the resize.\n\n\nI don’t use Adobe Photoshop so XKCD 1784 is just a bit too inside baseball for me. It feels like there might be a missed opportunity here to have said something about continental drift and Pangaea and supercontinents, which is a topic that has come up in other XKCDs (see # 1449 Red Rover). Something about how Africa and South America fitting together so well is evidence for plate tectonics liquid resize."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-united-stralia",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-united-stralia",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#8 Bad Map Projection: The United Stralia",
    "text": "#8 Bad Map Projection: The United Stralia\n\n\n\nAlt-text: This projection distorts both area and direction, but preserves Melbourne.\n\n\nXKCD 2999 just doesn’t work for me.5 I appreciate the attempt at many to one projection, that is to say more than one place on Earth mapping onto a single location in the map. And it’s certainly a relief to know that Melbourne is preserved.6\nI didn’t even try to replicate this one as it seems self-evidently something you’d have to do in a drawing package. Overall, I think the idea of two places into one is much better realised elsewhere in this list."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-greenland-special",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-greenland-special",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#7 Bad Map Projection: The Greenland Special",
    "text": "#7 Bad Map Projection: The Greenland Special\n\n\n\nAlt-text: The projection for those who think the Mercator projection gives people a distorted idea of how big Greenland is, but a very accurate idea of how big it SHOULD be.\n\n\nThe whole Greenland thing in map projections is a bit played out. I realise it continues to blow minds how exaggerated its size is in the Mercator projection.7 Anyway, it’s not too hard to remake XKCD 2489 using R, provided you aren’t too respectful of, you know, actual projections. Apparently Mercator Greenland is too big even for this comedy projection, as I had to downscale it to 85% of its ‘raw’ Mercator size to get a good match to the published map.\n\n\nCode\ngreenland &lt;- World |&gt;\n  filter(name == \"Greenland\") |&gt;\n  st_transform(\"+proj=merc\") |&gt;\n  mutate(geometry = \n1    (geometry + c(2e6, -3.1e6)) * diag(1, 2, 2) * 0.85) |&gt;\n2  st_set_crs(\"+proj=merc\")\n\n3greenland_buffer &lt;- greenland |&gt;\n  st_buffer(1e5)\n\nw &lt;- World |&gt;\n4  st_transform(\"+proj=moll\") |&gt;\n5  st_make_valid() |&gt;\n  st_set_crs(\"+proj=merc\") |&gt;\n6  st_difference(greenland_buffer) |&gt;\n7  bind_rows(greenland)\n\nggplot(w) +\n  geom_sf() +\n  coord_sf(xlim = c(-1.2e7, 1.7e7), expand = FALSE) +\n  theme_void()\n\n\n\n1\n\nExperimentation suggested that some rescaling of Greenland as it is projected ‘raw’ is required to get the outcome in the comic.\n\n2\n\nAfter rescaling and shifting Greenland we have to tell a white lie and tell R the data are still Mercator projected.\n\n3\n\nA buffered Greenland clears a passage between Greenland and Canada.\n\n4\n\nFairly confident the rest of the world is Mollweide projected.\n\n5\n\nIt’s not uncommon for world data sets to have invalid polygons after projection and that’s the case here.\n\n6\n\nThis erases the existing Greenland and removes some of Canada’s offshore islands.\n\n7\n\nAdd Mercator Greenland into the data."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-madagascator",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-madagascator",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#6 Bad Map Projection: Madagascator",
    "text": "#6 Bad Map Projection: Madagascator\n\n\n\nAlt-text: The projection’s north pole is in a small lake on the island of Mahé in the Seychelles, which is off the top of the map and larger than the rest of the Earth’s land area combined.\n\n\nXKCD 2613 went up in my estimation after I figured out how to make it… the alt-text provides enough information to find the central coordinates, which are in La Gogue Lake on Mahé in the Seychelles.\nArmed with the ludicrously precise lat-lon coordinates8 Google provided me for this spot, I figured out how to approximate this map.\n\n\nCode\n1ll &lt;- c(-4.595750619515433, 55.43837198904654)\n\nmadagascator &lt;- World |&gt;\n  st_transform(str_glue(\"+proj=laea +lon_0={ll[2]} +lat_0={ll[1]}\")) |&gt;\n2  st_set_crs(\"+proj=laea +lon_0=150 +lat_0=90\") |&gt;\n3  st_transform(\"+proj=merc\")\n\nggplot(madagascator) +\n  geom_sf(fill = \"lightgrey\", \n          color = \"darkgrey\") +\n  geom_sf_text(aes(label = name), \n               check_overlap = TRUE, \n               size = 3) +\n  theme_void()\n\n\n\n1\n\nThose ludicrously precise coordinates from Google.\n\n2\n\nTell sf that the data are centred on somewhere different than they really are! Some experimentation was required here to find a central ‘meridian’ that didn’t cut Antarctica in half.\n\n3\n\nApply Mercator to the recentred data. There’s probably a setting of the Oblique Mercator (+proj=omerc) that can do this sequence of transforms in one go, but since what I have works, I am leaving well alone."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-south-america",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-south-america",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#5 Bad Map Projection: South America",
    "text": "#5 Bad Map Projection: South America\n\n\n\nAlt-text: The projection does a good job preserving both distance and azimuth, at the cost of really exaggerating how many South Americas there are.\n\n\nXKCD 2256 is just delightfully dumb really. Truly a bad map projection. It’s also hard (for me) not to love a map projection that will throw everyone doing Worldle off their game."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-exterior-kansas",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-exterior-kansas",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#4 Bad Map Projection: Exterior Kansas",
    "text": "#4 Bad Map Projection: Exterior Kansas\n\n\n\nAlt-text: Although Kansas is widely thought to contain the geographic center of the contiguous 48 states, topologists now believe that it’s actually their outer edge.\n\n\nThe ‘comical’ claim about topologists in the alt-text for XKCD 2951 is actually… well… true on a spherical surface.\nI figured out a way to approximate this projection. First, we project the contiguous 48 states centred on the geographic center and enlarge them so they ostensibly cover a greater part of Earth’s surface. Any projection should work here; I’ve used an azimuthal equidistant one.\n\n\nCode\n1ctr &lt;- c(-98.583333, 39.833333)\n2anti &lt;- c(ctr[1] + 180, -ctr[2])\n3proj1 &lt;- str_glue(\"+proj=aeqd +lon_0={ctr[1]} +lat_0={ctr[2]}\")\n4proj2 &lt;- str_glue(\"+proj=aeqd +lon_0={anti[1]} +lat_0={anti[2]}\")\n\nstates_big &lt;- usmap::us_map() |&gt;\n  rename(geometry = geom) |&gt;\n  smoothr::densify(20) |&gt;\n  filter(!(abbr %in% c(\"AK\", \"HI\"))) |&gt;\n  st_transform(proj1) |&gt;\n5  mutate(geometry = geometry * matrix(c(5, 0, 0, 5), 2, 2)) |&gt;\n6  st_set_crs(proj1)\n\nggplot(states_big) + geom_sf() + theme_minimal()\n\n\n\n1\n\nThe ostensible centre of the contiguous US.\n\n2\n\nThe antipode of the central location.\n\n3\n\nForward projection: this can likely be any sensible projection with a central coordinate pair.\n\n4\n\nThe distance projection from the antipode, that will ‘invert’ the space.\n\n5\n\nThis multiplication expands the extent of the states.\n\n6\n\nAfter the multiplication we have to reset the projection.\n\n\n\n\n\n\n\n\n\n\n\nNext we apply an azimuthal equidistant projection centred on the antipode of the geographical centre. At this point we have to ‘turn Kansas inside out’, then remove and add it back into the inverted states.\n\n\nCode\nstates_inv &lt;- states_big |&gt;\n  st_transform(proj2)\n\n1kansas &lt;- states_inv |&gt;\n  st_bbox() |&gt;                        \n  st_as_sfc() |&gt;\n  as.data.frame() |&gt;\n  st_as_sf() |&gt;\n  st_difference(states_inv |&gt; \n                  filter(abbr == \"KS\")) |&gt;\n  st_as_sf()\n\nstates_inv &lt;- states_inv |&gt;\n2  filter(abbr != \"KS\") |&gt;\n3  bind_rows(kansas)\n\n\n\n1\n\nMake ‘inverse’ Kansas by subtracting Kansas from the bounding box of the data.\n\n2\n\nRemove Kansas.\n\n3\n\nAdd back inverted Kansas.\n\n\n\n\nAnd now we can make the final map. Note that it seems like an adjustment to the aspect ratio has been applied in the comic. I’ve not bothered with that step here. I assume that at some point Randall Munroe exports map data to a drawing package and does final tweaks there.\n\n\nCode\nggplot(states_inv) +\n  geom_sf(fill = \"white\") +\n  geom_sf_text(\n    data = states_inv |&gt; st_point_on_surface(), \n    aes(label = full), check_overlap = TRUE, size = 3) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"grey\"),\n        panel.border = element_rect(fill = NA, colour = \"black\",\n                                    linewidth = 1))"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#upside-down-map",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#upside-down-map",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#3 Upside-down Map",
    "text": "#3 Upside-down Map\n\n\n\nAlt-text: Due to their proximity across the channel, there’s long been tension between North Korea and the United Kingdom of Great Britain and Southern Ireland.\n\n\nPersonally, living in Aotearoa New Zealand I enjoy the genre of ‘upside down’ maps, but have always felt that the notion they change your whole perspective on the world is overstated, so XKCD 1500 is a nice gentle undermining of that, which I appreciate.9\nI briefly contemplated an attempt at making this one, but thought better of it. Turning a whole map through 180° is not difficult:\n\n\nCode\nw180 &lt;- World |&gt;  \n  mutate(geometry = geometry * diag(-1, 2, 2)) |&gt;\n  st_set_crs(4326) |&gt;\n  st_transform(\"+proj=eqearth\")\n\nggplot(w180) +\n  geom_sf() +\n  coord_sf(expand = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLikely the easiest way to make this into a map like the one in the comic is to export to a drawing format and move things around by hand."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-abslongitude",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-abslongitude",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#2 Bad Map Projection: ABS(Longitude)",
    "text": "#2 Bad Map Projection: ABS(Longitude)\n\n\n\nAlt-text: Positive vibes/longitudes only\n\n\nIf only rendering XKCD 2807 was as easy as doing longitude = abs(longitude).10 In R things get a bit more involved. This would really be very straightforward in a drawing package.\n\n\nCode\n1get_hemisphere &lt;- function(central_meridian = 0,\n2                           density = 1) {\n  lons &lt;- c( 1,  1, -1, -1,  1) * 90 + central_meridian\n  lats &lt;- c(-1,  1,  1, -1, -1) * 90\n  st_polygon(list(matrix(c(lons, lats), ncol = 2))) |&gt;\n    st_sfc() |&gt;\n    as.data.frame() |&gt;\n    st_as_sf(crs = 4326) |&gt;\n    smoothr::densify(density)\n}\n\n3proj = \"+proj=eqearth\"\nhemi_w &lt;- get_hemisphere(-90)\nhemi_e &lt;- get_hemisphere(90)\n\n4world_w &lt;- World |&gt;\n  st_intersection(hemi_w) |&gt;\n  mutate(geometry = geometry * matrix(c(-1, 0, 0, 1), 2, 2)) |&gt;\n  st_set_crs(4326)\n\nworld_e &lt;- World |&gt;\n  st_intersection(hemi_e)\n\n5world_abs &lt;- world_w |&gt;\n  bind_rows(world_e) |&gt;\n  mutate(id = 1) |&gt;\n  group_by(id) |&gt;\n  summarise() |&gt;\n  mutate(geometry = geometry - c(90, 0)) |&gt;\n  st_set_crs(4326) |&gt;\n  st_transform(proj)\n\nhalf_globe &lt;- get_hemisphere(density = 50) |&gt; \n  st_transform(proj)\n\nggplot() + \n  geom_sf(data = half_globe, lwd = 0) +\n  geom_sf(data = world_abs, fill = \"white\") +\n  geom_sf(data = half_globe, fill = NA, lwd = 1) +\n  coord_sf(expand = FALSE) +\n  theme_void()\n\n\n\n1\n\nIt’s convenient to have a function for making hemispheres centred on a specified meridian.\n\n2\n\nThe density parameter gives us hemispheres that still look right after transformation to a non-rectangular projection.\n\n3\n\nAn educated guess that the map is in the Equal Earth projection.\n\n4\n\nThis is where we make all longitudes positive.\n\n5\n\nCombine the hemispheres and shift them so they are centred on the prime meridian and transform to Equal Earth"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-time-zones",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-time-zones",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#1 Bad Map Projection: Time Zones",
    "text": "#1 Bad Map Projection: Time Zones\n\n\n\nAlt-text: This is probably the first projection in cartographic history that can be criticized for its disproportionate focus on Finland, Mongolia, and the Democratic Republic of the Congo.\n\n\nI’m really not convinced XKCD 1799 is a bad map projection. It looks wonky as hell, but it’s certainly useful, which is all we can ask of any projection.11 I’m fairly confident Waldo Tobler would have approved, although I never had an opportunity to ask.\nIt’s interesting to contemplate what kind of automated process might be used to produce this map, but… I’m not even going to try. Happy to hear from anyone who gives it a go!"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#in-conclusion",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#in-conclusion",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "In conclusion",
    "text": "In conclusion\nClose analysis of the data confirms no clear long term trend in the badness of XKCD’s bad map projections, although more recent examples may be getting worse.12\n\n\n\n\n\n\n\n\n\nAnd that’s a wrap (or fold, or cut, or… some transformation anyway)."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#footnotes",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#footnotes",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInsert ‘You won’t believe how bad they are’ or similar clickbait headline here.↩︎\nWe’re working with a loose definition of the word ‘serious’ here.↩︎\nAlong with #1500 which was not called out as such↩︎\nYMMV. Don’t @ me.↩︎\nI’d obviously rank it higher if it was called United Straya.↩︎\nI have relatives there.↩︎\nMaybe someone should have a word with a recently elected president on this matter.↩︎\nSee XKCD 2170 on that subject↩︎\nI’m not sure how I feel about ‘The United Kingdom of Great Britain and Southern Ireland’, but that’s a whole other question…↩︎\nProbably in d3 it is.↩︎\nOr for that matter, model…↩︎\nThe same analysis suggests that while negative ranks are a possibility they remain unlikely.↩︎"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "",
    "text": "I wrote a paper waaaay back in 20041 which among other things was a plea for qualitatively inclined geographers to take complexity science seriously, as a sincere attempt to understand the world as it is, in all its, uh… complexity. It’s a tricky argument to make because the preferred tools of complexity science are computational or mathematical models, which are necessarily simplifications. Models, after all, are only useful if we can make sense of them and that’s only likely to be possible if they simplify the complex realities we are trying to understand.2\nThe paper has been widely cited, but a glance at where it has been cited suggests that my its readership has mostly been already sympathetic fellow-travellers in quantitative geography, not the more diverse audience I was hoping for, from across the discipline and beyond. While writing the paper I sent a draft to :Doreen Massey who was enthusiastic. The paper is framed as a response to her calls around that time for more dialogue across the divide between human and physical geography. The emergence of critical physical geography3 suggests that Massey’s call has not gone unheard.\nIt would be absurd to suggest that my paper has had even a fraction of that impact. Equally, it would be wrong to think that geography as a discipline has enthusiastically embraced the more holistic and open attitude to methods that taking seriously Massey’s call (or my paper) would entail.\nBut wait!"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Systems thinking spotted in the wild",
    "text": "Systems thinking spotted in the wild\nA workshop I was at last week suggests that maybe (just maybe) under the guise of systems thinking, complexity science might be starting to have an impact on more qualitative approaches.\nThe workshop was a gathering of some of the many researchers involved in the Moving the Middle (MtM) project down in Christchurch, ahead of the Agents of Change team’s Know Your Place environment + art event in Lyttelton Harbour. One of the presentations to the assembled team on the first morning of the workshop took me by (pleasant) surprise, as it was on :systems thinking. And one of the first things to be put on the screen was some version of the diagram below.\n\n\n\n\n\n\nFigure 1: A particularly complicated systems diagram—which is nevertheless a simplification4\n\n\n\nThis was part of an enjoyable talk by Nick Cradock-Henry summarising the elements of systems thinking, which the Agents of Change team have really picked up and run with in the last year as a framework for organising their thinking around the wider somewhat disparate MtM project.\nAs a long-time fan of :complexity science, which I consider to be either an evolution from systems thinking or a broader framework within which systems thinking sits,5 this was a big deal. I’ve wanted for years to see qualitatively inclined social scientists—the kind that talk about more-than-human geographies and such—to whole-heartedly engage with systems thinking!\nI’m not really sure what’s brought this on. Maybe it’s recent excitement about the :circular economy? Or a delayed reaction to COVID models? (Unlikely) Or maybe :Te Pūnaha Matatini is getting some traction with a wider audience? Whatever the reason, I’m glad it’s happening."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Making models and having conversations",
    "text": "Making models and having conversations\nThe primary tool of systems thinking the Agents of Change team seem to have been working with so far is causal loop diagrams (see Figure 1), which were ably explained both by Nick Cradock-Henry and by Justin Connolly. What this take on complexity/systems thinking brings to the fore is the importance of creating models (in this case primarily a visual model) collaboratively as things to talk about in a constructive way as groups of people try to understand some problem at hand.\nThe point of such models is not simulation or prediction, but understanding. Not even understanding necessarily, but arriving at sufficient agreement in a particular problem solving context about what makes things tick, so that useful conversations can be had.\nIf visual models eventually form a starting point for building system dynamics models, or agent-based models, or other kinds of computational models, that’s fine. But it’s also fine if that doesn’t happen. In fact, computational models can muddy the waters. They are expected or required to be predictive, and everyone becomes fixated on prediction and stops thinking. Or the model becomes a fall-guy (‘the model told me to do it!’). Or models are dragged into a role in monitoring and management that they weren’t designed for.6 When an organisation invests in building a simulation model, the chance of it being drafted into use for purposes well beyond its original scope are high, often with unintended consequences.\nOnce a computational model exists, there’s a danger of thinking that the topic at hand is now well enough understood that we have everything under control. But often it’s not really the model as end-product that is important, it’s the focus for conversation and discussion provided by collaboratively creating a model (informed by complexity/systems thinking that’s the important step), and that’s true whether the model is visual, computational, or even statistical."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Postscript: uncomplex thinking and big data analytics",
    "text": "Postscript: uncomplex thinking and big data analytics\nAnother reason I am happy to see a new audience get excited about the complexity/systems thinking approach to model-building is because there is far too much excitement about a very linear approach to model-building these days, in the multi-headed shape of big data analytics, machine-learning, and AI.7\nIt’s not that these methods aren’t useful. Of course they are! The problem is that they too often skip the collaborative model-building, or leave the model-building to machines, so that the most important opportunity for learning is lost. That’s not quite correct. The analyst developing such models often learns a lot in the process. The problem is that the point of the exercise is often not the learning along the way, but the final model that results, and once that’s done the assumption is that now we understand, and can predict and manage the problem at hand. And of course, for as long as the world continues to be open, interconnected, and processual (i.e., forever), that kind of model will inevitably be wrong if not today, then some day very soon.8"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe qualitative social scientists I was hanging out with last week are rightly skeptical of such uncomplex models. Their enthusiasm for complexity/systems thinking models on the other hand reflects how such approaches really do give us a better chance of getting a handle on the world. I really hope their enthusiasm isn’t a one-off but a harbinger of many more fruitful conversations ahead, across what have often seemed unbridgeable divides."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. It’s paywalled, but get in touch if you’d like a copy.↩︎\nI’ll note here that models “are never true, but fortunately it is only necessary that they be useful”, p.2 in Box GP. 1979. Some problems of statistics and everyday life. Journal of the American Statistical Association 74(365) 1-4. Paywalled, I’m afraid.↩︎\nThe original paper on this is Lave R et al. 2014. Intervention: Critical physical geography. The Canadian Geographer / Le Géographe canadien 58(1) 1–10. But that is paywalled. An open access paper which gives a sense of what critical physical geography aims to accomplish is Lave R. 2015. Engaging within the Academy: A Call for Critical Physical Geography. ACME: An International Journal for Critical Geographies 13(4) 508-515.↩︎\nFigure from Monat JP and TF Gannon. 2015. Using systems thinking to analyze ISIS. American Journal of Systems Science 4(2) 36–49.↩︎\nDepending on my mood, I can go either way, but if you need convincing of the links, see Merali Y and PM Allen. 2011. Complexity and Systems Thinking, 31-52 in The Sage Handbook of Complexity and Management PM Allen, S Maguire, and B McKelvey (eds) Sage.↩︎\nYes, I’m looking at you, Overseer.↩︎\nThis is something I’ve written about before: O’Sullivan D 2018. Big data … why (oh why?) this computational social science? In Thinking Big Data in Geography: New Regimes, New Research eds. JE Thatcher, J Eckert, and A Shears, 21–38. University of Nebraska Press. Again, paywalled: let me know if you’re interested to read a copy.↩︎\nIronically, models are most urgently needed for prediction precisely when prediction is difficult or even impossible.↩︎"
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html",
    "href": "posts/2024-10-17-duckdb/duckdb.html",
    "title": "The joy of DuckDB",
    "section": "",
    "text": "Just recently as part of this project I finally got around to putting some properly big data into an actual database and OMG! For the kind of situation I was in, I can’t recommend giving this a try enough."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "href": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "title": "The joy of DuckDB",
    "section": "Some background",
    "text": "Some background\nThe project in question as one component involves developing or at least exploring building :species distribution models for a large number of the bird species present in Aotearoa New Zealand. To that end we’ve obtained the latest eBird data collected for the New Zealand Bird Atlas. This is a phenomenal resource which includes the accumulated observations of thousands of citizen science volunteers, accumulated over several years.\nThe raw .txt file containing the observational data is a chunky 2.8GB with 7 million rows of data. Seven million rows isn’t so bad, right? Right, it really isn’t that bad. There is even an R package (of course there is), cheekily called auk1, for massaging the raw data down to the data you actually want.\nIn my case relevant data pertain only to the NZ Bird Atlas effort, and to complete checklists as only these provide the absence data required for occupancy modelling. The complexity of the data makes running auk to filter the raw data down slow, but it’s a one-time-only operation, so that’s OK, and now I have a 3.7 million row table, and we’re in business, no need to worry about setting anything up.\nThe problem comes when I have to blow those 3.7 million rows back up again for occupancy modelling to 110 million rows."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "href": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "title": "The joy of DuckDB",
    "section": "Wait, what? 110 million rows?!",
    "text": "Wait, what? 110 million rows?!\nYes, 110 million rows.\nHow it works is that each species (a little over 300) requires an entry in each complete checklist (around 360,000 of these) recording whether that species was observed (present) or not (absent) in that checklist. That results in 110 million row table. You can keep the two tables separate but then you have to keep joining them every time you go to use them. And if you save the 110 million table to disk it takes up around 8GB, and also takes a noticeable length of time to open and close for analysis in R.\nIt was about this time that I thought I should consider my options."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "href": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "title": "The joy of DuckDB",
    "section": "Enter DuckDB",
    "text": "Enter DuckDB\nDuckDB is an easy to install columnar database that you can drive using :SQL. It’s particularly easy to install because the R and Python APIs come bundled with the database itself. So, if you don’t want to, you don’t even have to install it, your platform of choice will do that for you.\nAnyway, if you do install it, which I did, so I could poke it around a little before going further, then to start it up from the command line type\n% duckdb\nAnd if you want to create a new database in the folder you are running from then it’s\n% duckdb my-new-database.db\nThe file extension is optional. Once in the session you can stash an existing CSV file in the database as a table with the command (D is the DuckDB command line prompt):\nD CREATE TABLE letters AS FROM 'letters.csv';\nand to see the results of your handiwork:\nD SELECT * FROM letters;\n┌─────────┬───────┬─────────┐\n│ column0 │  id   │ letter  │\n│  int64  │ int64 │ varchar │\n├─────────┼───────┼─────────┤\n│       1 │     1 │ a       │\n│       2 │     2 │ b       │\n│       3 │     3 │ c       │\n│       4 │     4 │ d       │\n│       5 │     5 │ e       │\n│       6 │     6 │ f       │\n│       7 │     7 │ g       │\n│       8 │     8 │ h       │\n│       9 │     9 │ i       │\n│      10 │    10 │ j       │\n├─────────┴───────┴─────────┤\n│ 10 rows         3 columns │\n└───────────────────────────┘\nD \nSatisfied it was this easy, I typed .exit to shut DuckDB down and moved on to consider how to use DuckDB from R. I should mention at this point that I’ve bounced off PostgreSQL a couple of times in the past when considering using it in classroom situations because it’s just not as easy to get into as this."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "href": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "title": "The joy of DuckDB",
    "section": "DuckDB in R",
    "text": "DuckDB in R\nThe R package you need is duckdb, so\n\ninstall.packages(\"duckdb\")\nlibrary(duckdb)\n\nand you are ready to go (no other installation of DuckDB required).\nNow if you have a giant dataframe called say my_giant_df, that you need to deal with, open a connection to a new database (or an existing one if you’ve been here before) with\n\ncon &lt;- dbConnect(duckdb(), \"my-giant-dataframe.db\")\n\nand write your dataframe into it as a table called giant_df with\n\ndbWriteTable(con, \"giant_df\", my_giant_df)\n\nIf that’s all you plan on doing then you should shut down the connection\n\ndbDisconnect(con, shutdown = TRUE)\n\nWhen I did this I was agreeably surprised to find that my 8GB file had shrunk down to a mere 750MB.\nBut there’s more. The reason I went down this route at all is that I generally only want to work with the data for one bird species at a time — data which come in handy packets of only 360,000 rows or so. Here’s how that works in practice. First open a connection to the database\n\ncon &lt;- dbConnect(duckdb(), dbdir = str_glue(\"the-birds.db\"))\n\nThe database has a table called observations containing the aforementioned 110 million rows. Each row includes among other things the common_name of a bird. We can get a vector containing those using\n\ncommon_names &lt;- dbGetQuery(con, \"SELECT DISTINCT common_name FROM observations\") |&gt;\n  pull(common_name) |&gt;\n  sort()\n\nNow we can iterate over each species by doing\n\nfor (common_name in common_names) {\n  sql_name &lt;- str_replace_all(common_name, \"'\", \"''\")\n  query &lt;- str_glue(str_glue(\"SELECT * FROM observations WHERE common_name = '{sql_name}'\"))\n  this_bird_df &lt;- dbGetQuery(con, query)\n  # ...\n  # do stuff with this_bird_df\n  # ...\n}\n\nThe only wrinkles here, for those paying attention, are using str_glue from the stringr package to form the SQL query I need, and related to that a str_replace_all to double up any single-quotes ' that happen to appear in those common names to '' so that they can be passed into an SQL query.\nIn general you query the database using dbGetQuery(&lt;connection&gt;, &lt;SQL&gt;) and you can execute a command with dbExecute(con, &lt;command&gt;).\nOf course, you have to know a bit of SQL, but for this kind of simple (local) data warehousing, there’s nothing you are likely to need that a quick google DuckDuckGo search won’t unearth.\nThis approach enabled me to iterate over all 300 species in the data and assemble a ‘mini-atlas’ of ggplot maps of each bird’s range in under a minute (snippet below), which is about how long it was previously taking R just to open the 8GB CSV file. Not to mention that the giant data table is never in working memory, only the chunks I need one at a time.\n\n\n\nYes, of course, Mallard is there for a reason\n\n\nIt’s safe to say, I’ll be using DuckDB a lot in many projects to come. There seem to be some wrinkles in relation to handling spatial data, specifically from R’s sf package but there’s a package for that,2 and it’s nothing to get too alarmed about."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "href": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "title": "The joy of DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA very nerdy deep cut from the Cornell Ornithology Lab. Respect.↩︎\n‘We have a package for that’ should be R’s tagline.↩︎"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html",
    "title": "Into the (LiDAR) void",
    "section": "",
    "text": "Last weekend the tables were turned on me by my elder son. Back when we lived in California, I dragged my two boys out on day hikes around the Bay Area much to their disgust (they had video games they’d rather have spent that time playing).\nBut lo and behold, a decade on and son #1 is planning on doing the :Te Araroa Trail next year sometime. Towards that end he’s doing a lot of overnight tramps and steadily working up to longer and more difficult expeditions (he’s done the :Tongariro Crossing and :Routeburn Track in the last couple of months, with another one coming up soon).\nAnyway, back to his revenge. I haven’t done any serious tramping lately, but he took me on an overnight trip to the Waiopehu Hut in the :Tararua Range.\nTo cut a long story short and answer the question in my subtitle: I’m afraid of the Tararuas! OMG they’re gnarly. In all honesty, my son overestimated my abilities and although I made it in and back out, several days later I am still feeling it!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#so-where-was-this-tramp",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#so-where-was-this-tramp",
    "title": "Into the (LiDAR) void",
    "section": "So where was this tramp?",
    "text": "So where was this tramp?\nHaving admitted that I have a healthy respect for the Tararuas, I was disappointed to find in putting together this post, that they are a LiDAR deadzone for some reason. I’d hoped I could explore the fractal nature of our tramp from the comfort of my keyboard, but to do that well I’d need more precise elevation data than are available.\nI’ve shown the LiDAR coverage available from LINZ overlaid on a LINZ Topo 50 map below. You can get the map data from here and of course there are also useful apps based on that layer. We also found the Organic Maps app which I have on my phone for everyday use, and which is based solely on OpenStreetMap data, to be very good.1\n\ntopo_map &lt;- rast(\"_data/nz-topo50.png\")\ncrs(topo_map) &lt;- st_crs(2193)$wkt\nlidar_coverage &lt;- st_read(\"_data/linz-lidar-available.gpkg\")\npath &lt;- st_read(\"_data/path.gpkg\") |&gt; st_reverse()\n\n\ntm_shape(topo_map) +\n  tm_rgb() +\n  tm_shape(lidar_coverage) +\n  tm_fill(fill = \"forestgreen\", fill_alpha = 0.5) +\n  tm_shape(path) +\n  tm_lines(col = \"black\", lwd = 3) +\n  tm_scalebar(position = tm_pos_out(\"center\", \"bottom\"))\n\n\n\n\n\n\n\n\nWithout LiDAR data exploring the fractal nature of the ups and downs of the trail is a bit of a non-starter. The reason I wanted to look at it that way is that one of the most salient features of this track was just how much ‘down’ there is mixed in with the ‘up’. It’s a little bit ridiculous how much you end up resenting the downhill stretches on the way up, when you know you’re shortly going to have to pay for them with more uphill. Not only that, in terrain this rugged, going downhill might use less energy than going uphill, but it demands just as much concentration, if not more!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#a-transect-of-our-travails",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#a-transect-of-our-travails",
    "title": "Into the (LiDAR) void",
    "section": "A transect of our travails",
    "text": "A transect of our travails\n\ndem &lt;- rast(\"_data/waiopehu-NZDEM_SoS_v1-0_14.tif\")\nhillshade &lt;- rast(\"_data/hillshade.tif\")\nwaterways &lt;- st_read(\"_data/waterways.gpkg\")\n\nAnyway, here’s another map, assembled from data variously obtained from the Otago School of Surveying NZSoSDEMv1 15m digital elevation model and OpenStreetMap. I made the hillshade layer in QGIS. The School of Surveying DEM is the best available for ‘analysis’ purposes in this area that I’m aware of. LINZ offer an 8m product, but are careful to say it’s only useful for cartographic visualization. As it turned out, that’s all I ended up doing, but never mind, the 15m product is more than adequate for making a transect of the track.\n\ntm_shape(dem) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\")) +\n  tm_shape(waterways) +\n  tm_lines(col = \"#89ddff\",\n           lwd = \"waterway\",\n           lwd.scale = tm_scale_categorical(\n             values = c(0.5, 1), levels = c(\"stream\", \"river\"))) +\n  tm_shape(hillshade) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.greys\"),\n            col_alpha = 0.15) +\n  tm_shape(path) +\n  tm_lines(col = \"black\", lwd = 3) +\n  tm_layout(legend.show = FALSE)\n\n\n\n\n\n\n\n\nSo here’s one approach to making a transect.\nFirst we have to take the path, convert it to a series of points and extract elevation values from the DEM at those points. I’ve used just the points along the linestring rather than interpolating along its length. How that would look at different resolutions were LiDAR data available would be interesting to explore.\n\n# function to return data for plotting a transect across a DEM along a line\nget_transect &lt;- function(dem, line) {\n  pts &lt;- line |&gt; \n1    st_cast(\"POINT\") |&gt;\n2    st_coordinates() |&gt;\n    as_tibble() |&gt;\n3    st_as_sf(coords = 1:2, crs = st_crs(line))\n4  names(dem) &lt;- \"z\"\n  transect &lt;- extract(dem, pts |&gt; as(\"SpatVector\"), \n5                      method = \"bilinear\", xy = TRUE) |&gt;\n    as_tibble() |&gt;\n6    mutate(dx = replace_na(x - lag(x), 0),\n           dy = replace_na(y - lag(y), 0),\n           dz = replace_na(z - lag(z), 0),\n           dxy = sqrt(dx ^ 2 + dy ^ 2),\n7           distance = cumsum(dxy),\n           Ascent = cumsum(if_else(dz &gt; 0, dz, 0)),\n           Descent = cumsum(if_else(dz &lt; 0, dz, 0)),\n8           downhill_start = dz &gt;= 0 & lead(dz) &lt; 0,\n           downhill_end = dz &gt;= 0 & lag(dz) &lt; 0) |&gt;\n    select(-dx, -dy, -dz, -dxy)\n}\n\nxyz &lt;- dem |&gt; get_transect(path)\n\n\n1\n\nFirst convert the LINESTRING to POINTs.\n\n2\n\nExtract the point coordinates.\n\n3\n\nConvert back to an sf dataset.\n\n4\n\nMake sure the height variable in the DEM is called z.\n\n5\n\nThe terra::extract function extracts values from the DEM at the supplied points (which have to be converted to a SpatVector layer for this to work). xy = TRUE ensures that the x, y coordinates are also retained in the output.\n\n6\n\nThe lag function allows us to calculate differences between consecutive values in the data frame. The first difference will come out as a NA result (since there is no value before the first row), so we use replace_na to set that result to 0.\n\n7\n\nUse cumsum to calculate a elapsed distance from the start, and also total ascent and descent determined separately.\n\n8\n\nDetecting when downhill sections of the trail start and end.\n\n\n\n\nFor plotting purposes it’s easier if we pull the data apart into two data frames, one with the cumulative ascent and descent data tagged as such, the other with them retained, and with a constant label added. This is mostly so that we can convince ggplot to give us two items in the plot legend!\n\nup_down &lt;- xyz |&gt; \n  select(distance, Ascent, Descent) |&gt;\n  pivot_longer(cols = c(Ascent, Descent))\n\ntransect &lt;- xyz |&gt;\n  select(distance, Ascent, Descent) |&gt;\n1  mutate(height = Ascent + Descent,\n2         gain = as.factor(c(\"Gain\")))\n\n\n1\n\nBecause descent values are negative we just add Ascent and Descent to get the net climb.\n\n2\n\nThe gain variable is constant, but used here so that it can be picked up by scale_colour_manual which persuades ggplot to produce a legend item for it.\n\n\n\n\nFinally we can make a plot.\n\ng &lt;- ggplot() + \n  geom_area(\n    data = up_down, \n    aes(x = distance, y = value, group = name, fill = name),\n        colour = NA, position = \"identity\", alpha = 0.35) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Up or down\") +\n  geom_line(\n    data = transect, \n    aes(x = distance, y = height, colour = gain)) +\n  scale_colour_manual(name = \"\", values = \"black\", labels = \"Net gain\") +\n  coord_cartesian(expand = FALSE) + \n  labs(x = \"Elapsed distance, m\", y = \"Height gain from start, m\")\n\ng + theme_minimal()\n\n\n\n\n\n\n\n\nI have split the climb out like this, because as I said above, some of the more emotionally uh… challenging parts of this trip were the sections going downhill when our overall course was up, and I wanted to see just how much of that there was.\nIt looks like a pretty relentless uphill tramp. But there really are quite a few sections of downhill. To emphasise this we can add to the plot, using the downhill start and end flags in the data.\n\ndownhill_starts &lt;- xyz |&gt;\n  filter(downhill_start) |&gt;\n  pull(distance)\n\ndownhill_ends &lt;- xyz |&gt;\n  filter(downhill_end) |&gt;\n  pull(distance)\n\ndownhill_sections &lt;- tibble(\n  dmin = downhill_starts,\n  dmax = downhill_ends)\n\ng + geom_rect(data = downhill_sections, \n              aes(xmin = dmin, xmax = dmax, \n                  ymin = min(xyz$Descent), \n                  ymax = max(xyz$Ascent)),\n              fill = \"#00000030\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnd now we can see that while many of those ‘downhill’ sections on the way up are really fairly level, there are a lot of them!\nAll in all, I’m not sure I’d recommend it as a first long hike back after a (too) long break. Maybe I’ll go back next year and see if I’ve gotten any fitter.\nMeanwhile, here’s a photograph looking further to the east from the hut when we finally got there. Lighting conditions were a bit challenging for my phone’s camera, but all in all… very much worth it, even if I won’t be hurrying back right away!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#footnotes",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#footnotes",
    "title": "Into the (LiDAR) void",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut take my advice: don’t take my advice on anything to do with tramping.↩︎"
  },
  {
    "objectID": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "href": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "title": "What three chords",
    "section": "",
    "text": "I don’t know if this needs or deserves any further explanation, but SERIOUSLY and I can’t emphasise this enough, turn down the volume on your device before clicking the button.\nWhere am I?"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "title": "Random points on the globe revisited",
    "section": "",
    "text": "NOTE: There were a few over-simplifications in the original version of this post, which I’ve done my best to correct. Turns out point patterns on the globe are even more complicated than I realised…\nThis holiday season I had reason to revisit a post from November 2021 about generating evenly distributed random points on the globe. This was in the course of writing (for fun) a NetLogo simulation of the boardgame Lacuna. You can find the simulation here. The simulation isn’t very developed and it turns out that the web version of NetLogo doesn’t implemented some functions important to its operation, so pending developing my Javascript skills further that’s likely where it will remain.\nAnyway, the setup rules for the physical version of Lacuna stipulate that the players should\nWriting the code for this immediately had me reaching for a sequential spatial inhibition process using spatstat in R, or for that matter an implementation of it in NetLogo1 This also reminded me that spatial inhibition processes require an inhibition distance to be specified, and got me wondering if there is a spatial point process that generates randomly distributed points ‘without clumping’ but with no need to specify a spacing parameter.\nThis is what is known as a rabbit hole.\nFor whatever reason, there doesn’t seem to be a widely used spatial point process model with this property, but there are :low discrepancy sequences most often used for balanced sampling in operations like Monte Carlo simulation that can generate evenly spaced spatial patterns in two dimensions. In this post, I look at a couple of these along with some other alternatives, in the context set by my earlier post of generating random point patterns of even intensity on the globe."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "title": "Random points on the globe revisited",
    "section": "Some preliminaries",
    "text": "Some preliminaries\nTo make things easier, I am going to generate patterns in latitude-longitude space, and then transforming them to patterns on the globe. Before we get started here are all the libraries I’m using.\n\nlibrary(rnaturalearth)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\n1library(spbal)\n2library(spatstat)\nlibrary(ggplot2)\nlibrary(cols4all)\n\ntheme_set(theme_minimal())\ntheme_update(axis.title = element_blank())\n\n3set.seed(1)\n\nprojection &lt;- \"+proj=moll\"\naspect_ratio &lt;- 2\nR &lt;- 6371000\nn_points &lt;- 1152\n\n\n1\n\nspbal provides Halton sequences.\n\n2\n\nspatstat provides a wide range of spatial point processes.\n\n3\n\nFor replicability.\n\n\n\n\nAnd we’ll set up a globe polygon and a world map.\n\nangles &lt;- 0:719 / 720 * 2 * pi\nangles &lt;- c(angles, angles[1])\nx &lt;- R * cos(angles) * 2 * sqrt(aspect_ratio)\ny &lt;- R * sin(angles) * 2 / sqrt(aspect_ratio)\n\nglobe &lt;- c(x, y) |&gt; \n  matrix(ncol = 2) |&gt;\n  list() |&gt;\n  st_polygon() |&gt;\n  st_sfc() |&gt;\n  st_as_sf(crs = projection)\n\nworld &lt;- ne_countries() |&gt;\n  select() |&gt;\n  st_transform(projection)\n\nAnd make a quick map to make sure everything is in order.\n\n\n\n\nggplot() +\n  geom_sf(data = globe, fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0)\n\n\n\n\n\n\n\n\n\n\nFigure 1: World map in Mollweide"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "title": "Random points on the globe revisited",
    "section": "Patterns in latitude-longitude space",
    "text": "Patterns in latitude-longitude space\n\nA simple random pattern\nWe can make this by drawing x and y coordinates from uniform distributions.\n1plot_pp &lt;- function(df) {\n  ggplot() +\n    geom_point(data = df, aes(x = x, y = y)) +\n    coord_equal(xlim = 180 * c (-1, 1),\n2                ylim = 90 * c(-1, 1), expand = FALSE) +\n3    theme(panel.background = element_rect(fill = NA))\n}\npattern1 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = runif(n_points) * 180 - 90,\n                   generator = \"Uniform random\")\nplot_pp(pattern1)\n\n\n1\n\nConvenient to have a single function for plotting point patterns.\n\n2\n\nexpand = FALSE stops ggplot adding a margin beyond the limits we’ve set.\n\n3\n\nIt’s good to be able to see the bounds, and theme_minimal would not normally display a frame.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simple uniform random pattern\n\n\n\n\n\nSimple random pattern with a cosine correction for latitude\nNext up the same pattern with the cosine correction from my earlier post.\n\n\n\n\npattern2 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = acos(runif(n_points, -1, 1)) / pi * 180 - 90,\n                   generator = \"Uniform random cosine-corrected\")\nplot_pp(pattern2)\n\n\n\n\n\n\n\n\n\n\nFigure 3: Uniform random pattern with cosine correction\n\n\n\nIn later patterns I have applied this cosine correction where the underlying process generates uniformly distributed y coordinates.\n\n\nA pattern from a spatial point process\nMy first thought here was to use a sequential spatial inhibition process. This is a spatial point process where points are generated at random locations, but rejected if they are closer than some inhibition distance to an existing point already in the pattern. This is easily done in Euclidean space using the spatstat::rSSI function. In latitude-longitude space this won’t work because distances are not calculable using the Pythagorean function on coordinates.\nInstead, I have opted for the uniform point process (just as above), but specifying a tiling (a tessellation) of the space with approximately equal-area tiles. Using the formula for the y coordinate of a cylindrical equal-area projection \\(y=sin\\phi\\), we can generate approximately equal-area rectangles in latitude-longitude space as below. I emphasise the approximation here, because it is only approximate. Rectangles in lat-lon space are not rectangles on the globe after all.\n1nx &lt;- sqrt(n_points / 2) * 2 + 1\nny &lt;- sqrt(n_points / 2) + 1\n2xg &lt;- seq(-1, 1, length.out = nx) * 180\n3yg &lt;- asin(seq(-1, 1, length.out = ny)) / pi * 180\n\npp &lt;- runifpoint(n = 1, win = tess(xgrid = xg, ygrid = yg))\npattern3 &lt;- pp |&gt;\n  as.data.frame() |&gt;\n  mutate(generator = \"Stratified point process\")\nplot_pp(pattern3) +\n  geom_point(data = expand_grid(xg, yg), aes(x = xg, y = yg), \n             colour = \"red\", shape = 3, size = 0.5)\n\n\n1\n\nThe number of gridlines we need in each direction is determined here.\n\n2\n\nx coordinates are trivially equally spaced.\n\n3\n\ny coordinates approximately equally spaced in the transformed space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Stratified random point process\n\n\n\nI’ve plotted the coordinates of the grid used to ‘thin’ the points at high latitudes for reference. The process generates only one point in each of these cells. However… the thinning is not continuous and there are a number of points very close to the poles, where of course the available area is zero!\n\n\nQuasi-random sequences\nThese are the processes that were new to me, which showed up when I started looking for R packages that could do even sampling in 2D spaces. The first package to show up was spacefillr. This implements a number of such sequences, but we’ll look here at the Halton sequence, because its workings are relatively easy to understand, and the implementation in the spbal package provides more options.\n:Halton sequences are generated using a pair of coprime numbers, i.e., two numbers with no common factors. The simplest example is 2 and 3. Each of the selected numbers specifies a sequence by repeated subdivision of the interval 0 to 1. So for 2 we get \\[\n\\frac{1}{2},\\frac{1}{4},\\frac{3}{4},\\frac{1}{8},\\frac{5}{8},\\frac{3}{8},\\frac{7}{8},\\ldots\n\\] and for 3 we get \\[\n\\frac{1}{3},\\frac{2}{3},\\frac{1}{9},\\frac{4}{9},\\frac{7}{9},\\frac{2}{9},\\frac{5}{9},\\ldots\n\\] Pairing these sequences gives us coordinates of points in the unit square. Different generating numbers can be chosen (provided they are coprime) and different starting points in each sequence can be paired, to give a wide variety of deterministically generated quasi-random patterns. The spbal package provides a highly configurable interface to generate such sequences using the cppRSHalton_br function. We can see the procedures inner workings clearly by examining the first few elements in the sequence:\n\ncppRSHalton_br(10, bases = 2:3, seeds = 0)$pts\n\n        [,1]       [,2]\n [1,] 0.0000 0.00000000\n [2,] 0.5000 0.33333333\n [3,] 0.2500 0.66666667\n [4,] 0.7500 0.11111111\n [5,] 0.1250 0.44444444\n [6,] 0.6250 0.77777778\n [7,] 0.3750 0.22222222\n [8,] 0.8750 0.55555556\n [9,] 0.0625 0.88888889\n[10,] 0.5625 0.03703704\n\n\nThese appear entirely regular, and some regularity is evident in the patterns generated (see Figure 5), although it is less apparent than might be anticipated on considering the numerical values alone. The main interest in Halton sequences is their desirable evenness of distribution for sampling purposes, which is apparent in Figure 5.\npattern4 &lt;- cppRSHalton_br(n_points, \n1                           bases = c(2, 3),\n2                           seeds = c(14, 21))$pts |&gt;\n  as.data.frame() |&gt;\n  rename(x = V1, y = V2) |&gt; \n  mutate(x = x * 360 - 180, \n         y = acos(y * 2 - 1) / pi * 180 - 90,\n         generator = \"Halton\")\nplot_pp(pattern4)\n\n\n1\n\nThe coprime generating values.\n\n2\n\nThe starting positions in the sequence for each generating value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Points generated by a Halton sequence\n\n\n\nDetails concerning generation of Halton sequences and their statistical properties are provided by Faure and Lemieux.2\n\n\nA ‘home-grown’ parameter-free pattern generator\nHere I use a seemingly trivial (but not very efficient!) algorithm to generate some home-made evenly distributed points, without the need to specify any (spatial) parameter like the inhibition distance required by rSSI. I found the inspiration for this in this detailed blog post about generating :blue noise, which was in turn based on an algorithm described by Mitchell.3\nMatters are complicated by having to calculate distances in latitude-longitude space, which we do using the :Haversine formula \\[\nd=2 r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\varphi_2 - \\varphi_1}{2}\\right) + \\cos(\\varphi_1) \\cos(\\varphi_2)\\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\n\\] for the distance between two lon-lat locations\\(\\left(\\lambda_1,\\varphi_1\\right)\\) and \\(\\left(\\lambda_2,\\varphi_2\\right)\\), although because we only need the relative distances we don’t use the \\(2r\\) scaling. The need to calculate toroidal distances described in the blogpost is obviated by calculating distances on the sphere which wraps in a similar way.\nThe simple idea of this algorithm is that each time we add a new point we generate a set of points (candidates) to choose from, and select the one with the largest minimum distance to an existing point in the pattern. Making the algorithm more efficient would involve only checking the distance to points known to be close to candidate points using some kind of spatial index or binning structure.\n\n1ll_distances &lt;- function(p1, p2) {\n  lon1 &lt;- p1[,1]; lat1 &lt;- p1[,2]\n  lon2 &lt;- p2[,1]; lat2 &lt;- p2[,2] \n  asin(sqrt(\n    sin(outer(lat1, lat2, \"-\") / 2) ^ 2 +\n    outer(cos(p1[, 2]), cos(p2[, 2]), \"*\") * \n      sin(outer(lon1, lon2, \"-\") / 2) ^ 2\n  ))\n}\n\nrescale &lt;- function(x, x1min, x2min, x1max, x2max) {\n  x2min + (x - x1min) / (x1max - x1min) * (x2max - x2min)\n}\n\nspaced_points &lt;- function(n = 50, choice_scaling = 1.5,\n                          input_bb = c(0, 0, 1, 1),\n                          output_bb = c(0, 0, 1, 1), dist_fn) {\n  points &lt;- c(runif(1, input_bb[1], input_bb[3]), \n              runif(1, input_bb[2], input_bb[4])) |&gt; \n    matrix(ncol = 2)\n  for (i in 1:(n-1)) {\n2    n_candidates &lt;- ceiling(log(i * exp(1) * choice_scaling))\n    candidates &lt;- c(runif(n_candidates, input_bb[1], input_bb[3]), \n                    runif(n_candidates, input_bb[2], input_bb[4])) |&gt; \n      matrix(ncol = 2)\n    r_max &lt;- dist_fn(candidates, points) |&gt; \n3      apply(1, min) |&gt;\n      which.max()\n    points &lt;- rbind(points, candidates[r_max, ])\n  }\n  points |&gt; \n    as.data.frame() |&gt;\n    rename(x = V1, y = V2) |&gt;\n    mutate(x = rescale(x, input_bb[1], output_bb[1], input_bb[3], output_bb[3]),\n           y = rescale(y, input_bb[2], output_bb[2], input_bb[4], output_bb[4]),\n           generator = \"Blue noise\")\n}\n\n\n1\n\nI use the outer function to do all pairwise differences and products of data in the two supplied matrices more efficiently than by nested loops.\n\n2\n\nThe choice_scaling parameter determines how rapidly the number of candidate points grows with the size of the existing data set. It should be strictly greater than 1 or no points will ever get added! Including a log factor stops the speed of the algorithm from falling too rapidly as points are added.\n\n3\n\nThe apply(1, min) operation finds the smallest distance in each row (i.e. distance to nearest neighbour in the existing set of points), and which.max() identifies the row with the largest minimum distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: A pattern generated using a ‘blue noise’ algorithm by iteratively choosing the most remote random additional point among a set of choices"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "title": "Random points on the globe revisited",
    "section": "Putting all these points on the globe",
    "text": "Putting all these points on the globe\nWe now apply a transformation from the lat-lon Cartesian coordinates to the globe that is equal-area as shown in the previous post. We can combine all the points into a single data set and use facet_wrap for side-by-side comparison.\n\n\n\n\ntransform_to_globe &lt;- function(df, proj) {\n  df |&gt;\n    st_as_sf(coords = c(\"x\", \"y\"), crs = 4326) |&gt; \n    st_transform(proj)  \n}\n\nall_points &lt;- bind_rows(pattern1, pattern2, pattern3, pattern4, pattern5) |&gt;\n  transform_to_globe(projection) |&gt;\n  mutate(generator = ordered(generator, c(\"Uniform random\", \n                                          \"Uniform random cosine-corrected\",\n                                          \"Stratified point process\",\n                                          \"Halton\", \"Blue noise\")))\nggplot(globe) +\n  geom_sf(fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0) +\n  geom_sf(data = all_points, size = 0.35, colour = \"red\") +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nFigure 7: Side-by-side comparison of five patterns projected onto the globe\n\n\n\nWith the obvious exception of the uniform random pattern, which exhibits definite clumping, all of these seem to do a reasonable job of producing a random arrangement of points evenly distributed over the globe.\nFrom the perspective of avoiding clumping we can try to make the comparison slightly more precise by making hexbin density plots of the points.\nxy &lt;- all_points |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  bind_cols(all_points)\n\nnx &lt;- sqrt(n_points / pi) * 2 * sqrt(aspect_ratio)\nny &lt;- nx * 2 / sqrt(3) / sqrt(aspect_ratio)\n\nggplot(xy) +\n  geom_hex(aes(x = X, y = Y, fill = as.factor(after_stat(count))), \n           bins = c(nx, ny), linewidth = 0.1, colour = \"#666666\") +\n1  scale_fill_manual(values = c4a(\"brewer.yl_gn_bu\", 11),\n                    breaks = 1:10, guide = \"legend\", name = \"#Points\") +\n  annotate(geom = \"path\", x = x, y = y, linewidth = 0.2) +\n  coord_equal() +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n1\n\nggplot2::geom_hex insists that the point counts are ‘continuous’ which is what forces me to convert at a factor, so that I can use a legend not a colour ramp here. Another instance of ggplot2’s strong preference for not classing data, see this recent post in the context of choropleth maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Side-by-side comparison of hexbin density plots of the five point patterns\n\n\n\nThe uniform random pattern clearly exhibits the most uneven distribution of points. The cosine-corrected version is better although there is still unevenness in the mid-latitudes, as we would expect because there is no interaction between points in the pattern: the presence of a previous point does not block another point showing up close by. The stratified point process and Halton patterns are better again. The latter of these is particularly surprising, given its completely deterministic generative process.\nSomewhat to my surprise my ‘homebrew’ blue noise compares well with the other four patterns for evenness of distribution. This is particularly nice given that it requires no spatial parameter to be supplied, only one that (perhaps only marginally) affects the quality of the patterns produced."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "title": "Random points on the globe revisited",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt is intriguing to me that blue noise is seemingly not a standard spatial point process. Unlike sequential spatial inhibition to which it is similar it requires no spatial parameter to be tuned to get a desired result, and it cannot fail to produce the requested number of points. It is also a reasonable plausible process from a ‘mechanism’ perspective. Imagine for example retailers considering premises in which to set up shop and examining a number of different sites, then choosing the one farthest from any potential competitor. It’s at least as compelling in that respect as a model of the outcome of competition between event locations as SSI.\nSince it seems useful, I provided some ‘hooks’ in the implementation above to allow use of different distance functions and ‘windows’ for point generation. Here’s an example in a simple Euclidean space, using :Manhattan distance to determine the best new point among candidates.\n\n\n\n\nabs_diffs &lt;- function(v1, v2) {\n  outer(v1, v2, \"-\") |&gt; abs()\n}\n\nmanhattan_distances &lt;- function(p1, p2) {\n  dx &lt;- abs_diffs(p1[, 1], p2[, 1])\n  dy &lt;- abs_diffs(p1[, 2], p2[, 2])\n  dx + dy\n}\n\nspaced_points(n_points, dist_fn = manhattan_distances) |&gt;\n  ggplot() +\n    geom_point(aes(x = x, y = y)) +\n    coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 9: A blue noise pattern in Euclidean space\n\n\n\nAn interesting rabbit hole indeed!"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "title": "Random points on the globe revisited",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHaving said that, the circular game board eventually led me to generate the patterns in NetLogo by randomly displacing each new flower from the centre of the circle by a random distance in a random direction, and retrying until they were no closer to another flower than some set distance.↩︎\nFaure H and C Lemieux. 2009. Generalized Halton sequences in 2008: A comparative study. ACM Transactions on Modeling and Computer Simulation 19(4) 15:1-15:31.↩︎\nMitchell DP. 1991. Spectrally optimal sampling for distribution ray tracing. SIGGRAPH Computer Graphics. 25(4) 157–164.↩︎"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html",
    "title": "ALGIM 2024",
    "section": "",
    "text": "What is ALGIM? According to the website,\nand\nSo ALGIM is a place where local government ICT people can pool resources and expertise to make the challenging business of doing local government a little bit easier. To that end ALGIM offers training, webinars, forums for the exchange of ideas. One of those forums is the annual conference.2"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "title": "ALGIM 2024",
    "section": "An industry conference not an academic conference",
    "text": "An industry conference not an academic conference\nALGIM’s annual conference is a showpiece event. This year the conference theme was ‘Driving collaboration’, approached from a range of perspectives. Topics such as AI in local government, standardisation of processes and plaforms, and fostering collaboration loomed large. There were interesting keynotes from an eclectic mix of people such as :Audrey Tang, Nicole Skews-Poole, Julian Moore, and Michael Baker.\nBut the ALGIM conference is certainly not an academic conference. I’m a veteran of those having attended 50 or so in the last two decades, and if you add in symposia, workshops, and sundry other varieties of academic meeting, a few more even than that. The rhythms of these are familiar: 20 minute talks in sessions an hour or two long, with breaks for refreshments, and (for the most part) not much connection to the world of work beyond the meeting. Obviously everyone participating is at work, and people gossip about work in the breaks, but the subject matter under discussion in sessions is not work itself, but scientific and scholarly questions of one kind or another.\nAnyway, this was not that, but a rather different kind of event. It’s very much a networking meeting for people with shared professional interests3 and as such absolutely about work. I do have some previous experience of this kind of event4 and knew going in that in addition to keynotes and shorter presentations on more specialised topics, many technology vendors would be there hawking their wares, or perhaps more realistically simply reminding local government ICT managers that they exist, should they be considering revisiting their backup, comms, data security, disaster recovery, online payment systems, or whatever other ICT options."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "title": "ALGIM 2024",
    "section": "Wot? No GIS?",
    "text": "Wot? No GIS?\nAnd that, to a ‘GISer’, was the most interesting aspect. I fully expected to see all the usual local geospatial suspects here. Esri’s local representative Eagle Technology was present, albeit not in force (mostly they were handing out ice cream!), along with one or two others, such as GBS and local FME partner Seamless, but that was more or less it. No Koordinates, Catalyst, Orbica, Lynker Analytics, or LINZ.\nNow… that might be because (i) nobody goes to a conference to buy enterprise GIS, or even to change their enterprise GIS plans, or (ii) Esri/Eagle’s position in GIS in local government in New Zealand is so strong that there isn’t much point in anybody else showing up. On the other hand, it might be because… well, because however often geospatial folks tell themselves (and anyone who will listen) that 80%—or some other high percentage!—of all data are spatial, and no matter how self-evidently spatial it seems that the job of local government might be, when it comes to information management in local government, GIS is not much of a thing.\nAnd yet, GIS clearly is important in local government, even if it is treated as slightly peripheral by ICT managers."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "title": "ALGIM 2024",
    "section": "What was I doing there anyway?",
    "text": "What was I doing there anyway?\nI was there to chair a panel discussion on current challenges/developments in geospatial (AI, digital twins, barriers to collaboration, convergence with data analytics, education and training), and to convene a ‘GIS Round Table discussion’.\nBoth sessions were interesting, although the panel discussion time frame was a little too short at only half an hour to really get into things with the excellent contributors Andrew Steffert from Horizons Regional Council, Kerri Gray from CreateBig, and Scott Campbell from Eagle.\nBut with all due deference to those experts, the Round Table was, I thought, more telling. It wasn’t very big with about a dozen in attendance, but it was fairly representative with ‘GIS officers’ (for want of a better term) from councils big and small. The striking thing was how quickly even this very limited forum became a source of support and consultation among the people around the table. The strong sense I got was of GIS officers overwhelmed by the numerous expectations around their role(s). There was also a sense, especially from smaller councils where there might be only one or two people ‘doing GIS’, that those people feel isolated with limited options to reach out for assistance. And so, within minutes of introductions, people were comparing notes on how others had approached various common tasks and what technologies or data resources they were using. There really is nothing quite like getting people in a room together for learning what their challenges are.\nIn short, however tacked on that “and GIS” might appear in ALGIM’s mission statement, the GIS officers in local government around New Zealand (and I imagine also Australia) seem more than ready to take advantage of whatever forums ALGIM might be able to offer for learning, exchanging ideas, sharing approaches, and collaborating. Hopefully, this can happen soon, since it doesn’t seem like the job of GIS officer in local government is going to get any easier any time in the near future!"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "title": "ALGIM 2024",
    "section": "PS Not forgetting",
    "text": "PS Not forgetting\n\n\n\nThe Hamilton City Kai Map\n\n\nI would be remiss if I didn’t also mention seeing some really nice presentations of award nominated geospatial projects such as the Hamilton City Kai Map, Northland Regional Council’s use of geospatial to integrate the Health and Safety processes associated with their numerous fieldwork activities, and a Christchurch City map-centred streamling of their building warrant of fitness process (i.e., building inspections). The latter two are internal work process streamlining projects so unfortunately don’t have public websites I can link to."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "title": "ALGIM 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomebody always has to come last, but park that slightly tacked on feeling “and GIS” just for now…↩︎\nALGIM now also has an Australian branch, which will see the organisation grow substantially in the next few years.↩︎\nAcademic conferences are also that, but with a different emphasis.↩︎\nAs I’ve recounted elsewhere it was at a meeting like this that I chanced upon GIS. If you’re interested it’s mentioned about 2 minutes into this podcast.↩︎"
  },
  {
    "objectID": "training/02-geographical-computing.html",
    "href": "training/02-geographical-computing.html",
    "title": "Geographical computing",
    "section": "",
    "text": "Go to the materials\nThese pages outline a one semester (36 contact hours) class in python programming for geospatial that was last taught at Victoria University of Wellington as GISC 420 in the first half of 2022.\nI am still in the process of cleaning the materials up for potential conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to python programming for geospatial but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A similar list is available at my ORCiD profile. You can also see them in citational context at my google scholar page, which is reasonably accurate. Links below are generally to official publisher sources, and are often paywalled. Open access copies are linked from my University of Auckland profile page. If you would like a copy of something in particular and can’t access it, then get in touch.\nJump to books, papers, book chapters, dissertations"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books\nO’Sullivan D. 2024. Computing Geographically: Bridging Giscience and Geography (Guilford Press: New York).\nO’Sullivan D. 2017. Section Editor for ‘Fundamentals of GIScience’ (32 articles) in The International Encyclopedia of Geography: People, the Earth, Environment, and Technology. Richardson, D. (ed). New York: Wiley.\nMiller JA, D O’Sullivan and Wiegand N eds. 2016. Geographic Information Science: Proceedings of the 9th International Conference, GIScience 2016 Lecture Notes in Computer Science Vol. 9927 (Springer: Cham, Switzerland).\nO’Sullivan D and GLW Perry. 2013. Spatial Simulation: Exploring Pattern and Process (Wiley: Chichester, UK).\nO’Sullivan D and DJ Unwin. 2010. Geographic Information Analysis 2nd edn. (Wiley: Hoboken, NJ).\nO’Sullivan D and DJ Unwin. 2003. Geographic Information Analysis (Wiley: Hoboken, NJ)."
  },
  {
    "objectID": "publications.html#in-refereed-journals",
    "href": "publications.html#in-refereed-journals",
    "title": "Publications",
    "section": "In refereed journals",
    "text": "In refereed journals\nMahmoudi D, J Thatcher, LB Imaoka, and D O’Sullivan. Online first. From FOSS to profit: Digital spatial technologies and the mode of production. Digital Geography and Society. 100101. doi: 10.1016/j.diggeo.2024.100101.\nO’Sullivan D. 2024. Environment and Planning B and me; or what is lost in data. Environment and Planning B: Urban Analytics and City Science 51(5) 1045–1048. doi: 10.1177/23998083241246320.\nEtherington TR, D O’Sullivan, GLW Perry, DR Richards, and J Wainwright. 2024. A least-cost network neutral landscape model of human sites and routes. Landscape Ecology 39(3) 52. doi: 10.1007/s10980-024-01836-w.\nAntosz P, D Birks, B Edmonds, A Heppenstall, R Meyer, JG Polhill, D O’Sullivan and N Wijermans. 2023. What do you want theory for? A pragmatic analysis of the roles of “theory” in agent-based modelling. Environmental Modelling & Software 168 105802. doi: 10.1016/j.envsoft.2023.105802\nLester PJ, D O’Sullivan and GLW Perry. 2023. Gene drives for invasive wasp control: Extinction is unlikely, with suppression dependent on dispersal and growth rates. Ecological Applications 33(7) e2912. doi: 10.1002/eap.2912\nBergmann L, LF Chaves, D O’Sullivan and RG Wallace. 2023. Dominant Modes of Agricultural Production Helped Structure Initial COVID-19 Spread in the U.S. Midwest. ISPRS International Journal of Geo-Information 12(5) 195. doi: 10.3390/ijgi12050195\nEtherington T, F Morgan, D O’Sullivan. 2022, Binary space partitioning generates hierarchical and rectilinear neutral landscape models suitable for human-dominated landscapes. Landscape Ecology 37(7) 1761–1769. doi: 10.1007/s10980-022-01452-6\nChaves LF, MD Friberg, LA Hurtado, RM Rodríguez, D O’Sullivan and LR Bergmann. 2022. Trade, uneven development and people in motion: Used territories and the initial spread of COVID-19 in Mesoamerica and the Caribbean. Socio-Economic Planning Sciences 80 (March) 101161. doi: 10.1016/j.seps.2021.101161\nGibadullina A, LR Bergmann and D O’Sullivan. 2021. For Geographical Network Analysis. Tijdschrift voor Economische en Sociale Geografie 112(4) 482-487. doi: 10.1111/tesg.12489.\nO’Sullivan D. 2021. New mappings of GIScience and geography. A commentary on May Yuan’s ‘GIS research to address tensions in geography.’ Singapore Journal of Tropical Geography 42(1) 31–35. doi: 10.1111/sjtg.12345\nO’Sullivan D. 2021. Things are how they are because of how they got that way: Thoughts from the beach, on 50 years of Geographical Analysis. Geographical Analysis 53(1) 157–163. doi: 10.1111/gean.12225\nFranklin, RS, V Houlden, C Robinson, D Arribas-Bel, EC Delmelle, U Demšar, HJ Miller, and D O’Sullivan. 2021. Who counts? Gender, gatekeeping, and quantitative human geography. The Professional Geographer 73(1) 48–61. doi: 10.1080/00330124.2020.1828944\nO’Sullivan D, M Gahegan, DJ Exeter and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967–1000. doi: 10.1111/tgis.12660\nPayne WB and D O’Sullivan. 2020. Exploding the phone book: Spatial data arbitrage in the 1990s Internet boom. Annals of the American Association of Geographers 110(2) 391–398. doi: 10.1080/24694452.2019.1656999\nManson S, L An, KC Clarke, A Heppenstall, J Koch, B Krzyzanowski, F Morgan, D O’Sullivan, BC Runck, E Shook and L Tesfatsion. 2020. Methodological Issues of Spatial Agent-Based Models. Journal of Artificial Societies and Social Simulation 23(1) 3. doi: 10.18564/jasss.4174\nChristophers B and D O’Sullivan. 2019. Intersections of inequality in homeownership in Sweden. Housing Studies 34(6) 897-924. doi: 10.1080/02673037.2018.1495695\nMavoa S, N Bagheri, MJ Koohsari, AT Kaczynski, KE Lamb, K Oka, D O’Sullivan and K Witten. 2019. How do neighbourhood definitions influence the associations between built environment and physical activity? International Journal of Environmentalal Research and Public Health 16. doi: 10.3390/ijerph16091501\nO’Sullivan, D 2019. Untangling knots: Thoughts on Wilson’s New Lines. Transactions in GIS 32(1) 168-169. doi: 10.1111/tgis.12502\nLiu C, D O’Sullivan and GLW Perry. 2018. The rent gap revisited: gentrification in Point Chevalier, Auckland. Urban Geography 39(9) 1300-1325. doi: 10.1080/02723638.2018.1446883\nPerry GLW and D O’Sullivan. 2018. Identifying narrative descriptions in agent-based models representing past human-environment interactions. Journal of Archaeological Method and Theory, 25(3) 795-813. doi: 10.1007/s10816-017-9355-x\nMavoa S, K Lamb, D O’Sullivan, K Witten and M. Smith. 2018. Are disadvantaged children more likely to be excluded from analysis when applying global positioning systems inclusion criteria? BMC Research Notes, 11 578. doi: 10.1186/s13104-018-3681-2\nMahdavi Ardestani B, D O’Sullivan, and P Davis. 2018. A multi-scaled agent-based model of residential segregation applied to a real metropolitan area. Computers, Environment and Urban Systems, 69 1-16. doi: 10.1016/j.compenvurbsys.2017.11.002\nBergmann LR and D O’Sullivan. 2018. Reimagining GIScience for relational spaces. The Canadian Geographer / Le Géographe canadien. 62(1) 7-14. doi: 10.1111/cag.12405\nGetz WM, CR Marshall, CJ Carlson, L Giuggioli, SJ Ryan, SS Romañach, C Boettiger, SD Chamberlain, L Larsen, P D’Odorico, D O’Sullivan. 2018. Making ecological models adequate. Ecology Letters 21(2) 153-166. doi: 10.1111/ele.12893\nO’Sullivan D, LR Bergmann, and JE Thatcher. 2018. Spatiality, maps, and mathematics in critical human geography: toward a repetition with difference. The Professional Geographer 70(1) 129-139. doi: 10.1080/00330124.2017.1326081\nHarris R, D O’Sullivan, M Gahegan, M Charlton, L Comber, P Longley, C Brunsdon, N Malleson, A Heppenstall, A Singleton, D Arribas-Bel, and A Evans. 2017. More bark than bytes? Reflections on 21+ years of geocomputation. Environment and Planning B: Urban Analytics and City Science 44(4) 598-617. doi: 10.1177/2399808317710132.\nLiu C and O’Sullivan, D. 2016. An abstract model of gentrification as a spatially contagious succession process. Computers, Environment and Urban Systems 59 1-10. doi: 10.1016/j.compenvurbsys.2016.04.004\nThatcher JE, D O’Sullivan and D Mahmoudi. 2016. Data colonialism through accumulation by dispossession: new metaphors for everyday data. Environment and Planning D: Society and Space 34(6) 990-1006. doi: 10.1177/0263775816633195\nCheung AK-L, G Brierley, and D O’Sullivan. 2016. Landscape structure and dynamics on the Qinghai-Tibetan Plateau. Ecological Modelling 339 7-22. doi: 10.1016/j.ecolmodel.2016.07.015\nThatcher JE, LR Bergmann, B Ricker, R Rose-Redwood, D O’Sullivan, TJ Barnes, LR Barnes­moore, L Beltz Imaoka, R Burns, J Cinnamon, CM Dalton, C Davis, S Dunn, F Harvey, J-K Jung, E Kersten, L Knigge, N Lally, W Lin, D Mahmoudi, M Martin, W Payne, A Sheikh, T Shelton, E Sheppard, CW Strother, A Tarr, MW Wilson, and JC Young. 2016. Revisiting critical GIS. Environment and Planning A 48(5) 815-824. doi: 10.1177/0308518X15622208\nO’Sullivan D, T Evans, SM Manson, S Metcalf, A Ligmann-Zielinska, and C Bone. 2016. Strategic directions for agent-based modeling: avoiding the YAAWN syndrome. Journal of Land Use Science 11(2) 177-187. doi: 10.1080/1747423X.2015.1030463\nO’Sullivan D and SM Manson. 2015. Do physicists have geography envy? And what can geographers learn from it? Annals of the Association of American Geographers 105(4) 704–722. doi: 10.1080/00045608.2015.1039105\nCheung AK-L, D O’Sullivan and G Brierley. 2015. Graph-assisted landscape monitoring. International Journal of Geographical Information Science 29(4) 580-605. doi: 10.1080/13658816.2014.989856\nEtherington TR, EP Holland, and D O’Sullivan. 2015. NLMpy: a python software package for the creation of neutral landscape models within a general numerical framework. Methods in Ecology and Evolution 6(2) 164-168. doi: 10.1111/2041-210X.12308\nHong S-Y, D O’Sullivan and Y Sadahiro. 2014. Implementing Spatial Segregation Measures in R. PLoS ONE 9(11): e113767. doi: 10.1371/journal.pone.0113767\nO’Sullivan D. 2014. Don’t panic! The need for change and for curricular pluralism. Dialogues in Human Geography 4(1) 39-44. doi: 10.1177/2043820614525712\nMillington JDA, D O’Sullivan and GLW Perry. 2012. Model histories: Narrative explanation in generative simulation modelling. Geoforum 43(6) 1025-1034. doi: 10.1016/j.geoforum.2012.06.017\nMueller S, DJ Exeter, H Petousis-Harris, N Turner, D O’Sullivan and CD Buck. 2012. Measuring disparities in immunisation coverage among children in New Zealand. Health and Place 18(6) 1217-1223. doi: 10.1016/j.healthplace.2012.08.003\nHong S-Y and D O’Sullivan. 2012. Detecting ethnic residential clusters using an optimisation clustering method. International Journal of Geographical Information Science 26(8) 1257-1277. doi: 10.1080/13658816.2011.637045\nXue J, W Friesen and D O’Sullivan. 2012. Diversity in Chinese Auckland: Hypothesising multiple ethnoburbs. Population, Space and Place 18 579-595. doi: 10.1002/psp.688\nMavoa S, K Witten, T McCreanor, and D O’Sullivan. 2012. GIS based destination accessibility via public transit and walking in Auckland, New Zealand. Journal of Transport Geography 20(1) 15-22. doi: 10.1016/j.jtrangeo.2011.10.001\nMateos P, PA Longley, and D O’Sullivan. 2011. Ethnicity and Population Structure in Personal Naming Networks. PLoS ONE 6(9) e22943. doi: 10.1371/journal.pone.0022943\nPearson J, R Lay-Yee, P Davis, D O’Sullivan, M von Randow, N Kerse and S Pradhan. 2011. Primary care in an aging society: Building and testing a microsimulation model for policy purposes. Social Science Computer Review 29(1) 21-36. doi: 10.1177/0894439310370087\nO’Sullivan D. 2009. What’s critical about critical GIS? Cartographica 44(1) 7-8. doi: 10.3138/carto.44.1.5\nO’Sullivan D and G. L. W. Perry. 2009. A discrete space model for continuous space dispersal processes. Ecological Informatics 4(2) 57-68. doi: 10.1016/j.ecoinf.2009.03.001\nO’Sullivan D. 2009. Changing neighborhoods – neighborhoods changing: a framework for spatially explicit agent-based models of social systems. Sociological Methods and Research 37(4) 498-530. doi: 10.1177/0049124109334793\nReardon SF, CR Farrell, SA Matthews, D O’Sullivan, K Bischoff and G Firebaugh. 2009. Race and space in the 1990s: changes in the geographic scale of racial residential segregation, 1990-2000. Social Science Research 38 55-70. doi: 10.1016/j.ssresearch.2008.10.002\nLee BA, SF Reardon, G Firebaugh, CR Farrell, SA Matthews and D O’Sullivan. 2008. Beyond the census tract: patterns and determinants of racial segregation at multiple geographic scales. American Sociological Review 73(October) 766-791. doi: 10.1177/000312240807300504\nReardon SF, SA Matthews, D O’Sullivan, BA Lee, G Firebaugh, CR Farrell and K Bischoff. 2008. The geographic scale of metropolitan segregation. Demography, 45(3) 489-514. doi: 10.1353/dem.0.0019\nO’Sullivan D. 2008. Geographical information science: agent-based models. Progress in Human Geography 32(2) 541-550. doi: 10.1177/0309132507086879\nO’Sullivan D and DWS Wong. 2007. A surface-based approach to measuring spatial segregation. Geographical Analysis 39(2) 147-168. doi: 10.1111/j.1538-4632.2007.00699.x\nO’Sullivan D. 2006. Geographical information science: critical GIS. Progress in Human Geography 30(6) 783-791. doi: 10.1177/0309132506071528\nRygel L, D O’Sullivan and B Yarnal. 2006. A method for constructing a social vulnerability index: an application to hurricane storm surges in a developed country. Mitigation and Adaptation Strategies for Global Change 11(3) 741-764. doi: 10.1007/s11027-006-0265-6\nManson SM and D O’Sullivan. 2006. Complexity theory in the study of space and place. Environment and Planning A 38(4) 677-692. doi: 10.1068/a37100\nO’Sullivan D, JP Messina, SM Manson and TW Crawford. 2006. Space, place, and complexity science. Environment and Planning A 38(4) 611-617. doi: 10.1068/a3812\nO’Sullivan D. 2005. Geographical information science: time changes everything. Progress in Human Geography 29(6) 749-756. doi: 10.1191/0309132505ph581pr\nCrawford TW, JP Messina, SM Manson and D O’Sullivan. 2005. Complexity science, complex systems, and land-use research. Environment and Planning B: Planning & Design 32(5) 792-798. doi: 10.1068/b3206ed\nReardon SF and D O’Sullivan. 2004. Measures of Spatial Segregation. Sociological Methodology 34(1) 121-162. doi: 10.1111/j.0081-1750.2004.00150.x\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. doi: 10.1111/j.0020-2754.2004.00321.x\nO’Sullivan D. 2002. Toward micro-scale spatial modelling of gentrification. Journal of Geographical Systems 4(3) 251-274. doi: 10.1007/s101090200086\nO’Sullivan D. 2001. Graph-cellular automata: a generalised discrete urban and regional model. Environment and Planning B: Planning & Design 28(5) 687-705. doi: 10.1068/b2707\nHaklay M, T Schelhorn, D O’Sullivan and M Thurstain-Goodwin. 2001. “So go down town”: Simulating pedestrian movement in town centres. Environment and Planning B: Planning & Design 28(3) 343-359. doi: 10.1068/b2758t\nTorrens PM and D O’Sullivan. 2001. Cellular automata and urban simulation: where do we go from here? Environment and Planning B: Planning & Design 28(2) 163-168. doi: 10.1068/b2802ed\nO’Sullivan D and A Turner. 2001. Visibility graphs and landscape visibility analysis. International Journal of Geographical Information Science 15(3) 221-237. doi: 10.1080/13658810151072859\nTurner A, M Doxa, D O’Sullivan and A Penn. 2001. From isovists to visibility graphs: a methodology for the analysis of architectural space. Environment and Planning B: Planning & Design 28(1) 103-121. doi: 10.1068/b2684\nO’Sullivan D. 2001. Exploring spatial process dynamics using irregular cellular automaton models. Geographical Analysis 33(1) 1-18. doi: 10.1111/j.1538-4632.2001.tb00433.x\nO’Sullivan D. and M Haklay. 2000. Agent-based models and individualism: is the world agent-based? Environment and Planning A 32(8) 1409-1425. doi: 10.1068%2Fa32140\nO’Sullivan D, A Morrison and J Shearer. 2000. Using desktop GIS for the investigation of accessibility by public transport: an isochrone approach. International Journal of Geographical Information Science 14(1) 85-104. doi: 10.1080/136588100240976"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters\nBergmann LR and D O’Sullivan. 2024. Space: towards a global sense of place. In A Research Agenda for Spatial Analysis (eds LJ Wolf, R Harris and AJ Heppenstall). Elgar Research Agendas. Edward Elgar Publishing. doi: 10.4337/9781802203233.00009.\nC Andris and D O’Sullivan. 2019. Spatial Networks. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer. doi: 10.1007/978-3-642-36203-3_67-1.\nD O’Sullivan. 2018. Cartography and geographic information systems. In J Ash, R Kitchin and A Leszczynski (eds), Digital Geographies, (Los Angeles: Sage), pages 118-128.\nD O’Sullivan. 2018. Big data … why (oh why?) this computational social science? In JE Thatcher, J Eckert and A Shears (eds), Thinking Big Data in Geography: New Regimes, New Research, (Lincoln: University of Nebraska Press), pages 21-38. doi: 10.2307/j.ctt21h4z6m.7\nLR Bergmann and D O’Sullivan. 2017. Computing with many spaces: Generalizing projections for the digital geohumanities and GIScience. In Proceedings of GeoHumanities’17: 1st ACM SIGSPATIAL Workshop on Geospatial Humanities, Redondo Beach, CA, November 7-10 (GeoHumanities’17), pages 31-38. doi: 10.1145/3149858.3149866\nThatcher JE, LR Bergmann and D O’Sullivan. 2016. Searching for common ground (again). In Short Paper Proceedings of 9th International Conference on Geographical Information Science, eds JA Miller, D O’Sullivan and N Wiegand, (Montreal, Canada), pages 304-307. doi: 10.21433/B3118nq409qz\nPfeffer K, J Martinez, D O’Sullivan and D Scott. 2015. Geo-Technologies for Spatial Knowledge: Challenges for Inclusive and Sustainable Urban Development. In J Gupta, K Pfeffer, H Verrest and M Ros-Tonen (eds), Geographies of Urban Governance (Cham: Springer International Publishing), pages 147-173. doi: 10.1007/978-3-319-21272-2_8\nO’Sullivan D. 2014. Spatial Network Analysis. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer, pages 1253-1273. doi: 10.1007/978-3-642-23430-9_67\nO’Sullivan D, JDA Millington, GLW Perry and J Wainwright. 2012. Agent-Based Models – Because They’re Worth It? In AJ Heppenstall, AJ Crooks, LM See, and M Batty (eds), Agent-Based Models of Geographical Systems (Springer: Dordrecht, Netherlands), pages 109-123. doi: 10.1007/978-90-481-8927-4_6\nHeppenstall AJ, AJ Evans, MH Birkin, JR Macgill and D O’Sullivan. 2005. The Use of Hybrid Agent-Based Systems to Model Petrol Markets. In T Terano, H Kita, H Kaneda, K Arai and H Deguchi (eds), Agent-Based Simulation: From Modelling Methodologies to Real-World Applications, Springer Series on Agent-Based Social Systems, pages 154-162. doi: 10.1007/4-431-26925-8_17\nO’Sullivan D. 2004. Too much of the wrong kind of data: implications for the practice of micro-scale spatial modelling. In MF Goodchild and D Janelle (eds), Spatially Integrated Social Science: Examples of Best Practice (Oxford University Press: Oxford), pages 95-107.\nO’Sullivan D, JR Macgill and C Yu. 2003. Agent-based residential segregation: a hierarchically structured spatial model. In C Macal, M North and D Sallach (eds), Agent 2003: Challenges in Social Simulation, pages 493-507 (Argonne National Laboratory: Chicago). www.agent2004.anl.gov/Agent2003.pdf\nO’Sullivan D. 2002. Understanding the difference that space can make: toward a geographical agent modeling environment. In C. Macal and D. Sallach (eds), Agent 2002: Ecology, Exchange, and Evolution, pages 13-25 (Argonne National Laboratory: Chicago). www.agent2003.anl.gov/proceedings/2002.pdf\nO’Sullivan D and PM Torrens. 2001. Cellular models of urban systems. In S Bandini and T Worsch (eds), Theoretical and Practical Issues on Cellular Automata, Proceedings of the Fourth International Conference on Cellular Automata for Research and Industry (ACRI 2000), October 4–6, Karlsruhe, Germany (Springer-Verlag: London), pages 108-116. [Also available as CASA Working Paper 22 at www.casa.ucl.ac.uk/cellularmodels.pdf"
  },
  {
    "objectID": "publications.html#dissertations",
    "href": "publications.html#dissertations",
    "title": "Publications",
    "section": "Dissertations",
    "text": "Dissertations\nPhD: O’Sullivan D. 2000. Graph-based Cellular Automaton Models of Urban Spatial Processes. University College London.\nMSc: O’Sullivan D. 1997. Using GIS to create public transport travel time isochrones for the Glasgow area. University of Glasgow."
  },
  {
    "objectID": "portfolio/01-oecd-rural-services.html",
    "href": "portfolio/01-oecd-rural-services.html",
    "title": "OECD report",
    "section": "",
    "text": "Not a lot to say here, just to note that I recently assisted with reviewing and commentary on an OECD Report entitled Getting to Services in Towns and Villages on the variations across the OECD in the accessibility of various services across the settlement hierarchy.\nAs a geographer it was interesting to see this work from a more economistic perspective, and also somewhat amusing to hear that getting maps into OECD publications is an uphill battle given some ongoing uh… debate about territorial claims across the organisation!\nThat minor issue notwithstanding, it was a pleasure working with the folks at OECD. There’s a lot of really great research going on outside the academy, something it is easy to lose site of when you’ve spent so long inside it."
  },
  {
    "objectID": "portfolio/05-jev.html",
    "href": "portfolio/05-jev.html",
    "title": "JEV risk mapping",
    "section": "",
    "text": "I’m excited to be doing the mapping work in a recently funded project investigating the risks for establishment in New Zealand of Japanese Encephalitis Virus (JEV), funded by the Ministry for Primary Industries - Manatū Ahu Matua, and led by Prof Phil Lester.\nIt’s early days, and my initial work is focused on marshalling data on the distribution of bird species using the recently compiled NZ Bird Atlas. Here’s a map of local favourite kākā sitings.\n\n\n\nSitings of kākā in the NZ Bird Atlas\n\n\nIf this map seems more optimistic than others you’ve seen that’s because it is derived from raw sightings. Translating that information into more reliable ‘range’ maps is one aspect of the work we need to consider.\nThe reason where the birds are matters in this project is that establishment of JEV in New Zealand is most likely to occur as a result of it circulating in some bird populations (likely not kākā) from which mosquitoes might then pass it on to other populations."
  },
  {
    "objectID": "portfolio/00-2-30-day-maps.html",
    "href": "portfolio/00-2-30-day-maps.html",
    "title": "The 30 day map challenge",
    "section": "",
    "text": "Finding myself unaccountably with time on my hands in November 2023, I thought I’d give the 30 Day Map Challenge a go. I am not sure I’d recommend the experience. I mean, it’s a kind of warped fun, but it’s hard. I might have restricted my focus too much for it to stay interesting for a whole month, perhaps. Anyway, you can see what I made here and a short slideshow reflecting on the experience here.\nHere are my two favourite maps from the month:\n\n\n\n#5 A hillshaded map made by crumpling paper. Yes… it’s a real place.\n\n\n\n\n\n#27 Dots within dots to show all seven components and the overall value of an index of deprivation."
  },
  {
    "objectID": "portfolio/00-1-nz-commute-viewer.html",
    "href": "portfolio/00-1-nz-commute-viewer.html",
    "title": "Aotearoa New Zealand commutes viewer",
    "section": "",
    "text": "I somehow convinced myself in the break between two exhausting semesters in 2020 (yes, that year) to enter Stats NZ’s There and back again visualization competition.\nI learned a lot about a bunch of things along the way, but probably needed to take a step back and learn more about web development frameworks, as there is altogether too much handcrafted javascript in there. Nevertheless, I had fun and I am reasonably happy with the end result. Also… it still works over four years later, which might not be true if I’d used something smarter than vanilla JS.\nGo to the viewer\nHere’s a screenshot for anyone not curious enough to click on the link.\n\nNo, I didn’t win the competition. That was this entry by Jono Cooper."
  },
  {
    "objectID": "portfolio/00-0-spatial-covid-model.html",
    "href": "portfolio/00-0-spatial-covid-model.html",
    "title": "A spatial COVID model",
    "section": "",
    "text": "During those ahem… heady days of the first COVID-19 lockdown in Aotearoa (7 or 8 weeks in April-May 2020) I worked intensely with colleagues on a simple spatial simulation model of the pandemic in New Zealand.\nInitially we hoped that we might get involved in the ongoing modelling efforts which the government was using to manage the national response given how self-evidently geographical a thing an epidemic is. But that clearly wasn’t a perspective shared by the public health professionals and others advising the government. Either that or they were just too damn busy dealing with events to welcome the additional distraction of thinking about the geography of the disease.\nOr maybe they thought it was just too much:\n\nWhatever the reason, we never did get involved in the national response, although it was clear by the time of the outbreak in later 2021 that they were, at least by then, thinking geographically about how to manage things. Anyway, you can play with the model by clicking below.\nGo to the model\nIt’s quite slow to load, and if you aren’t smart about the settings you use it’s likely to rapidly spin out of control (just like a uh… pandemic) and slow things down dramatically. For that reason I advise only using the go-one-day or go-one-week buttons until you get a feel for things.\nWe published a paper explaining the inner workings here:\n\nO’Sullivan D, M Gahegan, DJ Exeter, and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967-1000. doi: 10.1111/tgis.12660\n\nand I presented the work at the hard-to-believe-it-even-happened face-to-face-in-person 2020 meeting of the New Zealand Geographical Society in November that year.\nAll the materials are available at this repo."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "This page provides more information on projects both current and completed.\n\n\n\n\n\n\n\n\n\n\n\n\nJEV risk mapping\n\n\nThe birds and the… mozzies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScience impacts in Antarctica\n\n\nTake only pictures, leave only footprints…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-space mapping\n\n\nMore generally: folding space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent modeling of land management\n\n\nMoving the middle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOECD report\n\n\nAccess to services on the rural to urban continuum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA model for gene drive control of wasps\n\n\nSpoilers: it wouldn’t work\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiled and woven maps\n\n\nA new approach to mapping multivariate data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe 30 day map challenge\n\n\nIt’s called ‘challenge’ for a reason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAotearoa New Zealand commutes viewer\n\n\nMy take on the 2018 Census travel to work and study data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA spatial COVID model\n\n\nFiddling (with code) while the world burns\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html",
    "href": "notebooks/2025-02-18-marimo.html",
    "title": "Marimo notebooks",
    "section": "",
    "text": "In an ideal world I’d write this notebook in the tool I’m talking about and share it on my website that way. Unfortunately I haven’t been able to find an extension for Quarto (which I use to build the website) that handles marimo notebooks1, which is the tool in question.\nSo what’s a marimo notebook? The easiest way to think about it is as a reactive Python notebook. I’ve been using Jupyter notebooks since forever. I first taught with them in 2016 at Berkeley as a contribution to the then nascent major in Data Science there. Since then I’ve taught python programming using Jupyter notebooks as the platform.\nThat’s a good choice in some ways, but a bad one in others. The good is that you can introduce code in the context of working code for students to modify and get comfortable with, in a managed, predictable environment (unlike the wild west that is most student laptops). The bad reason is that Jupyter notebooks can get in a terrible tangle if you run the code cells out of sequence. Almost invariably when a student runs into problems (or for that matter I run into a problem) using a notebook, it’s because they I have been jumping back and forward running cells, forgetting that this will put the notebook into a weird state and break things.\nIt’s not unreasonable for students to expect a change anywhere in their code anywhere in a notebook to affect every other part of the notebook that uses affected variables and their values. In other words, for the notebook to ‘react’ to all changes. This enhanced interactivity is what marimo notebooks offer."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#jupyter-notebooks",
    "href": "notebooks/2025-02-18-marimo.html#jupyter-notebooks",
    "title": "Marimo notebooks",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nAnyway, if you are unfamiliar, this page is built from a Jupyter notebook. More accurately, most of it is, but I’ll get to that.\nThis being a notebook, I can mix markdown text such as this text you are reading, with code cells like this:\n\n%matplotlib inline\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nThat cell imports some python modules that I have installed locally, and now I can write python code using them, and run it interactively (locally) in a browser (or suitable coding environment like Visual Studio Code). For example, I can define a random point class RandomPoint and a function for generating a collection of points using the ‘blue noise’ method I described in a previous post.\n\nclass RandomPoint():\n\n    def __init__(self):\n        self.x = random.random()\n        self.y = random.random()\n\n    def distance2(self, pt):\n        dx = min(self.x - pt.x, 1 - self.x + pt.x)\n        dy = min(self.y - pt.y, 1 - self.y + pt.y)\n        return dx**2 + dy**2\n        \ndef spaced_points(n=100):\n    pts = [RandomPoint()]\n    for i in range(1, n):\n        n_candidates = math.ceil(math.log(1.1 * i * math.e))\n        p_new = [RandomPoint() for _ in range(n_candidates)]\n        d_mins = [min([p.distance2(p0) for p0 in pts]) for p in p_new]\n        i_max = max(enumerate(d_mins), key=lambda x: x[1])[0]\n        pts.append(p_new[i_max])\n    return pd.DataFrame(data = {'x': [p.x for p in pts], \n                                'y': [p.y for p in pts]})\n\nAnd I can make a point pattern and plot it.\n\npp = spaced_points(1000)\nscatter = plt.scatter(x=pp.x, y=pp.y)\nscatter.axes.set_aspect(1)\nscatter.axes.set_xlim(0, 1)\nscatter.axes.set_ylim(0, 1)\nplt.show()\n\n\n\n\n\n\n\n\nWhat you are seeing here is the output of my code parsed to produce a static web page in HTML. You can also parse notebooks to make PDFs, Word documents, and so on."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#interactive-jupyter",
    "href": "notebooks/2025-02-18-marimo.html#interactive-jupyter",
    "title": "Marimo notebooks",
    "section": "Interactive Jupyter",
    "text": "Interactive Jupyter\nThe experience of using Jupyter notebooks is highly interactive for the author of the notebook. That quickly leads to wanting to put together notebooks that are interactive for the end user. That’s a little bit more complicated to code, but not by much, using the ipywidgets module.\nIf I wrap the plotting code above in a function that takes as inputs a point pattern pp and the number of points n to plot, then using components from the ipywidgets module, I can control how many points in the pattern are displayed, so that I can see the progression of the generating function as points are added.\n\nimport ipywidgets as widgets\nfrom ipywidgets import interactive, fixed\n\ndef plot_spaced_points(pp:pd.DataFrame, n:int):\n  scatter = plt.scatter(x=pp.x[:n], y=pp.y[:n])\n  scatter.axes.set_aspect(1)\n  scatter.axes.set_xlim(0, 1)\n  scatter.axes.set_ylim(0, 1)\n  plt.show()\n  return None\n\npp = spaced_points(1000)\nnum_pts = widgets.IntSlider(value=100, min=10, max=pp.shape[0], step=5)\ninteractive(plot_spaced_points, pp=fixed(pp), n=num_pts)\n\n\n\n\nAnd it works. For me. Locally. Here’s a video to prove it!\n\nWhat’s much more difficult to arrange is hosting this interactive notebook in an online environment so that end users can interact with it like I can.2 For that I need to set up some kind of server infrastructure, or use an environment such as Google Colab, or CoCalc, Azure Notebooks, or SaturnCloud.\nSetting up your own infrastructure is technically demanding involving scary sounding tools like Kubernetes. It’s by no means impossible, but you need good people to set things up, and security can be a concern.\nThe hosted environments can be a useful alternative, but they can also be limited by low power cores, or limited storage (on free tiers), or can get quite pricey quickly if you need more oomph, or want multiple users at a time (this is a common problem getting Jupyter notebooks into the lab in New Zealand universities)."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#enter-marimo",
    "href": "notebooks/2025-02-18-marimo.html#enter-marimo",
    "title": "Marimo notebooks",
    "section": "Enter marimo",
    "text": "Enter marimo\nThis is where marimo comes in. Below is a marimo notebook embedded in a HTML iframe.3\nYou can drag the slider to see the number of points in the plot change. You can even edit the code blocks to see the effect of changes to the code! If you make it crash, you’ll have to relaunch the Restart kernel option in the controls dropdown menu in the upper right.\n\n\nPutting it in an iframe like this isn’t ideal. If you’d just like to visit the page, then go to southosullivan.com/misc/meet-marimo/\nThe magic here is :WebAssembly (WASM) which my python ‘source code’ has been compiled to via the Pyodide project. Because the resulting HTML page is in WebAssembly it runs in the browser, not on a python kernel on a remote server, so it is effectively a static web page and I can even serve it on github pages. Here’s a more ambitious example from my work on tiled maps of multivariate data. Be warned it takes quite a while to download (about a minute to get up and running) because it has to install a bunch of relatively chunky python modules before it will work.\nAs a way to build relatively lightweight dashboards or nice interactive web-app ‘explainers’ without complicated backend hosting, this seems to hold a lot of potential!"
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#footnotes",
    "href": "notebooks/2025-02-18-marimo.html#footnotes",
    "title": "Marimo notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYet… and it may never happen, given the investment of Posit in Shiny↩︎\nNote from future self: actually not that difficult. See this post.↩︎\nOne wrinkle here is that I have to host this notebook on another server because I can’t show a local HTML file in an iframe because of a disallowed MIME type (“text/html”) according to my browser console.↩︎"
  }
]