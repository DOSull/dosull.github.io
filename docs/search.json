[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Note: some posts show code that has been superseded. Code ran as written at last modified dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom points on the globe revisited\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nOf Christmas gifts and rabbit holes\n\n\n\nDavid O’Sullivan\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA welcome (re)emergence of systems thinking\n\n\n\ncomplexity\n\n\naotearoa\n\n\nconferences\n\n\nsystems\n\n\n\nAnd it’s not before time!\n\n\n\nDavid O’Sullivan\n\n\nDec 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nALGIM 2024\n\n\n\nlocal government\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\n\nOn geospatial in local government\n\n\n\nDavid O’Sullivan\n\n\nNov 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ntmap vs. ggplot2 for mapping\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\ntmap\n\n\nggplot\n\n\n\nI think I prefer…\n\n\n\nDavid O’Sullivan\n\n\nNov 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of DuckDB\n\n\n\ngeospatial\n\n\nduckdb\n\n\nR\n\n\ntutorial\n\n\n\nInsert duck-related joke here\n\n\n\nDavid O’Sullivan\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn praise of GeoPackages\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\n\nShapefiles… who needs ’em?\n\n\n\nDavid O’Sullivan\n\n\nOct 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGiscience 2025\n\n\n\nconferences\n\n\ngeospatial\n\n\naotearoa\n\n\n\nCall for papers\n\n\n\nDavid O’Sullivan\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeoCart’2024\n\n\n\ncartography\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\ntime-space\n\n\n\nReflections on the meeting\n\n\n\nDavid O’Sullivan\n\n\nAug 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful utilities for NetLogo\n\n\n\nnetlogo\n\n\nsimulation\n\n\n\nFor when you are missing R/python\n\n\n\nDavid O’Sullivan\n\n\nAug 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe model zoo\n\n\n\nnetlogo\n\n\nbooks\n\n\nsimulation\n\n\n\nKeeping up with NetLogo\n\n\n\nDavid O’Sullivan\n\n\nAug 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe 30 day map challenge\n\n\n\nR\n\n\ngeospatial\n\n\nmaps\n\n\norigami\n\n\ncartography\n\n\n\nIt’s called ‘challenge’ for a reason\n\n\n\nDavid O’Sullivan\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments with R interpolators\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nMessing around with interpolating via a triangulation to apply coordinate transformations\n\n\n\nDavid O’Sullivan\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSpiral origami\n\n\n\njavascript\n\n\norigami\n\n\nstuff\n\n\n\nThings to make and do\n\n\n\nDavid O’Sullivan\n\n\nAug 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nOK COVID, you win\n\n\n\nvisualization\n\n\ncovid\n\n\nR\n\n\nmaps\n\n\n\nThe end of the line\n\n\n\nDavid O’Sullivan\n\n\nMar 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLow level handling of sf objects\n\n\n\nR\n\n\ngeospatial\n\n\n\nGoddamnit, floating point!\n\n\n\nDavid O’Sullivan\n\n\nDec 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLocations of interest in 2021 delta outbreak\n\n\n\nmaps\n\n\nvisualization\n\n\ncovid\n\n\n\nWhen everything went pear-shaped\n\n\n\nDavid O’Sullivan\n\n\nOct 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nInverse distance weighted (IDW) interpolation using spatstat\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nSo many packages so little time\n\n\n\nDavid O’Sullivan\n\n\nOct 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nKernel density estimation in R spatial\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nHere’s one way to do kernel density estimation in R spatial\n\n\n\nDavid O’Sullivan\n\n\nOct 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUniform random points on the globe\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nThe earth’s a sphere: who knew?\n\n\n\nDavid O’Sullivan\n\n\nOct 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the Dulux colours\n\n\n\nR\n\n\ngeospatial\n\n\nstuff\n\n\nmaps\n\n\naotearoa\n\n\n\nThe ultimate in categorical mapping\n\n\n\nDavid O’Sullivan\n\n\nSep 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAffine transformations of sf objects\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nManipulating simple features in sf is sorta simple, sorta not…\n\n\n\nDavid O’Sullivan\n\n\nAug 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAotearoa New Zealand commutes viewer\n\n\n\njavascript\n\n\nvisualization\n\n\naotearoa\n\n\nstuff\n\n\n\nI have no idea what this is, but it looks… cool?\n\n\n\nDavid O’Sullivan\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWhat three chords\n\n\n\njavascript\n\n\ngeospatial\n\n\nstuff\n\n\n\nSeriously, WTC?!\n\n\n\nDavid O’Sullivan\n\n\nJul 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA spatial COVID model\n\n\n\nnetlogo\n\n\ncovid\n\n\nsimulation\n\n\ngeospatial\n\n\n\nFiddling (with code) while the world burns\n\n\n\nDavid O’Sullivan\n\n\nMay 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "This pages provides more information on projects I’ve been working on lately.\n\n\n\n\n\n\n\n\n\n\n\n\nJEV risk mapping\n\n\nThe birds and the… mozzies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScience impacts in Antarctica\n\n\nTake only pictures, leave only footprints…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-space mapping\n\n\nMore generally: folding space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent modeling of land management\n\n\nMoving the middle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOECD report\n\n\nAccess to services on the rural to urban continuum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiled and woven maps\n\n\nA new approach to mapping multivariate data\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/03-time-space-mapping.html",
    "href": "portfolio/03-time-space-mapping.html",
    "title": "Time-space mapping",
    "section": "",
    "text": "Another ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThis is one aspect of a wider project aimed at developing maps where the geometry is based not on ‘location’ but on the relationships — whether of time, money, people, or anything else of interest between places. Space conceived not as a fixed absolute background to things but as a living moving thing that both shapes and is shaped by unfolding events.\nTime-space mapping is a particular subset of this wider project focused on relations of estimated travel time between locations in a mountainous environment. There’s more detail on what we’re up to in this presentation I gave at GeoCart 2024.\nYou’ll find a video of dynamic time-space in the presentation and there’s another in my diary entry about GeoCart. Meanwhile, here’s a series of maps of estimated hiking times from a given starting location on a series of idealised conical mountains of increasing steepness."
  },
  {
    "objectID": "portfolio/05-jev.html",
    "href": "portfolio/05-jev.html",
    "title": "JEV risk mapping",
    "section": "",
    "text": "I’m excited to be doing the mapping work in a recently funded project investigating the risks for establishment in New Zealand of Japanese Encephalitis Virus (JEV), funded by the Ministry for Primary Industries - Manatū Ahu Matua, and led by Prof Phil Lester.\nIt’s early days, and my initial work is focused on marshalling data on the distribution of bird species using the recently compiled NZ Bird Atlas. Here’s a map of local favourite kākā sitings.\n\n\n\nSitings of kākā in the NZ Bird Atlas\n\n\nIf this map seems more optimistic than others you’ve seen that’s because it is derived from raw sitings. Translating that information into more reliable ‘range’ maps is one aspect of the work we need to consider.\nThe reason where the birds are matters in this project is that establishment of JEV in New Zealand is most likely to occur as a result of it circulating in some bird populations (likely not kākā) from which mosquitoes might then pass it on to other populations."
  },
  {
    "objectID": "portfolio/01-oecd-rural-services.html",
    "href": "portfolio/01-oecd-rural-services.html",
    "title": "OECD report",
    "section": "",
    "text": "Not a lot to say here, just to note that I recently assisted with reviewing and commentary on an OECD Report entitled Getting to Services in Towns and Villages on the variations across the OECD in the accessibility of various services across the settlement hierarchy.\nAs a geographer it was interesting to see this work from a more economistic perspective, and also somewhat amusing to hear that getting maps into OECD publications is an uphill battle given some ongoing uh… debate about territorial claims across the organisation!\nThat minor issue notwithstanding, it was a pleasure working with the folks at OECD. There’s a lot of really great research going on outside the academy, something it is easy to lose site of when you’ve spent so long inside it."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A similar list is available at my ORCiD profile. You can also see them in citational context at my google scholar page, which is reasonably accurate. Links below are generally to official publisher sources, and are often paywalled. Open access copies are linked from my University of Auckland profile page. If you would like a copy of something in particular and can’t access it, then get in touch.\nJump to books, papers, book chapters, dissertations"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books\n\nO’Sullivan D. 2024. Computing Geographically: Bridging Giscience and Geography (Guilford Press: New York).\nO’Sullivan D. 2017. Section Editor for ‘Fundamentals of GIScience’ (32 articles) in The International Encyclopedia of Geography: People, the Earth, Environment, and Technology. Richardson, D. (ed). New York: Wiley.\nMiller JA, D O’Sullivan and Wiegand N eds. 2016. Geographic Information Science: Proceedings of the 9th International Conference, GIScience 2016 Lecture Notes in Computer Science Vol. 9927 (Springer: Cham, Switzerland).\nO’Sullivan D and GLW Perry. 2013. Spatial Simulation: Exploring Pattern and Process (Wiley: Chichester, UK).\nO’Sullivan D and DJ Unwin. 2010. Geographic Information Analysis 2nd edn. (Wiley: Hoboken, NJ).\nO’Sullivan D and DJ Unwin. 2003. Geographic Information Analysis (Wiley: Hoboken, NJ)."
  },
  {
    "objectID": "publications.html#in-refereed-journals",
    "href": "publications.html#in-refereed-journals",
    "title": "Publications",
    "section": "In refereed journals",
    "text": "In refereed journals\n\nMahmoudi D, J Thatcher, LB Imaoka, and D O’Sullivan. Online first. From FOSS to profit: Digital spatial technologies and the mode of production. Digital Geography and Society. 100101. doi: 10.1016/j.diggeo.2024.100101.\nO’Sullivan D. 2024. Environment and Planning B and me; or what is lost in data. Environment and Planning B: Urban Analytics and City Science 51(5) 1045–1048. doi: 10.1177/23998083241246320.\nEtherington TR, D O’Sullivan, GLW Perry, DR Richards, and J Wainwright. 2024. A least-cost network neutral landscape model of human sites and routes. Landscape Ecology 39(3) 52. doi: 10.1007/s10980-024-01836-w.\nAntosz P, D Birks, B Edmonds, A Heppenstall, R Meyer, JG Polhill, D O’Sullivan and N Wijermans. 2023. What do you want theory for? A pragmatic analysis of the roles of “theory” in agent-based modelling. Environmental Modelling & Software 168 105802. doi: 10.1016/j.envsoft.2023.105802\nLester PJ, D O’Sullivan and GLW Perry. 2023. Gene drives for invasive wasp control: Extinction is unlikely, with suppression dependent on dispersal and growth rates. Ecological Applications 33(7) e2912. doi: 10.1002/eap.2912\nBergmann L, LF Chaves, D O’Sullivan and RG Wallace. 2023. Dominant Modes of Agricultural Production Helped Structure Initial COVID-19 Spread in the U.S. Midwest. ISPRS International Journal of Geo-Information 12(5) 195. doi: 10.3390/ijgi12050195\nEtherington T, F Morgan, D O’Sullivan. 2022, Binary space partitioning generates hierarchical and rectilinear neutral landscape models suitable for human-dominated landscapes. Landscape Ecology 37(7) 1761–1769. doi: 10.1007/s10980-022-01452-6\nChaves LF, MD Friberg, LA Hurtado, RM Rodríguez, D O’Sullivan and LR Bergmann. 2022. Trade, uneven development and people in motion: Used territories and the initial spread of COVID-19 in Mesoamerica and the Caribbean. Socio-Economic Planning Sciences 80 (March) 101161. doi: 10.1016/j.seps.2021.101161\nGibadullina A, LR Bergmann and D O’Sullivan. 2021. For Geographical Network Analysis. Tijdschrift voor Economische en Sociale Geografie 112(4) 482-487. doi: 10.1111/tesg.12489.\nO’Sullivan D. 2021. New mappings of GIScience and geography. A commentary on May Yuan’s ‘GIS research to address tensions in geography.’ Singapore Journal of Tropical Geography 42(1) 31–35. doi: 10.1111/sjtg.12345\nO’Sullivan D. 2021. Things are how they are because of how they got that way: Thoughts from the beach, on 50 years of Geographical Analysis. Geographical Analysis 53(1) 157–163. doi: 10.1111/gean.12225\nFranklin, RS, V Houlden, C Robinson, D Arribas-Bel, EC Delmelle, U Demšar, HJ Miller, and D O’Sullivan. 2021. Who counts? Gender, gatekeeping, and quantitative human geography. The Professional Geographer 73(1) 48–61. doi: 10.1080/00330124.2020.1828944\nO’Sullivan D, M Gahegan, DJ Exeter and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967–1000. doi: 10.1111/tgis.12660\nPayne WB and D O’Sullivan. 2020. Exploding the phone book: Spatial data arbitrage in the 1990s Internet boom. Annals of the American Association of Geographers 110(2) 391–398. doi: 10.1080/24694452.2019.1656999\nManson S, L An, KC Clarke, A Heppenstall, J Koch, B Krzyzanowski, F Morgan, D O’Sullivan, BC Runck, E Shook and L Tesfatsion. 2020. Methodological Issues of Spatial Agent-Based Models. Journal of Artificial Societies and Social Simulation 23(1) 3. doi: 10.18564/jasss.4174\nChristophers B and D O’Sullivan. 2019. Intersections of inequality in homeownership in Sweden. Housing Studies 34(6) 897-924. doi: 10.1080/02673037.2018.1495695\nMavoa S, N Bagheri, MJ Koohsari, AT Kaczynski, KE Lamb, K Oka, D O’Sullivan and K Witten. 2019. How do neighbourhood definitions influence the associations between built environment and physical activity? International Journal of Environmentalal Research and Public Health 16. doi: 10.3390/ijerph16091501\nO’Sullivan, D 2019. Untangling knots: Thoughts on Wilson’s New Lines. Transactions in GIS 32(1) 168-169. doi: 10.1111/tgis.12502\nLiu C, D O’Sullivan and GLW Perry. 2018. The rent gap revisited: gentrification in Point Chevalier, Auckland. Urban Geography 39(9) 1300-1325. doi: 10.1080/02723638.2018.1446883\nPerry GLW and D O’Sullivan. 2018. Identifying narrative descriptions in agent-based models representing past human-environment interactions. Journal of Archaeological Method and Theory, 25(3) 795-813. doi: 10.1007/s10816-017-9355-x\nMavoa S, K Lamb, D O’Sullivan, K Witten and M. Smith. 2018. Are disadvantaged children more likely to be excluded from analysis when applying global positioning systems inclusion criteria? BMC Research Notes, 11 578. doi: 10.1186/s13104-018-3681-2\nMahdavi Ardestani B, D O’Sullivan, and P Davis. 2018. A multi-scaled agent-based model of residential segregation applied to a real metropolitan area. Computers, Environment and Urban Systems, 69 1-16. doi: 10.1016/j.compenvurbsys.2017.11.002\nBergmann LR and D O’Sullivan. 2018. Reimagining GIScience for relational spaces. The Canadian Geographer / Le Géographe canadien. 62(1) 7-14. doi: 10.1111/cag.12405\nGetz WM, CR Marshall, CJ Carlson, L Giuggioli, SJ Ryan, SS Romañach, C Boettiger, SD Chamberlain, L Larsen, P D’Odorico, D O’Sullivan. 2018. Making ecological models adequate. Ecology Letters 21(2) 153-166. doi: 10.1111/ele.12893\nO’Sullivan D, LR Bergmann, and JE Thatcher. 2018. Spatiality, maps, and mathematics in critical human geography: toward a repetition with difference. The Professional Geographer 70(1) 129-139. doi: 10.1080/00330124.2017.1326081\nHarris R, D O’Sullivan, M Gahegan, M Charlton, L Comber, P Longley, C Brunsdon, N Malleson, A Heppenstall, A Singleton, D Arribas-Bel, and A Evans. 2017. More bark than bytes? Reflections on 21+ years of geocomputation. Environment and Planning B: Urban Analytics and City Science 44(4) 598-617. doi: 10.1177/2399808317710132.\nLiu C and O’Sullivan, D. 2016. An abstract model of gentrification as a spatially contagious succession process. Computers, Environment and Urban Systems 59 1-10. doi: 10.1016/j.compenvurbsys.2016.04.004\nThatcher JE, D O’Sullivan and D Mahmoudi. 2016. Data colonialism through accumulation by dispossession: new metaphors for everyday data. Environment and Planning D: Society and Space 34(6) 990-1006. doi: 10.1177/0263775816633195\nCheung AK-L, G Brierley, and D O’Sullivan. 2016. Landscape structure and dynamics on the Qinghai-Tibetan Plateau. Ecological Modelling 339 7-22. doi: 10.1016/j.ecolmodel.2016.07.015\nThatcher JE, LR Bergmann, B Ricker, R Rose-Redwood, D O’Sullivan, TJ Barnes, LR Barnesmoore, L Beltz Imaoka, R Burns, J Cinnamon, CM Dalton, C Davis, S Dunn, F Harvey, J-K Jung, E Kersten, L Knigge, N Lally, W Lin, D Mahmoudi, M Martin, W Payne, A Sheikh, T Shelton, E Sheppard, CW Strother, A Tarr, MW Wilson, and JC Young. 2016. Revisiting critical GIS. Environment and Planning A 48(5) 815-824. doi: 10.1177/0308518X15622208\nO’Sullivan D, T Evans, SM Manson, S Metcalf, A Ligmann-Zielinska, and C Bone. 2016. Strategic directions for agent-based modeling: avoiding the YAAWN syndrome. Journal of Land Use Science 11(2) 177-187. doi: 10.1080/1747423X.2015.1030463\nO’Sullivan D and SM Manson. 2015. Do physicists have geography envy? And what can geographers learn from it? Annals of the Association of American Geographers 105(4) 704–722. doi: 10.1080/00045608.2015.1039105\nCheung AK-L, D O’Sullivan and G Brierley. 2015. Graph-assisted landscape monitoring. International Journal of Geographical Information Science 29(4) 580-605. doi: 10.1080/13658816.2014.989856\nEtherington TR, EP Holland, and D O’Sullivan. 2015. NLMpy: a python software package for the creation of neutral landscape models within a general numerical framework. Methods in Ecology and Evolution 6(2) 164-168. doi: 10.1111/2041-210X.12308\nHong S-Y, D O’Sullivan and Y Sadahiro. 2014. Implementing Spatial Segregation Measures in R. PLoS ONE 9(11): e113767. doi: 10.1371/journal.pone.0113767\nO’Sullivan D. 2014. Don’t panic! The need for change and for curricular pluralism. Dialogues in Human Geography 4(1) 39-44. doi: 10.1177/2043820614525712\nMillington JDA, D O’Sullivan and GLW Perry. 2012. Model histories: Narrative explanation in generative simulation modelling. Geoforum 43(6) 1025-1034. doi: 10.1016/j.geoforum.2012.06.017\nMueller S, DJ Exeter, H Petousis-Harris, N Turner, D O’Sullivan and CD Buck. 2012. Measuring disparities in immunisation coverage among children in New Zealand. Health and Place 18(6) 1217-1223. doi: 10.1016/j.healthplace.2012.08.003\nHong S-Y and D O’Sullivan. 2012. Detecting ethnic residential clusters using an optimisation clustering method. International Journal of Geographical Information Science 26(8) 1257-1277. doi: 10.1080/13658816.2011.637045\nXue J, W Friesen and D O’Sullivan. 2012. Diversity in Chinese Auckland: Hypothesising multiple ethnoburbs. Population, Space and Place 18 579-595. doi: 10.1002/psp.688\nMavoa S, K Witten, T McCreanor, and D O’Sullivan. 2012. GIS based destination accessibility via public transit and walking in Auckland, New Zealand. Journal of Transport Geography 20(1) 15-22. doi: 10.1016/j.jtrangeo.2011.10.001\nMateos P, PA Longley, and D O’Sullivan. 2011. Ethnicity and Population Structure in Personal Naming Networks. PLoS ONE 6(9) e22943. doi: 10.1371/journal.pone.0022943\nPearson J, R Lay-Yee, P Davis, D O’Sullivan, M von Randow, N Kerse and S Pradhan. 2011. Primary care in an aging society: Building and testing a microsimulation model for policy purposes. Social Science Computer Review 29(1) 21-36. doi: 10.1177/0894439310370087\nO’Sullivan D. 2009. What’s critical about critical GIS? Cartographica 44(1) 7-8. doi: 10.3138/carto.44.1.5\nO’Sullivan D and G. L. W. Perry. 2009. A discrete space model for continuous space dispersal processes. Ecological Informatics 4(2) 57-68. doi: 10.1016/j.ecoinf.2009.03.001\nO’Sullivan D. 2009. Changing neighborhoods – neighborhoods changing: a framework for spatially explicit agent-based models of social systems. Sociological Methods and Research 37(4) 498-530. doi: 10.1177/0049124109334793\nReardon SF, CR Farrell, SA Matthews, D O’Sullivan, K Bischoff and G Firebaugh. 2009. Race and space in the 1990s: changes in the geographic scale of racial residential segregation, 1990-2000. Social Science Research 38 55-70. doi: 10.1016/j.ssresearch.2008.10.002\nLee BA, SF Reardon, G Firebaugh, CR Farrell, SA Matthews and D O’Sullivan. 2008. Beyond the census tract: patterns and determinants of racial segregation at multiple geographic scales. American Sociological Review 73(October) 766-791. doi: 10.1177/000312240807300504\nReardon SF, SA Matthews, D O’Sullivan, BA Lee, G Firebaugh, CR Farrell and K Bischoff. 2008. The geographic scale of metropolitan segregation. Demography, 45(3) 489-514. doi: 10.1353/dem.0.0019\nO’Sullivan D. 2008. Geographical information science: agent-based models. Progress in Human Geography 32(2) 541-550. doi: 10.1177/0309132507086879\nO’Sullivan D and DWS Wong. 2007. A surface-based approach to measuring spatial segregation. Geographical Analysis 39(2) 147-168. doi: 10.1111/j.1538-4632.2007.00699.x\nO’Sullivan D. 2006. Geographical information science: critical GIS. Progress in Human Geography 30(6) 783-791. doi: 10.1177/0309132506071528\nRygel L, D O’Sullivan and B Yarnal. 2006. A method for constructing a social vulnerability index: an application to hurricane storm surges in a developed country. Mitigation and Adaptation Strategies for Global Change 11(3) 741-764. doi: 10.1007/s11027-006-0265-6\nManson SM and D O’Sullivan. 2006. Complexity theory in the study of space and place. Environment and Planning A 38(4) 677-692. doi: 10.1068/a37100\nO’Sullivan D, JP Messina, SM Manson and TW Crawford. 2006. Space, place, and complexity science. Environment and Planning A 38(4) 611-617. doi: 10.1068/a3812\nO’Sullivan D. 2005. Geographical information science: time changes everything. Progress in Human Geography 29(6) 749-756. doi: 10.1191/0309132505ph581pr\nCrawford TW, JP Messina, SM Manson and D O’Sullivan. 2005. Complexity science, complex systems, and land-use research. Environment and Planning B: Planning & Design 32(5) 792-798. doi: 10.1068/b3206ed\nReardon SF and D O’Sullivan. 2004. Measures of Spatial Segregation. Sociological Methodology 34(1) 121-162. doi: 10.1111/j.0081-1750.2004.00150.x\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. doi: 10.1111/j.0020-2754.2004.00321.x\nO’Sullivan D. 2002. Toward micro-scale spatial modelling of gentrification. Journal of Geographical Systems 4(3) 251-274. doi: 10.1007/s101090200086\nO’Sullivan D. 2001. Graph-cellular automata: a generalised discrete urban and regional model. Environment and Planning B: Planning & Design 28(5) 687-705. doi: 10.1068/b2707\nHaklay M, T Schelhorn, D O’Sullivan and M Thurstain-Goodwin. 2001. “So go down town”: Simulating pedestrian movement in town centres. Environment and Planning B: Planning & Design 28(3) 343-359. doi: 10.1068/b2758t\nTorrens PM and D O’Sullivan. 2001. Cellular automata and urban simulation: where do we go from here? Environment and Planning B: Planning & Design 28(2) 163-168. doi: 10.1068/b2802ed\nO’Sullivan D and A Turner. 2001. Visibility graphs and landscape visibility analysis. International Journal of Geographical Information Science 15(3) 221-237. doi: 10.1080/13658810151072859\nTurner A, M Doxa, D O’Sullivan and A Penn. 2001. From isovists to visibility graphs: a methodology for the analysis of architectural space. Environment and Planning B: Planning & Design 28(1) 103-121. doi: 10.1068/b2684\nO’Sullivan D. 2001. Exploring spatial process dynamics using irregular cellular automaton models. Geographical Analysis 33(1) 1-18. doi: 10.1111/j.1538-4632.2001.tb00433.x\nO’Sullivan D. and M Haklay. 2000. Agent-based models and individualism: is the world agent-based? Environment and Planning A 32(8) 1409-1425. doi: 10.1068%2Fa32140\nO’Sullivan D, A Morrison and J Shearer. 2000. Using desktop GIS for the investigation of accessibility by public transport: an isochrone approach. International Journal of Geographical Information Science 14(1) 85-104. doi: 10.1080/136588100240976"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters\n\nBergmann LR and D O’Sullivan. 2024. Space: towards a global sense of place. In A Research Agenda for Spatial Analysis (eds LJ Wolf, R Harris and AJ Heppenstall). Elgar Research Agendas. Edward Elgar Publishing. doi: 10.4337/9781802203233.00009.\nC Andris and D O’Sullivan. 2019. Spatial Networks. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer. doi: 10.1007/978-3-642-36203-3_67-1.\nD O’Sullivan. 2018. Cartography and geographic information systems. In J Ash, R Kitchin and A Leszczynski (eds), Digital Geographies, (Los Angeles: Sage), pages 118-128.\nD O’Sullivan. 2018. Big data … why (oh why?) this computational social science? In JE Thatcher, J Eckert and A Shears (eds), Thinking Big Data in Geography: New Regimes, New Research, (Lincoln: University of Nebraska Press), pages 21-38. doi: 10.2307/j.ctt21h4z6m.7\nLR Bergmann and D O’Sullivan. 2017. Computing with many spaces: Generalizing projections for the digital geohumanities and GIScience. In Proceedings of GeoHumanities’17: 1st ACM SIGSPATIAL Workshop on Geospatial Humanities, Redondo Beach, CA, November 7-10 (GeoHumanities’17), pages 31-38. doi: 10.1145/3149858.3149866\nThatcher JE, LR Bergmann and D O’Sullivan. 2016. Searching for common ground (again). In Short Paper Proceedings of 9th International Conference on Geographical Information Science, eds JA Miller, D O’Sullivan and N Wiegand, (Montreal, Canada), pages 304-307. doi: 10.21433/B3118nq409qz\nPfeffer K, J Martinez, D O’Sullivan and D Scott. 2015. Geo-Technologies for Spatial Knowledge: Challenges for Inclusive and Sustainable Urban Development. In J Gupta, K Pfeffer, H Verrest and M Ros-Tonen (eds), Geographies of Urban Governance (Cham: Springer International Publishing), pages 147-173. doi: 10.1007/978-3-319-21272-2_8\nO’Sullivan D. 2014. Spatial Network Analysis. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer, pages 1253-1273. doi: 10.1007/978-3-642-23430-9_67\nO’Sullivan D, JDA Millington, GLW Perry and J Wainwright. 2012. Agent-Based Models – Because They’re Worth It? In AJ Heppenstall, AJ Crooks, LM See, and M Batty (eds), Agent-Based Models of Geographical Systems (Springer: Dordrecht, Netherlands), pages 109-123. doi: 10.1007/978-90-481-8927-4_6\nHeppenstall AJ, AJ Evans, MH Birkin, JR Macgill and D O’Sullivan. 2005. The Use of Hybrid Agent-Based Systems to Model Petrol Markets. In T Terano, H Kita, H Kaneda, K Arai and H Deguchi (eds), Agent-Based Simulation: From Modelling Methodologies to Real-World Applications, Springer Series on Agent-Based Social Systems, pages 154-162. doi: 10.1007/4-431-26925-8_17\nO’Sullivan D. 2004. Too much of the wrong kind of data: implications for the practice of micro-scale spatial modelling. In MF Goodchild and D Janelle (eds), Spatially Integrated Social Science: Examples of Best Practice (Oxford University Press: Oxford), pages 95-107.\nO’Sullivan D, JR Macgill and C Yu. 2003. Agent-based residential segregation: a hierarchically structured spatial model. In C Macal, M North and D Sallach (eds), Agent 2003: Challenges in Social Simulation, pages 493-507 (Argonne National Laboratory: Chicago). www.agent2004.anl.gov/Agent2003.pdf\nO’Sullivan D. 2002. Understanding the difference that space can make: toward a geographical agent modeling environment. In C. Macal and D. Sallach (eds), Agent 2002: Ecology, Exchange, and Evolution, pages 13-25 (Argonne National Laboratory: Chicago). www.agent2003.anl.gov/proceedings/2002.pdf\nO’Sullivan D and PM Torrens. 2001. Cellular models of urban systems. In S Bandini and T Worsch (eds), Theoretical and Practical Issues on Cellular Automata, Proceedings of the Fourth International Conference on Cellular Automata for Research and Industry (ACRI 2000), October 4–6, Karlsruhe, Germany (Springer-Verlag: London), pages 108-116. [Also available as CASA Working Paper 22 at www.casa.ucl.ac.uk/cellularmodels.pdf"
  },
  {
    "objectID": "publications.html#dissertations",
    "href": "publications.html#dissertations",
    "title": "Publications",
    "section": "Dissertations",
    "text": "Dissertations\n\nPhD: O’Sullivan D. 2000. Graph-based Cellular Automaton Models of Urban Spatial Processes. University College London.\nMSc: O’Sullivan D. 1997. Using GIS to create public transport travel time isochrones for the Glasgow area. University of Glasgow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is GEOSPATIAL STUFF where I offer expertise in data analytics and visualisation, simulation modelling, and training on all things geospatial. I am an experienced geospatial researcher, an Honorary Professor at the University of Auckland, and Research Affiliate at Motu. Find examples of current work in my portfolio and blog posts. For more information get in touch.\n\n\n\n\nCommmunicating stuff\nI have written three books and many research articles. I have presented complex ideas to diverse audiences for 20+ years: see my presentations, or these slides. Based on my decades of teaching experience I offer training to help organisations join up their data analytics and GIS teams using modern geospatial tools.\n\n\n\nBuilding things\nI develop workflows to organise and analyse data, especially spatial data. I have built visualizations of complex data for many projects. You’ll find examples among my posts. I write well documented code and can prototype websites as needed. My web work is linked from these pages, just look around."
  },
  {
    "objectID": "training/02-geographical-computing.html",
    "href": "training/02-geographical-computing.html",
    "title": "Geographical computing",
    "section": "",
    "text": "Go to the materials\n\nThese pages outline a one semester (36 contact hours) class in python programming for geospatial that was last taught at Victoria University of Wellington as GISC 420 in the first half of 2022.\nI am still in the process of cleaning the materials up for potential conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to python programming for geospatial but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html",
    "title": "ALGIM 2024",
    "section": "",
    "text": "What is ALGIM? According to the website,\nand\nSo ALGIM is a place where local government ICT people can pool resources and expertise to make the challenging business of doing local government a little bit easier. To that end ALGIM offers training, webinars, forums for the exchange of ideas. One of those forums is the annual conference.2"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "title": "ALGIM 2024",
    "section": "An industry conference not an academic conference",
    "text": "An industry conference not an academic conference\nALGIM’s annual conference is a showpiece event. This year the conference theme was ‘Driving collaboration’, approached from a range of perspectives. Topics such as AI in local government, standardisation of processes and plaforms, and fostering collaboration loomed large. There were interesting keynotes from an eclectic mix of people such as :Audrey Tang, Nicole Skews-Poole, Julian Moore, and Michael Baker.\nBut the ALGIM conference is certainly not an academic conference. I’m a veteran of those having attended 50 or so in the last two decades, and if you add in symposia, workshops, and sundry other varieties of academic meeting, a few more even than that. The rhythms of these are familiar: 20 minute talks in sessions an hour or two long, with breaks for refreshments, and (for the most part) not much connection to the world of work beyond the meeting. Obviously everyone participating is at work, and people gossip about work in the breaks, but the subject matter under discussion in sessions is not work itself, but scientific and scholarly questions of one kind or another.\nAnyway, this was not that, but a rather different kind of event. It’s very much a networking meeting for people with shared professional interests3 and as such absolutely about work. I do have some previous experience of this kind of event4 and knew going in that in addition to keynotes and shorter presentations on more specialised topics, many technology vendors would be there hawking their wares, or perhaps more realistically simply reminding local government ICT managers that they exist, should they be considering revisiting their backup, comms, data security, disaster recovery, online payment systems, or whatever other ICT options."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "title": "ALGIM 2024",
    "section": "Wot? No GIS?",
    "text": "Wot? No GIS?\nAnd that, to a ‘GISer’, was the most interesting aspect. I fully expected to see all the usual local geospatial suspects here. Esri’s local representative Eagle Technology was present, albeit not in force (mostly they were handing out ice cream!), along with one or two others, such as GBS and local FME partner Seamless, but that was more or less it. No Koordinates, Catalyst, Orbica, Lynker Analytics, or LINZ.\nNow… that might be because (i) nobody goes to a conference to buy enterprise GIS, or even to change their enterprise GIS plans, or (ii) Esri/Eagle’s position in GIS in local government in New Zealand is so strong that there isn’t much point in anybody else showing up. On the other hand, it might be because… well, because however often geospatial folks tell themselves (and anyone who will listen) that 80%—or some other high percentage!—of all data are spatial, and no matter how self-evidently spatial it seems that the job of local government might be, when it comes to information management in local government, GIS is not much of a thing.\nAnd yet, GIS clearly is important in local government, even if it is treated as slightly peripheral by ICT managers."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "title": "ALGIM 2024",
    "section": "What was I doing there anyway?",
    "text": "What was I doing there anyway?\nI was there to chair a panel discussion on current challenges/developments in geospatial (AI, digital twins, barriers to collaboration, convergence with data analytics, education and training), and to convene a ‘GIS Round Table discussion’.\nBoth sessions were interesting, although the panel discussion time frame was a little too short at only half an hour to really get into things with the excellent contributors Andrew Steffert from Horizons Regional Council, Kerri Gray from CreateBig, and Scott Campbell from Eagle.\nBut with all due deference to those experts, the Round Table was, I thought, more telling. It wasn’t very big with about a dozen in attendance, but it was fairly representative with ‘GIS officers’ (for want of a better term) from councils big and small. The striking thing was how quickly even this very limited forum became a source of support and consultation among the people around the table. The strong sense I got was of GIS officers overwhelmed by the numerous expectations around their role(s). There was also a sense, especially from smaller councils where there might be only one or two people ‘doing GIS’, that those people feel isolated with limited options to reach out for assistance. And so, within minutes of introductions, people were comparing notes on how others had approached various common tasks and what technologies or data resources they were using. There really is nothing quite like getting people in a room together for learning what their challenges are.\nIn short, however tacked on that “and GIS” might appear in ALGIM’s mission statement, the GIS officers in local government around New Zealand (and I imagine also Australia) seem more than ready to take advantage of whatever forums ALGIM might be able to offer for learning, exchanging ideas, sharing approaches, and collaborating. Hopefully, this can happen soon, since it doesn’t seem like the job of GIS officer in local government is going to get any easier any time in the near future!"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "title": "ALGIM 2024",
    "section": "PS Not forgetting",
    "text": "PS Not forgetting\n\n\n\nThe Hamilton City Kai Map\n\n\nI would be remiss if I didn’t also mention seeing some really nice presentations of award nominated geospatial projects such as the Hamilton City Kai Map, Northland Regional Council’s use of geospatial to integrate the Health and Safety processes associated with their numerous fieldwork activities, and a Christchurch City map-centred streamling of their building warrant of fitness process (i.e., building inspections). The latter two are internal work process streamlining projects so unfortunately don’t have public websites I can link to."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "title": "ALGIM 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomebody always has to come last, but park that slightly tacked on feeling “and GIS” just for now…↩︎\nALGIM now also has an Australian branch, which will see the organisation grow substantially in the next few years.↩︎\nAcademic conferences are also that, but with a different emphasis.↩︎\nAs I’ve recounted elsewhere it was at a meeting like this that I chanced upon GIS. If you’re interested it’s mentioned about 2 minutes into this podcast.↩︎"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "title": "Random points on the globe revisited",
    "section": "",
    "text": "NOTE: There were a few over-simplifications in the original version of this post, which I’ve done my best to correct. Turns out point patterns on the globe are even more complicated than I realised…\nThis holiday season I had reason to revisit a post from November 2021 about generating evenly distributed random points on the globe. This was in the course of writing (for fun) a NetLogo simulation of the boardgame Lacuna. You can find the simulation here. The simulation isn’t very developed and it turns out that the web version of NetLogo doesn’t implemented some functions important to its operation, so pending developing my Javascript skills further that’s likely where it will remain.\nAnyway, the setup rules for the physical version of Lacuna stipulate that the players should\nWriting the code for this immediately had me reaching for a sequential spatial inhibition process using spatstat in R, or for that matter an implementation of it in NetLogo1 This also reminded me that spatial inhibition processes require an inhibition distance to be specified, and got me wondering if there is a spatial point process that generates randomly distributed points ‘without clumping’ but with no need to specify a spacing parameter.\nThis is what is known as a rabbit hole.\nFor whatever reason, there doesn’t seem to be a widely used spatial point process model with this property, but there are :low discrepancy sequences most often used for balanced sampling in operations like Monte Carlo simulation that can generate evenly spaced spatial patterns in two dimensions. In this post, I look at a couple of these along with some other alternatives, in the context set by my earlier post of generating random point patterns of even intensity on the globe."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "title": "Random points on the globe revisited",
    "section": "Some preliminaries",
    "text": "Some preliminaries\nTo make things easier, I am going to generate patterns in latitude-longitude space, and then transforming them to patterns on the globe. Before we get started here are all the libraries I’m using.\n\nlibrary(rnaturalearth)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\n1library(spbal)\n2library(spatstat)\nlibrary(ggplot2)\nlibrary(cols4all)\n\ntheme_set(theme_minimal())\ntheme_update(axis.title = element_blank())\n\n3set.seed(1)\n\nprojection &lt;- \"+proj=moll\"\naspect_ratio &lt;- 2\nR &lt;- 6371000\nn_points &lt;- 1152\n\n\n1\n\nspbal provides Halton sequences.\n\n2\n\nspatstat provides a wide range of spatial point processes.\n\n3\n\nFor replicability.\n\n\n\n\nAnd we’ll set up a globe polygon and a world map.\n\nangles &lt;- 0:719 / 720 * 2 * pi\nangles &lt;- c(angles, angles[1])\nx &lt;- R * cos(angles) * 2 * sqrt(aspect_ratio)\ny &lt;- R * sin(angles) * 2 / sqrt(aspect_ratio)\n\nglobe &lt;- c(x, y) |&gt; \n  matrix(ncol = 2) |&gt;\n  list() |&gt;\n  st_polygon() |&gt;\n  st_sfc() |&gt;\n  st_as_sf(crs = projection)\n\nworld &lt;- ne_countries() |&gt;\n  select() |&gt;\n  st_transform(projection)\n\nAnd make a quick map to make sure everything is in order.\n\n\n\n\nggplot() +\n  geom_sf(data = globe, fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0)\n\n\n\n\n\n\n\n\n\n\nFigure 1: World map in Mollweide"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-the-unit-square",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-the-unit-square",
    "title": "Random points on the globe revisited",
    "section": "Patterns in the unit square",
    "text": "Patterns in the unit square\n\nA simple random pattern\nWe can make this by drawing x and y coordinates from uniform distributions. The \\(y\\) coordinate is transformed using the inverse cosine operation discussed in my original post.\n1plot_pp &lt;- function(df) {\n  ggplot() +\n    geom_point(data = df, aes(x = x, y = y)) +\n2    coord_equal(expand = FALSE) +\n3    theme(panel.background = element_rect(fill = NA))\n}\n\npattern1 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = acos(runif(n_points, -1, 1)) / pi * 180 - 90,\n                   generator = \"Uniform random\")\nplot_pp(pattern1)\n\n\n1\n\nConvenient to have a single function for plotting point patterns.\n\n2\n\nexpand = FALSE stops ggplot adding a margin around the unit square.\n\n3\n\nIt’s good in these plots to see the unit square!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simple uniform random pattern\n\n\n\n\n\nA pattern from spatial point process\nMy first thought here was to use a sequential spatial inhibition process. This is a spatial point process where points are generated at random locations, but rejected if they are closer than some inhibition distance to an existing point already in the pattern. This is easily done in Euclidean space using the spatstat::rSSI function. In latitude-longitude space this won’t work because distances are not calculable using the Pythagorean function on coordinates.\nInstead, I have opted for the uniform point process (just as above), but specifying a tiling (a tessellation) of the space with approximately equal-area tiles. Using the formula for the y coordinate of a cylindrical equal-area projection \\(y=sin\\phi\\), we can generate approximately equal-area rectangles in latitude-longitude space as below. I emphasise the approximation here, because it is only approximate. Rectangles in lat-lon space are not rectangles on the globe after all.\n1nx &lt;- sqrt(n_points / 2) * 2 + 1\nny &lt;- sqrt(n_points / 2) + 1\n2xg &lt;- seq(-1, 1, length.out = nx) * 180\nyg &lt;- asin(seq(-1, 1, length.out = ny)) / pi * 180\n\npp2 &lt;- runifpoint(n = 1, win = tess(xgrid = xg, ygrid = yg))\npattern2 &lt;- pp2 |&gt;\n  as.data.frame() |&gt;\n  mutate(generator = \"Stratified point process\")\nplot_pp(pattern2) +\n  geom_point(data = expand_grid(xg, yg), aes(x = xg, y = yg), \n             colour = \"red\", shape = 3, size = 0.5)\n\n\n1\n\nThe number of gridlines we need in each direction is determined here.\n\n2\n\nx coordinates are trivially equally spaced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Stratified random point process\n\n\n\nI’ve plotted the coordinates of the grid used to ‘thin’ the points at high latitudes for reference. The process generates only one point in each of these cells. However… the thinning is not continuous and there are a number of points very close to the poles, where of course the available area is zero!\n\n\nQuasi-random sequences\nThese are the processes that were new to me, which showed up when I started looking for R packages that could do even sampling in 2D spaces. The first package to show up was spacefillr. This implements a number of such sequences, but we’ll look here at the Halton sequence, because its workings are relatively easy to understand, and the implementation in the spbal package provides more options.\n:Halton sequences are generated using a pair of coprime numbers, i.e., two numbers with no common factors. The simplest example is 2 and 3. Each of the selected numbers specifies a sequence by repeated subdivision of the interval 0 to 1. So for 2 we get \\[\n\\frac{1}{2},\\frac{1}{4},\\frac{3}{4},\\frac{1}{8},\\frac{5}{8},\\frac{3}{8},\\frac{7}{8},\\ldots\n\\] and for 3 we get \\[\n\\frac{1}{3},\\frac{2}{3},\\frac{1}{9},\\frac{4}{9},\\frac{7}{9},\\frac{2}{9},\\frac{5}{9},\\ldots\n\\] Pairing these sequences gives us coordinates of points in the unit square. Different generating numbers can be chosen (provided they are coprime) and different starting points in each sequence can be paired, to give a wide variety of deterministically generated quasi-random patterns. The spbal package provides a highly configurable interface to generate such sequences using the cppRSHalton_br function. We can see the procedures inner workings clearly by examining the first few elements in the sequence:\n\ncppRSHalton_br(10, bases = 2:3, seeds = 0)$pts\n\n        [,1]       [,2]\n [1,] 0.0000 0.00000000\n [2,] 0.5000 0.33333333\n [3,] 0.2500 0.66666667\n [4,] 0.7500 0.11111111\n [5,] 0.1250 0.44444444\n [6,] 0.6250 0.77777778\n [7,] 0.3750 0.22222222\n [8,] 0.8750 0.55555556\n [9,] 0.0625 0.88888889\n[10,] 0.5625 0.03703704\n\n\nThese appear entirely regular, and some regularity is evident in the patterns generated (see Figure 4), although it is less apparent than might be anticipated on considering the numerical values alone. The main interest in Halton sequences is their desirable evenness of distribution for sampling purposes, which is apparent in Figure 4.\npattern3 &lt;- cppRSHalton_br(n_points, \n1                           bases = c(2, 3),\n2                           seeds = c(14, 21))$pts |&gt;\n  as.data.frame() |&gt;\n  rename(x = V1, y = V2) |&gt; \n  mutate(x = x * 360 - 180, \n         y = acos(y * 2 - 1) / pi * 180 - 90,\n         generator = \"Halton\")\nplot_pp(pattern3)\n\n\n1\n\nThe coprime generating values.\n\n2\n\nThe starting positions in the sequence for each generating value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Points generated by a Halton sequence\n\n\n\nDetails concerning generation of Halton sequences and their statistical properties are provided by Faure and Lemieux.2\n\n\nA ‘home-grown’ parameter-free pattern generator\nHere I use a seemingly trivial (but not very efficient!) algorithm to generate some home-made evenly distributed points, without the need to specify any (spatial) parameter like the inhibition distance required by rSSI. I found the inspiration for this in this detailed blog post about generating :blue noise, which was in turn based on an algorithm described by Mitchell.3\nMatters are complicated by having to calculate distances in latitude-longitude space, which we do using the :Haversine formula \\[\nd=2 r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\varphi_2 - \\varphi_1}{2}\\right) + \\cos(\\varphi_1) \\cos(\\varphi_2)\\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\n\\] for the distance between two lon-lat locations\\(\\left(\\lambda_1,\\varphi_1\\right)\\) and \\(\\left(\\lambda_2,\\varphi_2\\right)\\), although because we only need the relative distances we don’t use the \\(2r\\) scaling. The need to calculate toroidal distances described in the blogpost is obviated by calculating distances on the sphere which wraps in a similar way.\nThe simple idea of this algorithm is that each time we add a new point we generate a set of points (candidates) to choose from, and select the one with the largest minimum distance to an existing point in the pattern. Making the algorithm more efficient would involve only checking the distance to points known to be close to candidate points using some kind of spatial index or binning structure.\nAt a lower implementation level there’s likely a quicker way to do the distance calculations between every candidate point and every point already in the pattern than a nested loop in R (a construct generally best avoided…).\n\n1ll_distances &lt;- function(p1, p2) {\n  distances &lt;- matrix(0, nr = nrow(p1), nc = nrow(p2))\n  for (i in 1:nrow(p1)) {\n    for (j in 1:nrow(p2)) {\n      lon1 &lt;- p1[i, 1] \n      lat1 &lt;- p1[i, 2]\n      lon2 &lt;- p2[j, 1] \n      lat2 &lt;- p2[j, 2]\n      distances[i, j] &lt;- asin(sqrt(\n        (sin((lat2 - lat1) / 2)) ^ 2 +\n        cos(lat1) * cos(lat2) * sin((lon2 - lon1) / 2) ^ 2\n      ))\n    }\n  }\n  distances\n}\n\nrescale &lt;- function(x, x1min, x2min, x1max, x2max) {\n  x2min + (x - x1min) / (x1max - x1min) * (x2max - x2min)\n}\n\nspaced_points &lt;- function(n = 50, choice_scaling = 1.5,\n                          input_bb = c(0, 0, 1, 1),\n                          output_bb = c(0, 0, 1, 1), dist_fn) {\n  points &lt;- c(runif(1, input_bb[1], input_bb[3]), \n              runif(1, input_bb[2], input_bb[4])) |&gt; \n    matrix(ncol = 2)\n  for (i in 1:(n-1)) {\n2    n_candidates &lt;- ceiling(log(i * exp(1) * choice_scaling))\n    candidates &lt;- c(runif(n_candidates, input_bb[1], input_bb[3]), \n                    runif(n_candidates, input_bb[2], input_bb[4])) |&gt; \n      matrix(ncol = 2)\n    r_max &lt;- dist_fn(candidates, points) |&gt; \n3      apply(1, min) |&gt;\n      which.max()\n    points &lt;- rbind(points, candidates[r_max, ])\n  }\n  points |&gt; \n    as.data.frame() |&gt;\n    rename(x = V1, y = V2) |&gt;\n    mutate(x = rescale(x, input_bb[1], output_bb[1], input_bb[3], output_bb[3]),\n           y = rescale(y, input_bb[2], output_bb[2], input_bb[4], output_bb[4]),\n           generator = \"Blue noise\")\n}\n\n\n1\n\nRelative lat-lon distances determined using :Haversine formula.\n\n2\n\nThe choice_scaling parameter determines how rapidly the number of candidate points grows with the size of the existing data set. It should be strictly greater than 1 or no points will ever get added! Including a log factor stops the speed of the algorithm from falling too rapidly as points are added.\n\n3\n\nThe apply(1, min) operation finds the smallest distance in each row (i.e. distance to nearest neighbour in the existing set of points), and which.max() identifies the row with the largest minimum distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: A pattern generated using a ‘blue noise’ algorithm by iteratively choosing the most remote random additional point among a set of choices"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "title": "Random points on the globe revisited",
    "section": "Putting all these points on the globe",
    "text": "Putting all these points on the globe\nWe now apply a transformation from the unit square to the globe that is equal-area as shown in the previous post. We can combine all the points into a single data set and use facet_wrap for side-by-side comparison.\n\n\n\n\ntransform_to_globe &lt;- function(df, proj) {\n  df |&gt;\n    st_as_sf(coords = c(\"x\", \"y\"), crs = 4326) |&gt; \n    st_transform(proj)  \n}\n\nall_points &lt;- bind_rows(pattern1, pattern2, pattern3, pattern4, pattern5) |&gt;\n  transform_to_globe(projection) |&gt;\n  mutate(generator = ordered(generator, c(\"Uniform random\", \n                                          \"Uniform random cosine-corrected\",\n                                          \"Stratified point process\",\n                                          \"Halton\", \"Blue noise\")))\nggplot(globe) +\n  geom_sf(fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0) +\n  geom_sf(data = all_points, size = 0.35, colour = \"red\") +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nFigure 7: Side-by-side comparison of five patterns projected onto the globe\n\n\n\nWith the obvious exception of the uniform random pattern, which exhibits definite clumping, all of these seem to do a reasonable job of producing a random arrangement of points evenly distributed over the globe. The stratified point process as mentioned above may have too many points close to the poles.\nFrom the perspective of avoiding clumping we can try to make the comparison slightly more precise by making hexbin density plots of the points.\nxy &lt;- all_points |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  bind_cols(all_points)\n\nnx &lt;- sqrt(n_points / pi) * 2 * sqrt(aspect_ratio)\nny &lt;- nx * 2 / sqrt(3) / sqrt(aspect_ratio)\n\nggplot(xy) +\n  geom_hex(aes(x = X, y = Y, fill = as.factor(after_stat(count))), \n           bins = c(nx, ny), linewidth = 0.1, colour = \"#666666\") +\n1  scale_fill_manual(values = c4a(\"brewer.yl_gn_bu\", 11),\n                    breaks = 1:10, guide = \"legend\", name = \"#Points\") +\n  annotate(geom = \"path\", x = x, y = y, linewidth = 0.2) +\n  coord_equal() +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n1\n\nggplot2::geom_hex insists that the point counts are ‘continuous’ which is what forces me to convert at a factor, so that I can use a legend not a colour ramp here. Another instance of ggplot2’s strong preference for not classing data, see this recent post in the context of choropleth maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Side-by-side comparison of hexbin density plots of the five point patterns\n\n\n\nThe uniform random pattern clearly exhibits the most uneven distribution of points. The cosine-corrected version is better although there is still unevenness in the mid-latitudes, as we would expect because there is no interaction between points in the pattern: the presence of a previous point does not block another point showing up close by. The stratified point process and Halton patterns are better again. The latter of these is particularly surprising, given its completely deterministic generative process.\nSomewhat to my surprise my ‘homebrew’ blue noise compares well with the other four patterns for evenness of distribution. This is particularly nice given that it requires no spatial parameter to be supplied, only one that (perhaps only marginally) affects the quality of the patterns produced."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "title": "Random points on the globe revisited",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHaving said that, the circular game board eventually led me to generate the patterns in NetLogo by randomly displacing each new flower from the centre of the circle by a random distance in a random direction, and retrying until they were no closer to another flower than some set distance.↩︎\nFaure H and C Lemieux. 2009. Generalized Halton sequences in 2008: A comparative study. ACM Transactions on Modeling and Computer Simulation 19(4) 15:1-15:31.↩︎\nMitchell DP. 1991. Spectrally optimal sampling for distribution ray tracing. SIGGRAPH Computer Graphics. 25(4) 157–164.↩︎"
  },
  {
    "objectID": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "href": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "title": "Spiral origami",
    "section": "",
    "text": "I made an observable notebook to generate crease patterns for the flat-foldable origami ‘whirpool spirals’ designed by Tomoko Fuse and presented in chapter 3 of her amazing book Spiral: Origami|Art|Design. You’ll find more details in the notebook.\n\nThe picture is an example of the folds it produces, or at any rate of the things you can fold by cutting out and following the crease patterns it produces. You’ll have to do the folding yourself, I’m afraid.\nIf you’d like an idea of how the fold works without going to all the trouble of actually making one, then visit Amanda Ghassaei’s Origami Simulator and navigate to Examples - Tessellations - Whirlpool Spiral."
  },
  {
    "objectID": "posts/2020-07-22-nz-commute-viewer/nz-commute-viewer.html",
    "href": "posts/2020-07-22-nz-commute-viewer/nz-commute-viewer.html",
    "title": "Aotearoa New Zealand commutes viewer",
    "section": "",
    "text": "I somehow convinced myself in the break between two exhausting semesters in 2020 (yes, that year) to enter Stats NZ’s There and back again visualization competition.\nI learned a lot about a bunch of things along the way, but probably needed to take a step back and learn more about web development frameworks, as there is altogether too much handcrafted javascript in there. Nevertheless, I had fun and I am reasonably happy with the end result. Also… it still works over four years later, which might not be true if I’d used something smarter than vanilla JS.\n\nGo to the viewer\n\nHere’s a screenshot for anyone not curious enough to click on the link.\n\nNo, I didn’t win the competition. That was this entry by Jono Cooper."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "",
    "text": "I wrote a paper waaaay back in 20041 which among other things was a plea for qualitatively inclined geographers to take complexity science seriously, as a sincere attempt to understand the world as it is, in all its, uh… complexity. It’s a tricky argument to make because the preferred tools of complexity science are computational or mathematical models, which are necessarily simplifications. Models, after all, are only useful if we can make sense of them and that’s only likely to be possible if they simplify the complex realities we are trying to understand.2\nThe paper has been widely cited, but a glance at where it has been cited suggests that my its readership has mostly been already sympathetic fellow-travellers in quantitative geography, not the more diverse audience I was hoping for, from across the discipline and beyond. While writing the paper I sent a draft to :Doreen Massey who was enthusiastic. The paper is framed as a response to her calls around that time for more dialogue across the divide between human and physical geography. The emergence of critical physical geography3 suggests that Massey’s call has not gone unheard.\nIt would be absurd to suggest that my paper has had even a fraction of that impact. Equally, it would be wrong to think that geography as a discipline has enthusiastically embraced the more holistic and open attitude to methods that taking seriously Massey’s call (or my paper) would entail.\nBut wait!"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Systems thinking spotted in the wild",
    "text": "Systems thinking spotted in the wild\nA workshop I was at last week suggests that maybe (just maybe) under the guise of systems thinking, complexity science might be starting to have an impact on more qualitative approaches.\nThe workshop was a gathering of some of the many researchers involved in the Moving the Middle (MtM) project down in Christchurch, ahead of the Agents of Change team’s Know Your Place environment + art event in Lyttelton Harbour. One of the presentations to the assembled team on the first morning of the workshop took me by (pleasant) surprise, as it was on :systems thinking. And one of the first things to be put on the screen was some version of the diagram below.\n\n\n\n\n\n\nFigure 1: A particularly complicated systems diagram—which is nevertheless a simplification4\n\n\n\nThis was part of an enjoyable talk by Nick Cradock-Henry summarising the elements of systems thinking, which the Agents of Change team have really picked up and run with in the last year as a framework for organising their thinking around the wider somewhat disparate MtM project.\nAs a long-time fan of :complexity science, which I consider to be either an evolution from systems thinking or a broader framework within which systems thinking sits,5 this was a big deal. I’ve wanted for years to see qualitatively inclined social scientists—the kind that talk about more-than-human geographies and such—to whole-heartedly engage with systems thinking!\nI’m not really sure what’s brought this on. Maybe it’s recent excitement about the :circular economy? Or a delayed reaction to COVID models? (Unlikely) Or maybe :Te Pūnaha Matatini is getting some traction with a wider audience? Whatever the reason, I’m glad it’s happening."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Making models and having conversations",
    "text": "Making models and having conversations\nThe primary tool of systems thinking the Agents of Change team seem to have been working with so far is causal loop diagrams (see Figure 1), which were ably explained both by Nick Cradock-Henry and by Justin Connolly. What this take on complexity/systems thinking brings to the fore is the importance of creating models (in this case primarily a visual model) collaboratively as things to talk about in a constructive way as groups of people try to understand some problem at hand.\nThe point of such models is not simulation or prediction, but understanding. Not even understanding necessarily, but arriving at sufficient agreement in a particular problem solving context about what makes things tick, so that useful conversations can be had.\nIf visual models eventually form a starting point for building system dynamics models, or agent-based models, or other kinds of computational models, that’s fine. But it’s also fine if that doesn’t happen. In fact, computational models can muddy the waters. They are expected or required to be predictive, and everyone becomes fixated on prediction and stops thinking. Or the model becomes a fall-guy (‘the model told me to do it!’). Or models are dragged into a role in monitoring and management that they weren’t designed for.6 When an organisation invests in building a simulation model, the chance of it being drafted into use for purposes well beyond its original scope are high, often with unintended consequences.\nOnce a computational model exists, there’s a danger of thinking that the topic at hand is now well enough understood that we have everything under control. But often it’s not really the model as end-product that is important, it’s the focus for conversation and discussion provided by collaboratively creating a model (informed by complexity/systems thinking that’s the important step), and that’s true whether the model is visual, computational, or even statistical."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Postscript: uncomplex thinking and big data analytics",
    "text": "Postscript: uncomplex thinking and big data analytics\nAnother reason I am happy to see a new audience get excited about the complexity/systems thinking approach to model-building is because there is far too much excitement about a very linear approach to model-building these days, in the multi-headed shape of big data analytics, machine-learning, and AI.7\nIt’s not that these methods aren’t useful. Of course they are! The problem is that they too often skip the collaborative model-building, or leave the model-building to machines, so that the most important opportunity for learning is lost. That’s not quite correct. The analyst developing such models often learns a lot in the process. The problem is that the point of the exercise is often not the learning along the way, but the final model that results, and once that’s done the assumption is that now we understand, and can predict and manage the problem at hand. And of course, for as long as the world continues to be open, interconnected, and processual (i.e., forever), that kind of model will inevitably be wrong if not today, then some day very soon.8"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe qualitative social scientists I was hanging out with last week are rightly skeptical of such uncomplex models. Their enthusiasm for complexity/systems thinking models on the other hand reflects how such approaches really do give us a better chance of getting a handle on the world. I really hope their enthusiasm isn’t a one-off but a harbinger of many more fruitful conversations ahead, across what have often seemed unbridgeable divides."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. It’s paywalled, but get in touch if you’d like a copy.↩︎\nI’ll note here that models “are never true, but fortunately it is only necessary that they be useful”, p.2 in Box GP. 1979. Some problems of statistics and everyday life. Journal of the American Statistical Association 74(365) 1-4. Paywalled, I’m afraid.↩︎\nThe original paper on this is Lave R et al. 2014. Intervention: Critical physical geography. The Canadian Geographer / Le Géographe canadien 58(1) 1–10. But that is paywalled. An open access paper which gives a sense of what critical physical geography aims to accomplish is Lave R. 2015. Engaging within the Academy: A Call for Critical Physical Geography. ACME: An International Journal for Critical Geographies 13(4) 508-515.↩︎\nFigure from Monat JP and TF Gannon. 2015. Using systems thinking to analyze ISIS. American Journal of Systems Science 4(2) 36–49.↩︎\nDepending on my mood, I can go either way, but if you need convincing of the links, see Merali Y and PM Allen. 2011. Complexity and Systems Thinking, 31-52 in The Sage Handbook of Complexity and Management PM Allen, S Maguire, and B McKelvey (eds) Sage.↩︎\nYes, I’m looking at you, Overseer.↩︎\nThis is something I’ve written about before: O’Sullivan D 2018. Big data … why (oh why?) this computational social science? In Thinking Big Data in Geography: New Regimes, New Research eds. JE Thatcher, J Eckert, and A Shears, 21–38. University of Nebraska Press. Again, paywalled: let me know if you’re interested to read a copy.↩︎\nIronically, models are most urgently needed for prediction precisely when prediction is difficult or even impossible.↩︎"
  },
  {
    "objectID": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "href": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "title": "Giscience 2025",
    "section": "",
    "text": "The first Call for Submissions to GIScience 2025 has just come out (deadline for proceedings papers Jan 31 2025; for other submissions Apr 4 2025).\nWe’re excited in Aotearoa New Zealand to be hosting this one, even if I think it should be called giscience 2025 (note the non-capitalisation). I’m on the program committee so look forward (a little nervously) to a flood of submissions. It’s nice for a change that it won’t be us Aotearoans making the long trip.\nThe last big geospatial conference in NZ was Geocomputation 2019 which was greatly enjoyed by all who made it, and this one will be even better, I’m certain of it!"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "",
    "text": "One of the joys (ahem) of R spatial is moving data around between formats so you can use the best packages for particular jobs. Here’s an example using IDW interpolation in spatstat."
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Libraries",
    "text": "Libraries\nLibraries are the usual suspects plus spatstat (duh) and maptools for some extra conversions. We also need terra for the data prep.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Introduction",
    "text": "Introduction\nAs is often the case there is useful functionality in a package that doesn’t play nice with the core R-spatial packages. spatstat is really great for lots of things, but does not support sf and even needs a bit of persuading to handle sp data. Its implementation of IDW interpolation is nice however, so it’s nice to know how to use it. Whether or not you should ever use IDW is another question altogether, but we can worry about that some other time.\n\nData\nFirst we need a set of values to interpolate. I made a projected version of the R core dataset volcano which is a nice place to start.\n\nmaungawhau &lt;-  rast(\"maungawhau.tif\")\n\n\n\nSome random control points and a study area\nWe can get a dataframe of random points on the surface using terra::spatSample. We’ll make this into a sf object as a starting point because that’s the most likely situation when you want to interpolate data (you will have an sf source).\n\npts &lt;- maungawhau |&gt;\n  spatSample(500, xy = TRUE) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\")) |&gt;\n  st_set_crs(2193) |&gt;\n  st_jitter(5)\n\nWe also need a spatial extent for the interpolation, so let’s just make a convex hull of the points\n\nspatial_extent &lt;- pts |&gt;\n  st_union() |&gt;\n  st_convex_hull() |&gt;\n  st_sf()\n\nAnd just to see where we are at\n\ntm_shape(maungawhau) + \n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\")) +\n  tm_shape(pts) + \n  tm_dots() + \n  tm_shape(spatial_extent) + \n  tm_borders() + \n  tm_layout(legend.frame = FALSE)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Make the data into a spatstat point pattern",
    "text": "Make the data into a spatstat point pattern\nspatstat has its own format for point patterns, including coordinates, marks (the values) and a window or owin (the spatial extent). It’s best to make the window first and then we can make the whole thing all at once. spatstat prefers sp objects, so we go via ‘Spatial’ to get a spatstat::owin object. maptools provides the conversion to an owin.\n\nW &lt;- spatial_extent |&gt;\n  as.owin()\n\nWe also need the control point coordinates\n\nxy &lt;- pts |&gt;\n  st_coordinates() |&gt;\n  as_tibble()\n\nNow we can make a spatstat::ppp point pattern\n\npp &lt;- ppp(x = xy$X, xy$Y, marks = pts$maungawhau, window = W)\nplot(pp)\n\n\n\n\n\n\n\n\nSuccess!\nA previous notebook showed an even quicker way to do this, but where the window will be formed from a bounding box (and where’s the fun in that?)\npts |&gt;\n  as(\"Spatial\") |&gt;\n  as.ppp()"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Interpolation",
    "text": "Interpolation\nIt’s easy from here. power is the inverse power applied to distances, and eps is the resolution in units of the coordinate system.\n\nresult &lt;- idw(pp, power = 2, eps = 10)\nplot(result)\n\n\n\n\n\n\n\n\nThis can be converted back to a terra raster for comparison with the original surface.\n\n# stack the layers so we can 'facet' plot them\ncomparison_raster &lt;- rast(\n  list(maungawhau = maungawhau, \n       interpolation = rast(result) |&gt; resample(maungawhau)\n  ))\n\ntm_shape(comparison_raster) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n            col.free = FALSE,\n            col.legend = tm_legend(\n              title = \"Elevation\", \n              position = tm_pos_out(\"right\", \"center\"), \n              frame = FALSE))\n\n\n\n\n\n\n\n\nLike I said, IDW is not necessarily a great interpolation method!"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html",
    "href": "posts/2021-10-21-kde/kde.html",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#packages",
    "href": "posts/2021-10-21-kde/kde.html#packages",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#data",
    "href": "posts/2021-10-21-kde/kde.html#data",
    "title": "Kernel density estimation in R spatial",
    "section": "Data",
    "text": "Data\nThe data are some point data (Airbnb listings from here) and some polygon data (NZ census Statistical Area 2 data).\n\nLoad the data\n\npolys &lt;- st_read(\"sa2.gpkg\")\n\nReading layer `sa2' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/sa2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 78 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1735096 ymin: 5419590 xmax: 1759041 ymax: 5443768\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\npts &lt;- st_read(\"abb.gpkg\")\n\nReading layer `abb' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1254 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1742685 ymin: 5420357 xmax: 1755385 ymax: 5442630\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAnd have a look\n\ntm_shape(polys) +\n  tm_polygons() + \n  tm_shape(pts) + \n  tm_dots()"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "href": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "title": "Kernel density estimation in R spatial",
    "section": "spatstat for density estimation",
    "text": "spatstat for density estimation\nThe best way I know to do density estimation in the R ecosystem is using the spatstat library’s specialisation of base R’s density function. That means converting the point data to a spatstat planar point pattern (ppp) object, which involves a couple of steps.\n\npts.ppp &lt;- pts$geom %&gt;% \n  as.ppp()\n\nA point pattern also needs a ‘window’, which we’ll make from the polygons.\n\npts.ppp$window &lt;- polys %&gt;%\n  st_union() %&gt;%       # combine all the polygons into a single shape\n  as.owin()            # convert to spatstat owin - again maptools...\n\n\nNow the kernel density\nWe need some bounding box info to manage the density estimation resolution\n\nbb &lt;- st_bbox(polys)\ncellsize &lt;- 100\nheight &lt;- (bb$ymax - bb$ymin) / cellsize\nwidth &lt;- (bb$xmax - bb$xmin) / cellsize\n\nNow we specify the size of the raster we want with dimyx (note the order, y then x) using height and width.\nWe can convert this directly to a raster, but have to supply a CRS which we pull from the original points input dataset. At the time of writing (August 2021) you’ll get a complaint about the New Zealand Geodetic Datum 2000 because recent changes in how projections and datums are handled are still working themselves out.\n\nkde &lt;- density(pts.ppp, sigma = 500, dimyx = c(height, width)) %&gt;%\n  raster() \ncrs(kde) = st_crs(pts)$wkt  # a ppp has no CRS information so add it\n\n\n\nLet’s see what we got\nWe can map this using tmap.\n\ntm_shape(kde) +\n  tm_raster(palette =  \"Reds\")\n\n\n\n\n\n\n\n\n\n\nA fallback sanity check\nTo give us an alternative view of the data, let’s just count points in polygons\n\npolys$n &lt;- polys %&gt;%\n  st_contains(pts) %&gt;%\n  lengths()\n\nAnd map the result\n\ntm_shape(polys) +\n  tm_polygons(col = \"n\", palette = \"Reds\", title = \"Points in polygons\")\n\n\n\n\n\n\n\n\n\n\nAggregate the density surface pixels to polygons\nThis isn’t at all necessary, but is also useful to know. This is also a relatively slow operation. Note that we add together the density estimates in the pixels contained by each polygon.\n\nsummed_densities &lt;- raster::extract(kde, polys, fun = sum)\n\nAppend this to the polygons and rescale so the result is an estimate of the original count. We multiply by cellsize^2 because each cell contains an estimate of the per sq metre (in this case, but per sq distance unit in general) density, so multiplying by the area of the cells gives an estimated count.\n\npolys$estimated_count = summed_densities[, 1] * cellsize ^ 2\n\nAnd now we can make another map\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\",\n              title = \"500m KDE summed\")\n\n\n\n\n\n\n\n\nSpot the deliberate mistake?!\nSomething doesn’t seem quite right! What’s with the large numbers in the large rural area to the west of the city? Thing is, you shouldn’t really map count data like this, but should instead convert to densities. If we include that option in the tm_polygons function, then order is restored.\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\", convert2density = TRUE,\n              title = \"500m KDE estimate\")\n\n\n\n\n\n\n\n\nReally, this should be done with the earlier map of points in polygons too, so let’s show all three side by side. tmap_arrange is nice for this, although it has trouble making legend title font sizes match, unless you do some creative renaming. I’ve also multiplied the KDE result by 1,000,000 to convert the density to listings per sq. km, and we can see that the three maps are comparable.\n\nm1 &lt;- tm_shape(kde * 1000000) + \n  tm_raster(palette = \"Reds\", title = \"500m KDE\")\nm2 &lt;- tm_shape(polys) + \n  tm_fill(col = \"n\", palette = \"Blues\", convert2density = TRUE,\n              title = \"Point density\")\nm3 &lt;- tm_shape(polys) + \n  tm_fill(col = \"estimated_count\", palette = \"Greens\", convert2density = TRUE,\n              title = \"KDE summed\")\ntmap_arrange(m1, m2, m3, nrow = 1)"
  },
  {
    "objectID": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "href": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "title": "Mapping the Dulux colours",
    "section": "",
    "text": "Dulux have had a range of colours named for places in New Zealand for several years now. Clearly this is an opportunity for mapping too good to be missed. You can thank me later.\n\n\nGo to the map\n\nI gave a presentation about how this map was made using R to a Maptime! Wellington audience, and it takes you through the code in enough detail to require no further explanation from this post!"
  },
  {
    "objectID": "posts/2023-11-30-30-day-maps/30-day-maps.html",
    "href": "posts/2023-11-30-30-day-maps/30-day-maps.html",
    "title": "The 30 day map challenge",
    "section": "",
    "text": "Finding myself unaccountably with time on my hands in November 2023, I thought I’d give the 30 Day Map Challenge. I am not sure I’d recommend the experience. I mean, it’s a kind of warped fun, but it’s hard. I might have restricted my focus too much for it to stay interesting for a whole month, perhaps. Anyway, you can see what I made here and a short slideshow reflecting on the experience here.\nHere are my two favourite maps from the month:\n\n\n\n#5 A hillshaded map made by crumpling paper\n\n\n\n\n\n#27 Dots within dots to show all seven components and the overall value of an index of deprivation"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html",
    "title": "GeoCart’2024",
    "section": "",
    "text": "Another (even-number year) August coming to a close and another GeoCart’ is over.\nThe passing last year of Igor Drecki inevitably cast a shadow. But importantly, and even under that shadow, it was a fun meeting, as Igor would have wanted. Given the current woes of Wellington’s public sector, attendance was as expected down a little, but there were still enough faces old and new for the meeting to retain its usual friendly buzz, without it being too overwhelming.\nThe organisers of GeoCart’2024 can be proud of what they achieved in putting on a meeting that Igor would have enjoyed. It bore all the hallmarks of Igor’s vision for the meeting. Many keynotes (perhaps too many, but more on that below…) and many opportunities for informal discussion and exchange in the breaks between presentations, over lunch, and at the Icebreaker and Conference Dinner events. The latter in particular was really excellent—I don’t recall ever before having FOUR choices of starter, main, and dessert at a catered sit-down meal like this one: well done Dockside.1"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "title": "GeoCart’2024",
    "section": "Keynotes",
    "text": "Keynotes\nSo many keynotes…\nA two-and-a-half day meeting with six (yes SIX!) ‘all-hands’ presentations seems perhaps a little too much of a good thing. Having said that, all the keynotes were excellent. Like, really excellent. I didn’t necessarily agree with everything everyone said, but I was certainly engaged throughout. In chronological order:\nOcean Mercier kicked things off with ‘Te Taunaha Whenua: Mapping Connections to Place’ emphasising the role that maps and mapping play in defining and making place, and the more than coincidental relationship between those processes and colonisation and (yay!) decolonisation. I loved that Ocean also shared her longtime passion for orienteering— and its slightly goofy (but highly functional) maps. She was even heading off :rogaining one evening of the conference.2\n\n\n\nDaniel Huffman in their presentation ‘Sharing the Emotional Work of Mapping’ spoke to a topic that has been front of mind for me lately: the rollercoaster ride of the uncertainties of taking on projects as they appear, and balancing ambitions for your creative work against the expectations of paying clients. I have to confess to some disappointment that we didn’t get some Huffman eye candy along the way, but instead had to content ourselves with :Madison, Wisconsin’s—admittedly very nice—flag.\n\n\n\nDaniel O’Donohue. After Daniel H’s quietly reflective take on the emotional journey of making maps, next morning podcaster and self-described geospatial evangelist Daniel O gave everyone a good kick in the pants encouraged us all to get busy making millions on the interwebs. Some of this talk leaned a bit too much for cycnical old gen-X-er me into the social media influencer vibe, but more than a couple of people were taking notes, and I expect to see mass-market cartography rocket to the top of Aoteroa’s export earnings charts in the years to come. Seriously though, I did have a couple of conversations with folks prompted by this talk to explore the possibilities for getting paid more for their skills. Being underpaid is a problem in cartography/geospatial,3 so I wish them all the best, and will join them, just as soon as I come up with an idea. Also: we all got socks.4\n\n\n\nDavid Garcia somehow managed to keep the energy going in a talk entitled ‘Revisiting the Social Nature of Mapping’. As in Daniel O’s talk, the memes were flowing, but so were the serious messages echoing both Ocean’s and Daniel H’s talks from the previous day in taking seriously the emotional heft of cartography, and its roles in colonisation and decolonisation. The recently minted Dr. Garcia wears his learning lightly and with great good humour, but his work is deeply serious. For a sense of just how serious, here’s the thesis in full. I have made my own (marginal) contributions to critical GIS and critical cartography, and even managed to inject a little of it into the program at VUW but I still learned a great deal from this talk (and I also laughed a lot…).\n\n\n\nSarah Bell introduced the final day with an emotionally5 nuanced presentation pushing us to carefully consider the many ways that maps can be biased. So far so obvious,6 but this talk added a few more dimensions for the manipulation of an audience, most poignantly sound, in an animated revisiting of Snow’s cholera data. As co-author of a book whose first edition cover featured :the John Snow map, I was fully prepared to yawn my way through that part of the presentation, but this was very compelling.\n\n\n\nAnd finally, Wendy Shaw Toitū Te Whenua - Land Information New Zealand and Secretary of Ngā Pou Taunaha o Aotearoa - New Zealand Geographic Board, on the 100th anniversary of the board, updated us on progress in the ongoing consideration and restoration of Māori placenames across Aotearoa. The quick summary is that much has been done, and that much remains to be done. I scored a couple of the associated maps to add to Daniel O’s socks.\n\n\n\n\nToo much of a good thing?\nSo… I’ll say it again: all the keynotes were excellent. I can’t help wondering even so, if this balance in the meeting foregrounds the accomplishments of the already well-recognised, to the disadvantage of the great work being done by others.\nI am only really say this comparing GeoCart’s complement of keynotes relative to other conferences of similar size, where there is more likely to be one keynote a day, not two. In part the meeting follows this schedule to justify its two-and-a-half day duration so that there is space for two evening social events. There is likely not enough submitted work to make for two parallel tracks of presentations over that length of time. But I do wonder if in a small meeting like this two tracks are really needed. Cutting things back to three keynotes and a single track would make this a non-issue—and nobody would have to miss any of the talks!\nBut really, it’s not actually a problem, especially not if the keynotes can maintain this level of quality. And I know that for Igor a key mission for GeoCart was to connect cartography in Aotearoa New Zealand with the wider world, and keynotes from afar are one important way to do this. And having our visiting keynotes see the quality of the work in Aotearoa as exemplified in the ‘home team’ keynotes doesn’t hurt either.\nOne thing I do know is that finding that many keynote-worthy speakers every two years is a challenge: don’t hesitate to offer suggestions to the NZGS Committee!"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "title": "GeoCart’2024",
    "section": "Other highlights",
    "text": "Other highlights\nAs noted, with two tracks, you can’t catch ’em all, so this is inevitably a partial perspective… but among the highlights for me were:\n\nKarl Baker spoke with great good humour and impressive clarity about the often rather sorry tale of the mishandling of New Zealand’s longest placename in maps. Say it with me: Te Taumata­whaka­tangi­hanga­kōauauo­tamatea­turi­pūkākā­piki­maunga­horo­nuku­pōkai­whenua­kita­na­tahu.7 You can get some insight into the sorry tale of the use and abuse of the name from the wikipedia talk page for the place name, and there’s a concise correct pronunciation :here.\nAndy Tyrell presented a really cool workflow for automating the production of linework style oblique terrain maps like these.\nCraig Devereux’s map of the Roman Empire which won the map contest was a standout and hard to beat.\nJessie Colbert’s presentation of work with Katarzyna Sila-Nowicka and Dan Exeter visualising maps of multiple deprivation was a model of completeness and clarity. I hope to maybe get involved in this work with my ‘dots within dots’ and weaving contributions.\nSam van der Weerden gave a great great talk about Maynard Design’s work on the mapping for Auckland’s cycle network. Really interesting: particularly so was the insight that perhaps as cartographers we should sometimes resist our instinct to differentiate things by colour. After all, no matter what kind of cycle path it is, it’s still a cycle path and should be coloured green, and perhaps the less important differences among types of cycle path should be differentiated by line style not colour!\nDid I mention the dinner at Boatshed?\n\n\nSociety business\nAlso worth mentioning, the New Zealand Cartographic Society after somewhat inexplicably being refused permission to dual list its name in English and te reo Māori way back in 1989 (when doing so would have been well ahead of the curve) is finally en route, remaining bureaucratic hurdles notwithstanding, to adopting a te reo Māori name: Te Rōpū Tuhi Whenua o Aotearoa."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "title": "GeoCart’2024",
    "section": "Finally, permit me a moment of self-promotion",
    "text": "Finally, permit me a moment of self-promotion\nI enjoyed giving a talk on time-space maps of mountainous terrain. Here, as a bonus is an animated time-space map of a loop walk around Taranaki Maunga, which probably should have made it into the talk, but for one reason or another did not."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "title": "GeoCart’2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDisclaimer: I have not been recompensed in any form by Dockside.↩︎\nThat word rogaining is very weird derived from the given names of its ‘inventors’ ROd, GAil, and NEil. The names of new sports aren’t what they used to be: see also pickleball.↩︎\nThis recent episode of the MapScaping podcast is thought-provoking on the subject.↩︎\nI’m wearing mine now.↩︎\nThere’s that word again.↩︎\nMonmonier’s How To Lie With Maps anyone?↩︎\nI’ve gone with the longer not yet official form.↩︎"
  },
  {
    "objectID": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "href": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "title": "The model zoo",
    "section": "",
    "text": "I’ve been diligently keeping the so-called ‘model zoo’ associated with our book Spatial Simulation updated for compatibility with new releases of NetLogo. Each time I generally manage to clean up a few parts of the code and slowly complete the ‘Info’ tabs (otherwise known as user documentation).\nThe models are available from this repo and you can get an overview of what they’re all about at this website. Really though, you should buy the book:"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Computing Geographically argues for the importance of giscience taking geography seriously, and also for geography taking giscience more seriously. Too much GIS work is conducted as if \\((x,y)\\) coordinates were all that is needed to make data geographical; equally, far too many geographers think that those coordinates are all that GISers care about. This book aims to bridge the divide. An accompanying website includes high resolution copies of all the figures, and ‘bonus material’ (mostly code). See: computinggeographically.org.\n Spatial Simulation: Exploring Pattern and Process with George Perry is an advanced introduction to simple spatial simulation models for researchers in fields such as geography, planning, archaeology, ecology and beyond. George and I believe such models have great value across the ‘spatial sciences’ but the literature is scattered, making it a challenge for researchers to get started. We read all those mathematics, physics and statistics papers, so you won’t have to! A library of models in NetLogo accompanies the book, and can be downloaded here or explored at patternandprocess.org.\n Geographic Information Analysis with Dave Unwin, was first published in 2003, with a second edition in 2010, and translations into Mandarin and Korean. It has been a mainstay of my teaching (and that of many others!) since first publication, and offers an excellent, proven introduction to key principles and methods of spatial analysis and modelling."
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "title": "Experiments with R interpolators",
    "section": "Get input datasets",
    "text": "Get input datasets\n\nThe empirical projection\nThis file contains points on a global grid system, generated using the dggridR package. We can see the points in ‘lat-lon’ space below. Note how because this is a global grid system the points appear to ‘thin out’ towards the poles. This is an artifact of plotting the points in lat-lon, which is also explored in this post.\n\nemp_proj &lt;- read.csv(\"dgg-2432-no-offsets-p4-briesemeister.csv\")\nggplot(emp_proj) +\n  geom_point(aes(x = lon, y = lat), size = 0.05) +\n  coord_equal()\n\n\n\n\n\n\n\n\nInspection of the data shows we have two sets of coordinates lon, lat and x, y.\n\nhead(emp_proj)\n\n  ID dir     lon      lat          x       y\n1  0   .   11.25 58.28253  -428675.9 1520344\n2  1   . -168.75 58.28253 -1197290.8 7794188\n3  2   . -168.75 65.09003 -1120150.7 7234575\n4  3   . -168.75 72.07407 -1048583.9 6613223\n5  4   . -168.75 79.18998  -973499.6 5945572\n6  5   . -168.75 86.38746  -892969.4 5242144\n\n\nThis projection is Briesemeister, which is an oblique form of the Hammer-Aitoff projection. See\n\nBriesemeister W, 1953, “A New Oblique Equal-Area Projection” Geographical Review 43(2) 260\n\nIt’s possible to form this projection with a proj string, but it is not commonly supported in GIS, and who knows proj strings that well?! For the record, this is the string you are looking for:\n+proj=ob_tran +o_proj=hammer +o_lat_p=45 +o_lon_p=-10 +lon_0=0 +R=6371007\n\n\nA sample dataset\nWe also want a set of points to project, and what better than a world map. Note that we can only project points, so this is points along world coastlines, not polygons.\n\npts &lt;- read.csv(\"world_better.csv\") |&gt;\n  dplyr::select(lon, lat)\n\n# sanity check with a map\nggplot(pts) + \n  geom_point(aes(x = lon, y = lat), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "title": "Experiments with R interpolators",
    "section": "Triangles interpolator",
    "text": "Triangles interpolator\nThere are many different ways we can do this kind of interpolation. The simplest is based on triangulation. This method is available in the package interp but also in akima which is much quicker. The output x and y coordinates are formed by interpolating as shown below. x and y are the known locations of the input coordinate, which here are the longitude and latitude in out empirical projection dataset emp_proj. The desired outputs are at the longitude and latitude coordinates in the world maps dataset pts. And we do the interpolation twice, once for the x coordinate and once for the y coordinate in our target projection.\n\nx_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$x,\n                xo = pts$lon, yo = pts$lat)\ny_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$y,\n                xo = pts$lon, yo = pts$lat)\n\nNow make up a results data table and map it. akima puts the result in a column z in its output.\n\nresult &lt;- data.frame(x = x_out$z, y = y_out$z)\nggplot(result) + \n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "title": "Experiments with R interpolators",
    "section": "Apply the empirical projection’s cut region",
    "text": "Apply the empirical projection’s cut region\nWhat are those dots across the southern area of the map? These are points that happen to fall in triangles in the first coordinate system (i.e. lon-lat) where one corner of the triangle lies on a different side of a discontinuity in the projection than the other corners. We should avoid projecting points inside these triangles because they project (as we can see!) unreliably.\nFor the Briesemeister projection we know the precise location of this discontinuity, and have prepared a file delineating the ‘cut’ position. We can use this to remove points from the sample dataset that lie inside triangles that intersect the cut region.\nFirst, here is the discontinuity. Points close to or on this line could end up in very different parts of the projected output and so are ‘unsafe’ to project using our interpolation-based approximation.\n\ncut_sf &lt;- st_read(\"briesemeister-cut.geojson\")\n\nReading layer `briesemeister-cut' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-experiments-with-r-interpolators/briesemeister-cut.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -179.8892 ymin: -82.94613 xmax: -0.3442386 ymax: 44.55223\nGeodetic CRS:  WGS 84\n\nggplot(cut_sf) + \n  geom_sf()\n\n\n\n\n\n\n\n\nNow triangulate the empirical projection data points, and assemble a polygon from all those triangles that are intersected by the discontinuity.\n\n# make the cut region into a sf dataset\nemp_proj_sf &lt;- emp_proj |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326)\n\ntriangles &lt;- emp_proj_sf |&gt;\n  st_union() |&gt;\n  st_triangulate() |&gt;   # triangulation of empirical projection points\n  st_cast() |&gt;\n  st_as_sf() \n\ncut_triangles &lt;- triangles |&gt;\n  st_filter(cut_sf)\n\ncut_region_sf &lt;- cut_triangles |&gt; \n  st_filter(cut_sf) |&gt;\n  st_union() |&gt;       \n  st_as_sf() \n\nWe quite reasonably get a warning that triangulation doesn’t really apply to geographical coordinates, but… akima did the interpolation by triangulating these points and it doesn’t know it’s unsafe (because it’s not a geospatial package). It’s not actually ‘unsafe’ as such in this case, because we aren’t using the triangulation for its metric properties anyway. So… we ignore this warning and plot this to see what we are dealing with\n\nggplot(triangles) +\n  geom_sf(colour = \"grey\") + \n  geom_sf(data = cut_triangles, fill = \"grey\", colour = \"white\") +\n  geom_sf(data = cut_region_sf, fill = \"#00000000\", colour = \"black\") +\n  geom_sf(data = cut_sf, color = \"red\")\n\n\n\n\n\n\n\n\nNow we use st_disjoint to remove points in the data to project that are inside the cut region.\n\npts_to_project_sp &lt;- pts |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326) |&gt;\n  st_filter(cut_region_sf, .predicate = st_disjoint) |&gt;\n  as(\"Spatial\")\n\nThe last step converts the points to the SpatialPointsDataFrame format of the sp package, which akima can also work with:\n\n# we also need the empirical projection data in the sp format\nemp_proj_sp &lt;- emp_proj_sf |&gt;\n  as(\"Spatial\")\n\nx &lt;- akima::interpp(emp_proj_sp, z = c(\"x\"), xo = pts_to_project_sp, linear = TRUE)\ny &lt;- akima::interpp(emp_proj_sp, z = c(\"y\"), xo = pts_to_project_sp, linear = TRUE)\n\nA bit unexpectedly, akima outputs the data to a two column dataframe with the interpolated values in a column with the same name as the input data, so getting the results into a final output table is as below.\n\nresult &lt;- data.frame(x = x$x, y = y$y)\nggplot(result) +\n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()\n\n\n\n\n\n\n\n\nAnd those rogue dots are all gone!"
  },
  {
    "objectID": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "href": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "title": "Useful utilities for NetLogo",
    "section": "",
    "text": "An intermittent side-project of mine over (github informs me) more than three years has been assembling a collection of useful functions or rather reporters and procedures written in NetLogo to make it just a little bit easier to do a range of fairly standard things in that language.\nDon’t get me wrong, NetLogo is wonderful and I love it dearly. I find it tremendous fun to code in. But any time I start dealing in any serious way with lists and strings or slightly more obscure probability distributions, I find that I am repeating myself rewriting code I’ve written dozens of times before. This probably became most apparent when I was writing the spatial COVID model.\nAnyway… the result has been this extremely intermittent and informal set of netlogo source (.nls) files that you can import into a NetLogo model to get access to more useful list and string handling without having to write an extension.\nNo warranties of fitness are expressed implied. If you find them useful, let me know!"
  },
  {
    "objectID": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "href": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "title": "Locations of interest in 2021 delta outbreak",
    "section": "",
    "text": "For a time, during the August 2021 COVID outbreak that started in Auckland but eventually led to the end of New Zealand’s attempt to stop COVID completely, I continuously updated the above kepler.gl animated map of ‘locations of interest’. Kepler.gl is a nice tool which I’d recommend for making quick visualizations like this.\n\nThere was something oddly compelling about the mundane details of the reported locations of interest.\n\nGo to the viewer\n\nLike most of the country I got pretty fed up doing it in the end, so I stopped… for whatever it’s worth, looking at the timeline on the visualization the delta outbreak did get reigned in eventually, but it it was pretty clear that the mood of the country had changed and lockdowns weren’t going to stick for much longer."
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "title": "Low level handling of sf objects",
    "section": "",
    "text": "You can handle sf objects at a low level but it can take a bit of getting used to, and you have to watch out for floating point gotchas.\nknitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "title": "Low level handling of sf objects",
    "section": "Packages",
    "text": "Packages\nEverything here needs just sf and dplyr.\n\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "title": "Low level handling of sf objects",
    "section": "Making polygons",
    "text": "Making polygons\nMy main confusion dealing with polygons in sf sounds dumb, but was easily fixed. Matrices in R get populated by column, by default, where the points in a polygon are in the rows of the matrix (as they would be in a dataframe with x and y attributes). You just have to make sure to populate the matrices in the right order.\nThere’s also the slightly strange fact that you have to wrap a matrix of points in a list to make a polygon.\nSo because of the row-column thing, there’s a tendency to do\n\nmat &lt;- matrix(c(0, 0,\n                1, 0,\n                1, 1,\n                0, 1,\n                0, 0), nrow = 5, ncol = 2)\nsquare &lt;- st_polygon(list(mat))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBut that fails, because the matrix we made was\n\nmat\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    0    0\n[3,]    1    1\n[4,]    0    0\n[5,]    1    0\n\n\nand the first and last rows don’t match (even if they did, it’s not actually a polygon!).\nBut specify that the matrix should be populated byrow and all is well\n\nsquare &lt;- st_polygon(list(matrix(c(0, 0,\n                                   1, 0,\n                                   1, 1,\n                                   0, 1,\n                                   0, 0), nrow = 5, ncol = 2, byrow = TRUE)))\nplot(square)\n\n\n\n\n\n\n\n\nIf you happen to have vectors of the x and y coordinates, then it’s easier.\n\nx &lt;- c(0, 1, 1, 0, 0)\ny &lt;- c(0, 0, 1, 1, 0)\nsquare &lt;- st_polygon(list(matrix(c(x, y), nrow = 5, ncol = 2)))\nplot(square)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "title": "Low level handling of sf objects",
    "section": "Floating point coordinates and their discontents",
    "text": "Floating point coordinates and their discontents\nsf defaults to using floating point calculations which has some annoying side-effects. For example, the code below results in an error\n\nangles &lt;- 0:3 * 2 * pi / 3\nx &lt;- cos(angles)\ny &lt;- sin(angles)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 7, ncol = 2)))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBecause sf defaults to floating point it doesn’t consider the polygon closed due to precision issues that mean R considers sin(0) != sin(2 * pi):\n\nsin(0) == sin(2 * pi)\n\n[1] FALSE\n\n\nThere is no easy way to fix this except to round the coordinates!\n\nx &lt;- round(x, 6)\ny &lt;- round(y, 6)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 4, ncol = 2)))\nplot(triangle)\n\n\n\n\n\n\n\n\nThere’s not a lot you can do about this when you are constructing sf objects. Polygons must be closed, and equality is strictly applied to the opening and closing points. You can’t ask st_polygon to automatically close polygons for you.\nOnce you have polygons to work with, the problem can come back to bite you, but there is a way around it. For example, this works OK:\n\nsquare |&gt; \n  st_difference(triangle) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nBut let’s make two squares that are theoretically adjacent to one another, but happen to have non-integer coordinates (which… is pretty commonplace!)\n\nangles &lt;- seq(1, 7, 2) * 2 * pi / 8\nangles &lt;- c(angles, angles[1])\nx1 &lt;- cos(angles)\ny1 &lt;- sin(angles)\n\ns1 &lt;- st_polygon(list(matrix(c(x1, y1), nrow = 5, ncol = 2)))\nbb &lt;- st_bbox(s1)\ns2 &lt;- s1 + c(bb$xmax - bb$xmin, 0)\n\nplot(s1, xlim = c(-1, 2.1))\nplot(s2, add = TRUE)\n\n\n\n\n\n\n\n\nTwo squares, next to one another as we might hope, but if, for example, we st_union them we get a MULTIPOLYGON.\n\ns3 &lt;- st_union(s1, s2)\ns3\n\nMULTIPOLYGON (((0.7071068 0.7071068, -0.7071068 0.7071068, -0.7071068 -0.7071068, 0.7071068 -0.7071068, 0.7071068 0.7071068)), ((2.12132 0.7071068, 0.7071068 0.7071068, 0.7071068 -0.7071068, 2.12132 -0.7071068, 2.12132 0.7071068)))\n\n\nIf we plot them, they still appear separate\n\nplot(s3)\n\n\n\n\n\n\n\n\nand if we measure the distance between them, turns out they don’t touch at all, but are in fact a miniscule distance apart…\n\ns1 |&gt; st_distance(s2)\n\n             [,1]\n[1,] 3.330669e-16\n\n\nIt’s probably not necessary to point out how silly this is, even if it is strictly correct.\n\nRXKCD::getXKCD(2170)$img\n\n\n\n\n\n\n\n\n[1] \"https://imgs.xkcd.com/comics/coordinate_precision.png\""
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "title": "Low level handling of sf objects",
    "section": "Specifying precision for spatial operations",
    "text": "Specifying precision for spatial operations\nBy contrast if we use rgeos functions the equivalent union operation works as we might expect (although we do have to feed rgeos the old sp types of polygon, which we can do via a call to as(\"Spatial\")…)\n\nrgeos::gUnion(as(s1, \"Spatial\"), as(s2, \"Spatial\")) |&gt;\n  st_as_sfc() |&gt;\n  plot()\n\nError in loadNamespace(x): there is no package called 'rgeos'\n\n\nsf does allow us to effectively emulate the rgeos behaviour, albeit not for simple geometries. When we instead bundle geometries up into feature collections, we can assign them a precision, and this will take care of the kinds of problems we see above:\n\ns1_sfc &lt;- s1 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\ns2_sfc &lt;- s2 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\n\ns1_sfc |&gt;\n  st_union(s2_sfc) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThe first time I looked this up in help, I got it wrong due to careless reading, and, I think, assuming that the number you provide to st_set_precision() was a ‘tolerance’, or, in effect a ‘snap distance’. The help is also a bit roundabout, and directs you to this page, for an explanation of how it works.\nIn effect all coordinates are adjusted by applying a function like this one:\n\nadjust_precision &lt;- function(x, precision) {\n  round(x * precision) / precision\n}\nsqrt(2) |&gt; adjust_precision(1000)\n\n[1] 1.414\n\n\n\nst_snap\nAnother possible fix for the floating point issue is snapping points to the coordinates of another object before applying operations. So this works, although it is not as clean as the st_precision option. On the other hand, it does work on plain geometry objects, not only on those that have been bundled up into collections.\n\ns1 |&gt; st_snap(s2, 1e-8) |&gt;\n  st_union(s2) |&gt;\n  plot()"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "title": "Low level handling of sf objects",
    "section": "In conclusion",
    "text": "In conclusion\nThe tools for making and manipulating geometries at a low level are available in sf but they are not always as simple as you’d like. Of course, most often you are dealing with datasets and that’s where sf comes into its own. Just remember st_set_precision() and you should be able to avoid quite a few headaches…"
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "title": "In praise of GeoPackages",
    "section": "",
    "text": "In the shapefiles must die wars I’ve been smugly using GeoPackages for several years now. Here are some reasons why."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "title": "In praise of GeoPackages",
    "section": "One file: no really, it’s just one file!",
    "text": "One file: no really, it’s just one file!\nIf there is one thing above all others to love about GeoPackages it’s this. When teaching newcomers, the standout advantage over shapefiles is simple: there is only one file, and not some number between three and seven (or is it eight? I’m just not sure).\nI’ve lost count of how often I had to disable the increasingly hard to find Hide file extensions option on a baffled student’s computer to reveal the disturbing truth that there were several identical-except-for-the-extension files that together formed a so-called shapefile. Or how when supplying data for lab assignments I had to include instructions about unzipping files to a known folder and so on (a seemingly simple requirement made much more complicated by more recent releases of Windows allowing the user to look inside a .zip file without actually unpacking the contents…)."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "title": "In praise of GeoPackages",
    "section": "Understandable ATTRIBUTE_",
    "text": "Understandable ATTRIBUTE_\nWhat I meant to say was: understandable attribute_names because you can have attribute names longer than 10 characters."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "title": "In praise of GeoPackages",
    "section": "Many layers",
    "text": "Many layers\nI’ve tended to shy away from packaging multiple datasets in GeoPackages as support for this feature has at times been uncertain and confusing.\nNow that I am less beholden to not confusing beginners where ‘one file = one layer’ is a useful rule to live by, I’ve started to look more closely at what’s going on here. It still has the potential to confuse — especially in QGIS’s right-click Export → Save Features As… — but there is untapped potential here for making life easier when it comes to sharing bundles of related data with a minimum of fuss.\nI’ll explain using my go to tool for general data wrangling, R’s sf package.\n\nMulti-layer geopackages in R\nAssuming you have a locally stored simple GeoPackage nz.gpkg, you read it using:\n\nlibrary(sf)\nnz &lt;- st_read(\"nz.gpkg\")\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow, if you’d like to add another dataset to that file, you can specify a layer to put it in. Before doing that, it’s probably best to check what layers are already there using st_layers():\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAs we might expect (and if you’ll excuse the awkward formatting due to the long crs_name) a single layer with the same layer name as the file itself. Say we buffer our data and want to store it back into the same file, then we can do the below, as long as we provide a new layer name to store it in:\n\nnz |&gt; \n  st_buffer(12000) |&gt; \n  st_write(\"nz.gpkg\", layer = \"coast\")\n\nWriting layer `coast' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nAnd now we can see that both layers are present in the file:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n2      coast Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n2 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow if you open this file in R unless you specify the layer you want, you’ll just get the first one:\n\nst_read(\"nz.gpkg\")\n\nMultiple layers are present in data source /Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg, reading layer `nz'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nand one of those warning messages it’s tempting not to read, but really should.\nAnd that’s it really, for multiple vector layers in GeoPackages in R.\n\n\nMulti-layer geopackages in QGIS\nMeanwhile, if you open a two-layer GPKG in QGIS you’ll see this:\n\n\n\n\n\nThat’s pretty clear. What’s unfortunately less clear than in R is the sequence of operations that will safely add a layer to an existing GeoPackage. That’s not quite fair: what is unclear is the warning message you get if you choose an existing .gpkg file as the destination for a dataset you’d like to save. The warning message looks like this:\n\n\n\n\n\nThis seems pretty scary. Before trying this at home I suggest you make a copy of the target geopackage if you are worried about losing your data, but if you steel yourself, and against every instinct hit Replace, then as long as you set a different name in the Layer name option of the Save Vector Layer as… dialog\n\n\n\n\n\nit will be fine, and you’ll end up with an additional layer in the target GeoPackage.\nYou can also manage the component layers of GeoPackages in QGIS’s Browser panel.\n\n\nOther platforms are available\nI should note that similar to R, the Python geopandas module through its read_file() and list_layers() functions, and its GeoDataframe’s to_file() method offers the same functionality as discussed above."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "title": "In praise of GeoPackages",
    "section": "Many formats",
    "text": "Many formats\nSo if you can put many different layers in a GeoPackage, can you mix vector and raster datasets in there too?\nTurns out you can, although, at least for R’s sf this is where things get a bit messy. terra is the package for dealing with raster data, so let’s load that and read in a raster dataset. Before doing that, I’ll clean up nz.gpkg so it only has one layer again:\n\nnz |&gt; st_write(\"nz.gpkg\", layer = \"vector\", delete_dsn = TRUE)\n\nDeleting source `nz.gpkg' using driver `GPKG'\nWriting layer `vector' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nNow load the raster layer\n\nlibrary(terra)\nnz_r &lt;- rast(\"nz.tif\")\nnz_r\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.tif \nname        : layer \nmin value   :     1 \nmax value   :     1 \n\n\nCrowbarring this thing into our GeoPackage is certainly possible, but it’s far from intuitive, and involves invoking some GDAL options.\n\nnz_r |&gt; \n  writeRaster(\"nz.gpkg\", \n              gdal = c(\"RASTER_TABLE=raster\", \"APPEND_SUBDATASET=YES\"))\n\nThe GDAL options are all documented, but applying them using terra::writeRaster is finicky, and clearly this is not for the faint-hearted!\nFurthermore… sf can’t ‘see’ the raster layer:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1     vector Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nI guess if you don’t ‘do’ raster layers then there is no point in being able to see them either ¯\\_(ツ)_/¯. terra is similarly see-no-evil about things and just reads in the raster layer that is in the file without commenting on other layers that might be present:\n\nrast(\"nz.gpkg\")\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.gpkg \nname        : Height \n\n\nQGIS actually does better here. It certainly sees both layers:\n\n\n\n\n\nIt’s worth noting that the QGIS Browser panel makes mixing raster and vector layers into your GeoPackages straightforward.\n\nOverall, I’ve been aware that I can bundle raster and vector layers in GeoPackages like this but haven’t used the capability. In part because I’ve only just figured out how to do it using the R tools(!), but mostly because I prefer to keep raster data in GeoTIFFs and vector data in GeoPackages so I can tell which is which at a glance."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "title": "In praise of GeoPackages",
    "section": "So, are GeoPackages perfect?",
    "text": "So, are GeoPackages perfect?\nOf course not. There’s an argument to be made that every format that perpetuates the simple features paradigm is a bad as every other. I even wrote a book that is — kind of — all about this. It’s one of the mysteries of the evolution of geospatial that topology was embedded in the ‘standard’ formats, until it wasn’t. For what it’s worth, I think we have relational databases to blame for that.\nGeoPackages don’t get us out of floating point geometry hell either.\nThere are also better formats for particular applications. GeoJSON is web-native in a way that GeoPackages never will be, and newer formats such as FlatGeoBuf and GeoParquet, and more recent approaches like Discrete Global Grids certainly have their place.\nBut in a world still dominated by relational DBMS, a geospatial format that is basically a wrapper around SQLite tables was almost certain to emerge eventually, and GeoPackages are that format. They’re vastly preferable to shapefiles, and it’s good to see them slowly (more quickly would be better) replacing them.\nThe shapefile is (almost) dead, long live the GeoPackage!"
  },
  {
    "objectID": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "href": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "title": "OK COVID, you win",
    "section": "",
    "text": "From some time in March 2020 for two years I downloaded the latest reported COVID data for New Zealand, added them to my spreadsheet of the various numbers, and updated a timeline I was keeping in R. The download process got a lot easier when I was introduced to the data downloader at University of Auckland eResearch.\nAs you can see, by the time I stopped things had gone pretty badly off the rails, and even a log scale wasn’t helping much.\nI can honestly say this was when I started to become competent with the ggplot2 package, and for that, as well as the reassurance the daily ritual provided for about a year and a half (we were doing so well…), I am grateful.\n\nAddendum\nI also mapped the progress of the vaccination program for a (much shorter) time. At least these numbers were released weekly and in a much more accessible form. Here’s how the critical ‘second dose’ went:"
  },
  {
    "objectID": "posts/2020-05-26-covid-model/spatial-covid-model.html",
    "href": "posts/2020-05-26-covid-model/spatial-covid-model.html",
    "title": "A spatial COVID model",
    "section": "",
    "text": "During those ahem… heady days of the first COVID-19 lockdown in Aotearoa (7 or 8 weeks in April-May 2020) I worked intensely with colleagues on a simple spatial simulation model of the pandemic in New Zealand.\nInitially we hoped that we might get involved in the ongoing modelling efforts which the government was using to manage the national response given how self-evidently geographical a thing an epidemic is. But that clearly wasn’t a perspective shared by the public health professionals and others advising the government. Either that or they were just too damn busy dealing with events to welcome the additional distraction of thinking about the geography of the disease.\nOr maybe they thought it was just too much:\n\nWhatever the reason, we never did get involved in the national response, although it was clear by the time of the outbreak in later 2021 that they were, at least by then, thinking geographically about how to manage things. Anyway, you can play with the model by clicking below.\n\nGo to the model\n\nIt’s quite slow to load, and if you aren’t smart about the settings you use it’s likely to rapidly spin out of control (just like a uh… pandemic) and slow things down dramatically. For that reason I advise only using the go-one-day or go-one-week buttons until you get a feel for things.\nWe published a paper explaining the inner workings here:\n\nO’Sullivan D, M Gahegan, DJ Exeter, and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967-1000. doi: 10.1111/tgis.12660\n\nand I presented the work at the hard-to-believe-it-even-happened face-to-face-in-person 2020 meeting of the New Zealand Geographical Society in November that year.\nAll the materials are available at this repo."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "",
    "text": "I’ve just spent much of a not especially nice Saturday (weather-wise) tidying up some loose ends on the Computing Geographically website. In particular, I’ve been updating the R code snippets that make many of the figures. I started out intending to update the tmap v3 code to v4, since the latter is now recommended by the developer. It has, for example, been adopted in the latest edition of the excellent Geocomputation with R.\nA little way into the process, I realised that since doing last year’s 30 Day Map Challenge (see my efforts here) I use ggplot2 a lot more for my everyday mapping work than I do tmap. That’s in spite of having taught tmap for several years, a choice I made because its learning curve is less steep than ggplot2’s. Anyway, long story short, migrating the code on my book website from tmap v3 to v4 turned into more of a mixed bag: migrating some code to tmap v4, and some to ggplot2.\nIn this post I discuss some things to consider if you are choosing which of these two excellent packages—that’s important: they are both excellent packages—to use in various situations."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Same only different",
    "text": "Same only different\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tmaptools)\nlibrary(tmap)\nlibrary(htmlwidgets)\nlibrary(cols4all)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(dplyr)\n\n\nWhat the packages have in common is that they implement the idea of a grammar of graphics (hence ggplot2’s name) where we progressively add layers to a graphic, specifying for each layer how data attributes are scaled to give values of :visual variables (colour, line width, line style, symbol size and so on). There’s a graphic (in German) showing this idea here.\nSo the packages are similar at a high-level. However, I’m not going to delve into details of the differences between these packages. For that, you will should explore the extensive online resources available. The best places to start are hard to pick. Maybe here for tmap and here for geospatial stuff in ggplot2. Just be aware that tmap is undergoing a major upheaval as it transitions to v4 and while older code should still work, you’ll see a lot of warnings. Meanwhile, ggplot2 has a habit of changing things without preserving backwards compatibility, so it’s advisable to be wary of any code snippets more than 3 or 4 years old when you are looking for help.\nRather than the details, I want to explore the packages ‘in use’ by looking at four broad aspects of mapping that speak to the advantages of a package specifically designed for mapping (tmap) over a more general purpose visualization tool (ggplot2), which nevertheless holds it own. Those four things are choropleth maps, raster data, web maps, and ‘map junk’ (north arrows and the like).\n\nChoropleth maps: tmap’s killer app\nMaking ‘proper’ choropleth maps in ggplot2 is no fun at all. See my Day 13 experience in the 2023 30 Day Map Challenge. The difficulty is that the central tenet of classified choropleth mapping is controlling how you relate data to colour fills. It’s not that ggplot2 won’t allow you fine control over this aspect of your choropleth map, it’s just that it really, really wants you to map your data linearly, and continuously to a colour ramp. That’s a reasonable design decision for a general scientific visualization tool. It’s just not what cartographers do in choropleth mapping.\nTo illustrate, here’s an old dataset I often use: TB cases in Auckland City in 2006 by Census Area Unit:\n\nak &lt;- st_read(\"ak-tb.gpkg\")\n\nAnd here’s the most basic ggplot2 choropleth map of the TB_RATE variable (which is cases per 100,000 population):\n\nggplot() +\n1  geom_sf(data = ak, aes(fill = TB_RATE))\n\n\n1\n\naes(fill = ...) specifies which variable maps to the colour fill\n\n\n\n\n\n\n\n\n\n\n\nA few things stand out here (to my eye at least):\n\nyou get a graticule in latitude-longitude even if the data are in a projected coordinate system (to be clear, the map is in the projected coordinates);\nthe default colour ramp goes from dark for low values to light for high values; and\nthe default colour ramp is black-to-blue, which might be preferable to the default muddy browns we’ve become accustomed to, but is an unusual choice for a map.\n\nOf course we can fix all these things:\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_continuous_c4a_seq(\n2    palette = \"brewer.reds\") +\n3  coord_sf(datum = st_crs(ak))\n\n\n1\n\nscale_fill_continuous_c4a_seq() is in the cols4all package which includes a very wide range of palettes\n\n2\n\nUse Brewer palette ‘Reds’\n\n3\n\nGet the coordinate system of the data using st_crs() and apply to the coordinate frame using coord_sf()\n\n\n\n\n\n\n\n\n\n\n\nNote that I am using a cols4all scale, because this is the preferred colour palettes package for tmap, and provides a wide array of options. For casual mapping, we likely don’t care about the grid, and we can get rid of that too using theme_void():\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"brewer.reds\") +\n  theme_void()\n\n\n\n\n\n\n\n\nSomething like the above, which is three lines of code after the introductory ggplot() call, is what I use most of the time. And the map is good enough, especially if it’s temporary and a stepping stone on the way to some other analysis.\nWhat does the same map look like in tmap v4?\n\ntm_shape(ak) +\n  tm_polygons(\n1    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(\n2      values = \"brewer.reds\")) +\n  tm_layout(\n    frame = FALSE, \n3    legend.frame = FALSE)\n\n\n1\n\nVariable name for fill in quotes\n\n2\n\nBrewer ‘Reds’ again\n\n3\n\nRemove annoying frames around map and legend\n\n\n\n\n\n\n\n\n\n\n\nEssentially the same map, up to minor aesthetic details. These are easily tweaked in either package, so we won’t worry about them too much here.\nWhere tmap wins out is if you want to experiment with classic choropleth map classification schemes, for example:\n\nm1 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\")) +\n1  tm_layout(title = \"Pretty\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\nm2 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n2      values = \"brewer.reds\", style = \"equal\", n = 6)) +\n  tm_layout(\n3    title = \"Equal intervals\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE) \n\nm3 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n4      values = \"brewer.reds\", style = \"quantile\", n = 6)) +\n  tm_layout(\n    title = \"Quantiles\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE)\n\nm4 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n5      values = \"brewer.reds\", style = \"sd\")) +\n  tm_layout(title = \"Std. Dev.\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\ntmap_arrange(m1, m2, m3, m4, ncol = 2)\n\n\n1\n\nThe default classification style is \"pretty\", i.e. human-friendly round numbers\n\n2\n\nstyle = \"equal\" to get equal intervals\n\n3\n\nadd a title to show the classification style\n\n4\n\nstyle = \"quantile\" for quantiles\n\n5\n\nstyle = \"sd\" for standard deviation breaks\n\n\n\n\n\n\n\n\n\n\n\nForget the minor issue with aligning these maps exactly, the point here is that these are all classified choropleth maps, with the classification method specified by the style parameter (more options are available than shown here). This is an established way to make choropleth colours easier to parse, or put another way, to make it easier to highlight the specific features of the data you want readers to focus on.\nThe magic here is that behind the scenes tmap is using the classInt package, and if we want to make similar maps in ggplot2 we have to do that work ourselves:\n\nlibrary(classInt)\n\nclass_breaks &lt;- ak$TB_RATE |&gt;\n  classIntervals(6, \"quantile\", digits = 1)\nbrks &lt;- round(class_breaks$brks, 1)\n\nggplot() + \n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_binned_c4a_seq(\n    palette = \"brewer.reds\", breaks = brks) + \n  theme_void()\n\n\n1\n\nNote the change to a binned scale, controlled by the breaks\n\n\n\n\n\n\n\n\n\n\n\nThe above is the minimal ggplot2 version of a quantile map I can come up with. If you want the nice class labels then you have to make those labels yourself and do more work on the legend. You can see an example of this in my Day 13 map in the 2023 30 Day Map Challenge.\nOverall, if you are making a lot of classified choropleth maps then tmap is your friend.\n\n\nRaster data: tmap’s other killer app\ntmap is comfortable dealing with raster data. Here’s a simple example:\n\ndem &lt;- rast(\"dem.tif\")\n\ntm_shape(dem) + \n1  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_intervals(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n1\n\ntm_raster() ‘announces’ that these data are raster so that some visual variables might be interpreted differently, e.g. col is now what fill is on a polygon layer, where col is interpreted as linework or outline colour\n\n\n\n\n\n\n\n\n\n\n\nNice! Note that the same tm_scale_intervals() function is used here to specify colours in this case as it was to specify fill colours in the choropleth map case. We can also have a continuous colour ramp, and layer on top a hillshade. First make the hillshade using some terra functions:\n\nslope &lt;- dem |&gt; terrain(\"slope\", unit = \"radians\")\naspect &lt;- dem |&gt; terrain(\"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, \n                   angle = 35, direction = 135)\n\nThen just add it as another layer also using the col aesthetic with a semi-transparent grey palette.\n\ntm_shape(dem) + \n  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_shape(hillshade) +\n  tm_raster(\n1    col = \"hillshade\",\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col_alpha = 0.35\n  ) +\n  tm_layout(\n    legend.frame = FALSE, legend.show = FALSE)\n\n\n1\n\ntmap is OK about adding a second layer using the col visual variable\n\n\n\n\n\n\n\n\n\n\n\nNicer! If I have one issue here it’s with using the parameter name values for the palette name, which takes a little bit of getting used to. Of course, the rationale for this is that the palette specifies where the colour visual variable is to get its values which in this context, are colours.\nggplot2 doesn’t really deal in rasters. We can make similar maps pretty easily, by converting to an x, y, attribute dataframe:\n\ndem_df &lt;- dem |&gt; as.data.frame(xy = TRUE)\nggplot(dem_df) +\n1  geom_raster(aes(x = x, y = y, fill = dem)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"hcl.terrain2\", name = \"Elevation\") +\n2  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(\n3    panel.border = element_rect(fill = NA, linewidth = 1))\n\n\n1\n\nNote that for ggplot2 the colours are still a fill here\n\n2\n\nexpand = FALSE prevents a margin around the data\n\n3\n\nggplot2’s theming system has a lot going on…\n\n\n\n\n\n\n\n\n\n\n\nYou can probably tell I’ve done a fair bit of this kind of thing: I didn’t just magic that expand = FALSE option and the panel.border thing out of nowhere.\nAnyway, while this approach is fine for a small raster like this one, even here the x-y dataframe equivalent to the raster has 35,000 or so rows, so this won’t scale well.\nAnd we haven’t even got into the headaches involved if you want to layer something else on top of a raster dataset where you need to use the fill aesthetic again, like say… a hillshade. Let’s just say the ggnewscale package is your friend. I leave that as an exercise for the reader…\nAgain… I think it’s clear that tmap is a better choice if you are making a lot of maps of raster data.\n\n\nWeb maps: tmap’s oth… OK, this is just getting silly\nWhen it comes to web maps ggplot2 doesn’t even try. tmap on the other hand has its ‘mode switch’ option. Set the mode to view and you are making web maps!\n\n1tmap_mode(\"view\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\", \n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"), \n    fill_alpha = 0.5)\n\n\n1\n\nNow you’re making web maps! Use tmap_mode(\"plot\") to switch back\n\n\n\n\n\n\n\n\n\n \nSo, yeah, if you are making bona fide web maps, tmap every time. Switch back to plot mode, and tmap also does a good job of allowing you to use a web map as a basemap layer:\n\ntmap_mode(\"plot\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"),\n    fill_alpha = 0.5) +\n1  tm_basemap(server = \"OpenStreetMap\") +\n  tm_layout(frame = FALSE, legend.frame = FALSE)\n\n\n1\n\nAdds a static web map as a basemap\n\n\n\n\n\n\n\n\n\n\n\nggplot2 can get the same effect using the ggspatial::annotation_maptile() function (you will also need to have the prettymapr package installed).\n\nggplot() + \n1  annotation_map_tile(zoomin = 1) +\n  geom_sf(data = ak, aes(fill = TB_RATE), alpha = 0.5) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  theme_void()\n\n\n1\n\nannotation_map_tile() is a function from ggspatial\n\n\n\n\n\n\n\n\n\n\n\nOverall though, another win for tmap.\n\n\nMap junk: whatever\nAs you might guess from the title of this section, I am not much concerned with north arrows and scalebars and such-like. tmap has them built in.\n\ntm_shape(ak) + \n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(values = \"brewer.reds\")) +\n  tm_compass() +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n\n\n\n\n\n\nTo get the same things in ggplot2 you need the ggspatial package, and they’re perfectly serviceable. I am about as excited about this as I sound.\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n1  annotation_north_arrow(location = \"br\") +\n2  annotation_scale(location = \"bl\") +\n  theme_void() + \n  theme(panel.border = element_rect(fill = NA))\n\n\n1\n\nAnother ggspatial function\n\n2\n\nAnd another\n\n\n\n\n\n\n\n\n\n\n\nNeedless to say, both tmap and ggplot2 offer lots of flexibility for adding titles and subtitles and positioning all these elements around the map wherever you want them.\nAll in all, this one is probably a tie, assuming you’ve installed ggspatial."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Final thoughts",
    "text": "Final thoughts\nIn brief then: tmap is better at:\n\nclassified choropleth maps;\ndealing with geospatial rasters; and\nweb maps,\n\nand about equal on\n\nweb map derived basemaps; and\nmap junk;\n\nThe latter two assuming that you have installed ggspatial. The funny thing is, I still find myself using ggplot2 more! For me the counterpoints to the above are:\n\nunclassified choropleths are often fine for a quick look-see at your data;\nI don’t make many raster-based maps, and ggplot2::geom_tile is often good enough for my purposes;\nWhere I previously used web maps in tmap to get an overview of my data, now I tend to use QGIS. If I really need a web map, I will certainly use tmap; and\nI am not a fan of map junk (hence: junk). Don’t get me wrong, I’ll but a north arrow and scalebar on a map if necessary, I just don’t make many maps like that.\n\nIf your mix of use-cases is different, especially if choropleth maps, raster data, and web maps loom larger in your world than they do in mine, then you might wind up making a different choice.\nI think the reason I end up gravitating to ggplot2 is that it is the entry point into a much wider visualization world. The idiom it has popularised, which tmap has reworked for mapping, of layering a series of aesthetics each linking a data variable to a visual variable (that so-called grammar of graphics) is more cleanly implemented in ggplot2, as you’d expect it to be. And the exact same semantics apply to scatterplots, boxplots, violin plots, histograms, bar charts, and so on. The ggplot2 ecosystem is sprawling and extremely powerful, and making maps is just one small corner of it.\nBecause of that, I am always making charts using ggplot(data) + geom_*() and it feels very natural to make maps the same way.\nHaving said that, for R coded maps that cover all the mapping bases, you should probably also get to grips with tmap. I use both regularly, even if I do use ggplot2 a little more regularly!"
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html",
    "href": "posts/2024-10-17-duckdb/duckdb.html",
    "title": "The joy of DuckDB",
    "section": "",
    "text": "Just recently as part of this project I finally got around to putting some properly big data into an actual database and OMG! For the kind of situation I was in, I can’t recommend giving this a try enough."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "href": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "title": "The joy of DuckDB",
    "section": "Some background",
    "text": "Some background\nThe project in question as one component involves developing or at least exploring building :species distribution models for a large number of the bird species present in Aotearoa New Zealand. To that end we’ve obtained the latest eBird data collected for the New Zealand Bird Atlas. This is a phenomenal resource which includes the accumulated observations of thousands of citizen science volunteers, accumulated over several years.\nThe raw .txt file containing the observational data is a chunky 2.8GB with 7 million rows of data. Seven million rows isn’t so bad, right? Right, it really isn’t that bad. There is even an R package (of course there is), cheekily called auk1, for massaging the raw data down to the data you actually want.\nIn my case relevant data pertain only to the NZ Bird Atlas effort, and to complete checklists as only these provide the absence data required for occupancy modelling. The complexity of the data makes running auk to filter the raw data down slow, but it’s a one-time-only operation, so that’s OK, and now I have a 3.7 million row table, and we’re in business, no need to worry about setting anything up.\nThe problem comes when I have to blow those 3.7 million rows back up again for occupancy modelling to 110 million rows."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "href": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "title": "The joy of DuckDB",
    "section": "Wait, what? 110 million rows?!",
    "text": "Wait, what? 110 million rows?!\nYes, 110 million rows.\nHow it works is that each species (a little over 300) requires an entry in each complete checklist (around 360,000 of these) recording whether that species was observed (present) or not (absent) in that checklist. That results in 110 million row table. You can keep the two tables separate but then you have to keep joining them every time you go to use them. And if you save the 110 million table to disk it takes up around 8GB, and also takes a noticeable length of time to open and close for analysis in R.\nIt was about this time that I thought I should consider my options."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "href": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "title": "The joy of DuckDB",
    "section": "Enter DuckDB",
    "text": "Enter DuckDB\nDuckDB is an easy to install columnar database that you can drive using :SQL. It’s particularly easy to install because the R and Python APIs come bundled with the database itself. So, if you don’t want to, you don’t even have to install it, your platform of choice will do that for you.\nAnyway, if you do install it, which I did, so I could poke it around a little before going further, then to start it up from the command line type\n% duckdb\nAnd if you want to create a new database in the folder you are running from then it’s\n% duckdb my-new-database.db\nThe file extension is optional. Once in the session you can stash an existing CSV file in the database as a table with the command (D is the DuckDB command line prompt):\nD CREATE TABLE letters AS FROM 'letters.csv';\nand to see the results of your handiwork:\nD SELECT * FROM letters;\n┌─────────┬───────┬─────────┐\n│ column0 │  id   │ letter  │\n│  int64  │ int64 │ varchar │\n├─────────┼───────┼─────────┤\n│       1 │     1 │ a       │\n│       2 │     2 │ b       │\n│       3 │     3 │ c       │\n│       4 │     4 │ d       │\n│       5 │     5 │ e       │\n│       6 │     6 │ f       │\n│       7 │     7 │ g       │\n│       8 │     8 │ h       │\n│       9 │     9 │ i       │\n│      10 │    10 │ j       │\n├─────────┴───────┴─────────┤\n│ 10 rows         3 columns │\n└───────────────────────────┘\nD \nSatisfied it was this easy, I typed .exit to shut DuckDB down and moved on to consider how to use DuckDB from R. I should mention at this point that I’ve bounced off PostgreSQL a couple of times in the past when considering using it in classroom situations because it’s just not as easy to get into as this."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "href": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "title": "The joy of DuckDB",
    "section": "DuckDB in R",
    "text": "DuckDB in R\nThe R package you need is duckdb, so\n\ninstall.packages(\"duckdb\")\nlibrary(duckdb)\n\nand you are ready to go (no other installation of DuckDB required).\nNow if you have a giant dataframe called say my_giant_df, that you need to deal with, open a connection to a new database (or an existing one if you’ve been here before) with\n\ncon &lt;- dbConnect(duckdb(), \"my-giant-dataframe.db\")\n\nand write your dataframe into it as a table called giant_df with\n\ndbWriteTable(con, \"giant_df\", my_giant_df)\n\nIf that’s all you plan on doing then you should shut down the connection\n\ndbDisconnect(con, shutdown = TRUE)\n\nWhen I did this I was agreeably surprised to find that my 8GB file had shrunk down to a mere 750MB.\nBut there’s more. The reason I went down this route at all is that I generally only want to work with the data for one bird species at a time — data which come in handy packets of only 360,000 rows or so. Here’s how that works in practice. First open a connection to the database\n\ncon &lt;- dbConnect(duckdb(), dbdir = str_glue(\"the-birds.db\"))\n\nThe database has a table called observations containing the aforementioned 110 million rows. Each row includes among other things the common_name of a bird. We can get a vector containing those using\n\ncommon_names &lt;- dbGetQuery(con, \"SELECT DISTINCT common_name FROM observations\") |&gt;\n  pull(common_name) |&gt;\n  sort()\n\nNow we can iterate over each species by doing\n\nfor (common_name in common_names) {\n  sql_name &lt;- str_replace_all(common_name, \"'\", \"''\")\n  query &lt;- str_glue(str_glue(\"SELECT * FROM observations WHERE common_name = '{sql_name}'\"))\n  this_bird_df &lt;- dbGetQuery(con, query)\n  # ...\n  # do stuff with this_bird_df\n  # ...\n}\n\nThe only wrinkles here, for those paying attention, are using str_glue from the stringr package to form the SQL query I need, and related to that a str_replace_all to double up any single-quotes ' that happen to appear in those common names to '' so that they can be passed into an SQL query.\nIn general you query the database using dbGetQuery(&lt;connection&gt;, &lt;SQL&gt;) and you can execute a command with dbExecute(con, &lt;command&gt;).\nOf course, you have to know a bit of SQL, but for this kind of simple (local) data warehousing, there’s nothing you are likely to need that a quick google DuckDuckGo search won’t unearth.\nThis approach enabled me to iterate over all 300 species in the data and assemble a ‘mini-atlas’ of ggplot maps of each bird’s range in under a minute (snippet below), which is about how long it was previously taking R just to open the 8GB CSV file. Not to mention that the giant data table is never in working memory, only the chunks I need one at a time.\n\n\n\nYes, of course, Mallard is there for a reason\n\n\nIt’s safe to say, I’ll be using DuckDB a lot in many projects to come. There seem to be some wrinkles in relation to handling spatial data, specifically from R’s sf package but there’s a package for that,2 and it’s nothing to get too alarmed about."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "href": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "title": "The joy of DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA very nerdy deep cut from the Cornell Ornithology Lab. Respect.↩︎\n‘We have a package for that’ should be R’s tagline.↩︎"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "title": "Affine transformations of sf objects",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "title": "Affine transformations of sf objects",
    "section": "Packages",
    "text": "Packages\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(wk)\n\nsf::sf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "title": "Affine transformations of sf objects",
    "section": "A simple square",
    "text": "A simple square\nJust to get things set up let’s make a simple square.\n\nsquare &lt;- (st_polygon(list(matrix(c(-1, -1, 1, -1, 1, 1, -1, 1, -1, -1), \n                                 5, 2, byrow = TRUE))) * 0.5 + c(1, 0)) |&gt;\n  st_sfc()\n\ntm_shape(square) + \n  tm_borders(col = \"red\") + \n  tm_grid()"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "title": "Affine transformations of sf objects",
    "section": "Simple transformations",
    "text": "Simple transformations\nIn the code above, we made a polygon and multipled it by 0.5, then added c(1,0) to it. This had the effect of scaling it by 0.5 andthen translating it by the vector \\[\\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\]\nThese unlikely looking operations are perfectly valid, although they feel a bit ‘off’.\nEven more unlikely is that you can multiply an sf object by a matrix…\n\nang &lt;- pi / 6\nmat &lt;- matrix(c(cos(ang), -sin(ang), \n                sin(ang),  cos(ang)), 2, 2, byrow = TRUE)\n(square * mat) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThis is very handy… but probably also a bad idea! Because you have to post-multiply by the matrix, the sense of many affine transformations is reversed and construction of the matrix is not ‘by the book’. Usually the affine transformation matrix \\(\\mathbf{A}\\) for an anti-clockwise rotation by angle \\(\\theta\\) around the origin, would be\n\\[\n\\mathbf{A} =\n\\left[\\begin{array}{cc}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{array}\\right]\n\\]\nHere, because we are post-multiplying the rotation will be in the other direction… and to rotate anti-clockwise, you use the \\(-\\mathbf{A}=\\mathbf{A}^T\\)\n\\[\n-\\mathbf{A} =\n\\left[\\begin{array}{cc}\n-\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & -\\cos\\theta\n\\end{array}\\right] =\n\\left[\\begin{array}{cc}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{array}\\right] = \\mathbf{A}^\\mathrm{T}\n\\]\nThis means that if you are doing any serious affine transforming of sf shapes at a low-level in R spatial, I recommend either writing some wrapper functions that generate and apply the necessary matrices on the fly, or, probably better yet, using the wk package which has proper support for affine transformations."
  },
  {
    "objectID": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "href": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "title": "What three chords",
    "section": "",
    "text": "I don’t know if this needs or deserves any further explanation, but SERIOUSLY and I can’t emphasise this enough, turn down the volume on your device before clicking the button.\n\nWhere am I?"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "title": "Uniform random points on the globe",
    "section": "",
    "text": "There isn’t as much land near the poles, so how do you make uniform randomly distributed points in lat-lng coordinate space. Here’s how!\nNeeded libraries are the usual suspects plus rnaturalearth for basemap data\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\nn &lt;- 2500\nThe key thing to realise here is that random uniform numbers in both latitude and longitude will not be evenly distributed on Earth’s surface, because the meridians converge toward the poles. We can make two datasets to show this. First a naive set of randomly located points:\npts_naive &lt;- data.frame(lon = runif(n) * 360 - 180,\n                        lat = runif(n) * 180 - 90,\n                        type = \"naive\")"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "title": "Uniform random points on the globe",
    "section": "Here comes the science…",
    "text": "Here comes the science…\nAnd now a set where inserting a cosine correction ensures that the distribution of latitudes is appropriately more dense close to the equator:\n\npts_even &lt;- data.frame(lon = runif(n) * 360 - 180,\n                       lat = acos(runif(n) * 2 - 1) * 180 / pi - 90,\n                       type = \"even\")\n\n\nCompare the latitude distributions\nWe can make up a combined data table and directly compare the distribution of the latitudes with a nice density plot. The increased representation of points in the mid-latitudes with the cosine correction is clear.\n\npts &lt;- bind_rows(pts_naive, pts_even)\nggplot(pts) +\n  geom_density(aes(y = lat, fill = type), alpha = 0.5, lwd = 0) +\n  scale_fill_viridis_d()"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "title": "Uniform random points on the globe",
    "section": "Make a map",
    "text": "Make a map\nUse an equal-area projection to clearly see the problem geographically.\n\nw &lt;- ne_countries(returnclass = \"sf\") |&gt;\n  st_transform(\"+proj=hammer\")\n\npts_sf &lt;- pts |&gt;\n  st_as_sf(coords = 1:2, crs = 4326)\n\nggplot(w) + \n  geom_sf(fill = \"#cccccc\", colour = \"white\", lwd = 0.35) +\n  geom_sf(data = pts_sf, aes(colour = type), alpha = 0.35) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe naively distributed points are clearly denser at the poles than they should be, where the cosine term in the ‘even’ points generation method makes them evenly distributed over the globe."
  },
  {
    "objectID": "training/00-spatial-data-science.html",
    "href": "training/00-spatial-data-science.html",
    "title": "Introducing spatial data science",
    "section": "",
    "text": "View the slides\n\nThese materials aim to help GIS users (even those with limited experience) transition to a more data science-centric approach. There is a particular emphasis on open source tools and on doing geospatial analysis in code using R as both are becoming increasingly important across the industry.\nThe materials are organised by theme and short courses to tailored to any or a selection of the themes can easily be arranged.\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "training/01-spatial-analysis.html",
    "href": "training/01-spatial-analysis.html",
    "title": "Spatial analysis and modelling",
    "section": "",
    "text": "Go to the materials\n\nThese materials outline a one semester (36 contact hours) class in spatial analysis and modelling that was last taught at Victoria University of Wellington as GISC 422 in the second half of 2023. The materials cover many of the topics introduced in my book Geographic Information Analysis a recognised classic text in the field.\nI am still in the process of cleaning the materials up for conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to spatial analysis in R but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "my-skills.html",
    "href": "my-skills.html",
    "title": "My skills",
    "section": "",
    "text": "For more information or advice on any of these contact me.\n\nWriting\nI have written 3 books and numerous articles, reviews, and book chapters.\n\n\nPresenting\nI have taught for over 20 years and can put together a mean slide show (I like to think…). These days, I use web-compatible formats, primarily revealjs or quarto. See my list of talks, or even the slides from a class for examples.\n\n\nAnalytics\nI work all the time in R and Python developing workflows to tidy, reformat, analyse, and visualize data, especially spatial data.\n\n\nVisualization\nI am a trained cartographer. I haven’t done a lot of publication cartography per se, but have regularly worked on visual analytics or maps of complex data in research projects.\n\n\nCoding\nI have been programming for as long a I can remember. The more or less full list of languages, with those I have taught in bold would be: (Apple) BASIC, PASCAL, Fortran, 6502 assembler, C/C++ (a little), Java (Masters and PhD work), NetLogo, Visual Basic (yeuch!), Python, R, and JavaScript (also HTML and CSS if you can call those programming languages).\nThese days, I use python and R all the time.\nI am not a software developer, but can write well organised and documented code.\nI love NetLogo, but freely admit that it is somewhat niche (surprisingly powerful though, see this geographical COVID model I built in lockdown).\nI have other languages in me: Julia and Rust seem particularly interesting, but I may wait for more mature geospatial stacks to emerge before tackling them.\n\n\nSimulation modelling\nSee above re NetLogo. I’ve also written a book about this.\n\n\nWeb\nMost of my web stuff is linked from these pages, so just have a look around. The most complicated web-thing I’ve built is probably this New Zealand 2018 commute visualization. I am not a web developer, but am happy to roll up my sleeves and figure stuff out if called on!\n\n\nProject management\nBefore academia I was a project engineer leading build, test, commissioning and maintenance of production line laser-scanning inspection equipment, working with major multinationals such as IBM, Kodak, Dow Chemical, and Sony. In that role I learned a great deal about project and people management."
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "I offer training in any of the areas listed below. Contact me for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing spatial data science\n\n\nA suite of short courses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial analysis and modelling\n\n\nGeographic information analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeographical computing\n\n\nPython programming for geospatial\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/02-moving-the-middle.html",
    "href": "portfolio/02-moving-the-middle.html",
    "title": "Agent modeling of land management",
    "section": "",
    "text": "This is part of a large MBIE funded project ‘Moving the middle’.\nThe part of the research I am most involved in is developing an agent-based model of how land managers (mostly farmers) make changes in landuse and farm management practices. Other parts of the wider project are examining the motivations and drivers of farmer behaviour at an individual level, whereas the model explores how individual actions might scale up across landscapes.\nSlides from a presentation I gave about the model can be viewed here.\nAnd here is a more recent screenshot of the model in action."
  },
  {
    "objectID": "portfolio/00-weaving-and-tiling-maps.html",
    "href": "portfolio/00-weaving-and-tiling-maps.html",
    "title": "Tiled and woven maps",
    "section": "",
    "text": "This is an ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThe idea is to map many different variables by combining them by ‘weaving’ or ‘tiling’. Here’s an example\n\nDifferent strands in the woven pattern represent different socioeconomic (or other) variables in the mapped area, and are coloured in the usual ‘choroplethic’ manner. We’re still figuring out the details, but some of the resulting maps are quite striking. Here’s a tiled (not woven) one\n\nExplanations of exactly what the heck is going on here are at the project repo, where you’ll also find links to presentations, example notebooks, and the code to make maps like these yourself!"
  },
  {
    "objectID": "portfolio/04-antarctica.html",
    "href": "portfolio/04-antarctica.html",
    "title": "Science impacts in Antarctica",
    "section": "",
    "text": "This work aims to understand the potential for damaging environmental impacts in Antarctica. You can follow the work as it develops at this website.\nThe approach I’m taking attempts to estimate ‘desire lines’ for human movement. Here’s a typical output:\n\nThis work is funded through Manaaki Whenua Landcare Research and Te Pūnaha Matatini."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Most talks are in web formats, and may include links that are now broken. Others are PDFs. Others are missing, but are listed because they give a sense of development over time.\n\n\n\nDate\nTitle\nNotes\n\n\n\n\n2024August\nTime-space mapping in mountainous terrain\nGeoCart 2024, National Library, Wellington, 21-23 August\n\n\n2024April\nA spatially explicit agent-based model of on-farm environmental interventions\nAnnual Meeting of the American Association of Geographers, Honolulu, Hawai’i, United States\n\n\n2024March\n30 Day Map Challenge 2023\nMaptime! Wellington\n\n\n2023September\nFrom geographical information science via spatial data science to geographical computing\nFourth Spatial Data Science Symposium, University of Canterbury Hub, 7 September\n\n\n2023August\nComputing Geographically: Bridging Giscience and Geography\nUniversity of Canterbury, School of Earth and Environment, Research Seminar, 10 August\n\n\n2022November\nTiled & woven thematic maps\nRegional GIS Forum, Palmerston North, 11 November\n\n\n2022August\nComputing geographically: rethinking space and place in giscience\nKeynote at New Zealand Geospatial Research Colloquium, 29-30 August\n\n\n2022August\nTiled & woven thematic maps\nGeoCart 2022, National Library, Wellington, 24-26 August\n\n\n2022July\nR for geospatial\nSpatial Literacy User Group, Te Herenga Waka - Victoria University of Wellington\n\n\n2021November\nWeaving maps of multivariate data\nState of New Zealand Cartography, Special Meeting of the New Zealand Cartographic Society, Wellington\n\n\n2021September\nMapping the Dulux Colours of New Zealand Using R\nMaptime! Aotearoa, online\n\n\n2020November\nSpatially-explicit models for exploring COVID-19 lockdown strategies\nNew Zealand Geographical Society, Wellington\n\n\n2020November\nComputing geographically: rethinking giscience as geography\nUniversity of Utah, Department of Geography, Geography Awareness Week Colloquium\n\n\n2019September\nA spatial simulation model to explore the potential impact of gene drives as a control on invasive wasps\nGeocomputation 2019, Queenstown, 18-21 September\n\n\n2019August\nTheoretical geography: definitely harder than physics!\nVvoIP_Physics_Debates symposium\n\n\n2019July\nAvoiding the YAAWN syndrome\nAgents for Theory: From Cases to General Principles, Theory Development through Agent-based Modeling, International Workshop held at Herrenhäuser Palace, Hanover, Germany\n\n\n2018May\nSome translation required, or: A city is not a network either!\nInaugural Brian Coffey lecture and workshop in Geographical Information Science University of Washington, Tacoma\n\n\n2018April\nComputing with many spaces: Generalizing projections for the digital geohumanities and GIScience\n(with Luke Bergmann who presented) 114th Annual Meeting of the American Association of Geographers, New Orleans, LA\n\n\n2018March\nReimagining GIScience for relational spaces\nUniversity of Colorado Boulder, Department of Geography Colloquium Series\n\n\n2017December\nBridging GIScience and Geographical Thought\nGeographic Data Science Lab, University of Liverpool\n\n\n2017September\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nGraduate webinar series on Agent-based models, University of Minnesota. These are the same slides as a talk at Stanford a couple of years earlier…\n\n\n2017September\nSome translation required, or: A city is not a network either!\nInternational Symposium on The Future of Urban Network Research, University of Ghent, Belgium\n\n\n2017April\n‘Same only different’: rethinking the practice of digital urban geographies\n113th Meeting of the American Association of Geographers, Boston, MA\n\n\n2016November\nSimple spatial models: Building blocks for a process-based GIS?\nGeolunch Series, Geospatial Innovation Facility (GIF), University of California, Berkeley\n\n\n2016September\nSearching for common ground (again)\n(with Jim Thatcher and Luke Bergmann) Presented at 9th International Conference on Geographical Information Science (GIScience 2016), Montreal, Canada\n\n\n2016September\nSimple simulation models as a complexity ‘pattern language’\nLightning talk at Rethinking the ABCs: Agent-Based Models and Complexity Science in the age of Big Data, CyberGIS, and Sensor Networks pre-conference workshop at GIScience 2016, Montreal, Canada\n\n\n2016March\nSpatiality, maps, mathematics and critical human geography\nUniversity of Uppsala, Department of Social and Economic Geography\n\n\n2016January\nFuture GIS\n(with Matt Wilson) University of British Columbia, Department of Geography\n\n\n2015December\nThinking with and about models in geography\nUniversity of California, Davis. Geography Graduate Group seminar series.\n\n\n2015August\n(with Alex Singleton and Seth Spielman) Our town: How socioeconomics shape functional neighborhoods in American cities\nGeocomputation 2015, UT Dallas\n\n\n2015May\nSpatial simulation: Exploring pattern and process\nUNIGIS Salzburg Webinar\n\n\n2015May\nSimple spatial models: Building blocks for process-based GIS?\nStanford University\n\n\n2015May\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nStanford University Libraries’ Center for Interdisciplinary Digital Research\n\n\n2015April\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\n(with George Perry) presented at the 110th Annual Meeting of the Association of American Geographers, Chicago, IL\n\n\n2014November\n‘Play well’: Learning about the world using spatial models\nUniversity of Oregon, Department of Geography and Complexity Science conference\n\n\n2014September\nUsing Personal Names to Explore Cultural, Ethnic and Linguistic Structure in Populations\nUC Berkeley, Department of Demography\n\n\n2014June\nSimple spatial models: building blocks for process-based GIS?\nInstitute of Australian Geographers – New Zealand Geographical Society Joint Conference, University of Melbourne\n\n\n2013August\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at SIRC NZ 2013, University of Otago, Dunedin, New Zealand. Extended abstract available here\n\n\n2013April\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at the 109th Annual Meeting of the Association of American Geographers, Los Angeles, CA\n\n\n2013February\nTowards a ‘pattern language’ for spatial simulation models\nDepartment of Geography, University of California Santa Barbara\n\n\n2012May\nNaming networks and population structure\nDepartment of Urban Engineering, University of Tokyo\n\n\n2012May\nSpatial Simulation: Exploring Pattern and Process – A Work in progress\nCentre for Spatial Information Science, University of Tokyo\n\n\n2012February\nAgent-based models: what are they good for? Or: did Schelling really need an ABM?\nPresented at the 108th Annual Meeting of the Association of American Geographers, New York, NY\n\n\n2011September\nModel Histories: The Generative Properties of Agent-Based Modelling\nPresented by James Millington (also with George Perry) at Annual Conference of the Royal Geographical Society with the Institute of British Geographers, Royal Geographical Society, London\n\n\n2011April\nNaming networks and population structure\n(with Pablo Mateos) presented at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA\n\n\n2011April\nDo physicists have geography envy?\nwith Steve Manson (who presented) at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA. This paper was eventually published in much different form in the Annals of the American Association of Geographers.\n\n\n2009December\nSimulating long distance dispersal processes in spatially heterogeneous landscapes\n(with George Perry) presented at Geocomputation 2009, University of New South Wales, Sydney, Australia. A version of this work was published in Ecological Informatics."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "title": "Random points on the globe revisited",
    "section": "Patterns in latitude-longitude space",
    "text": "Patterns in latitude-longitude space\n\nA simple random pattern\nWe can make this by drawing x and y coordinates from uniform distributions. The \\(y\\) coordinate is transformed using the inverse cosine operation discussed in my original post.\n1plot_pp &lt;- function(df) {\n  ggplot() +\n    geom_point(data = df, aes(x = x, y = y)) +\n    coord_equal(xlim = 180 * c (-1, 1),\n2                ylim = 90 * c(-1, 1), expand = FALSE) +\n3    theme(panel.background = element_rect(fill = NA))\n}\npattern1 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = runif(n_points) * 180 - 90,\n                   generator = \"Uniform random\")\nplot_pp(pattern1)\n\n\n1\n\nConvenient to have a single function for plotting point patterns.\n\n2\n\nexpand = FALSE stops ggplot adding a margin around the unit square.\n\n3\n\nIt’s good in these plots to see the unit square!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simple uniform random pattern\n\n\n\n\n\nSimple random pattern with a cosine correction for latitude\nNext up the same pattern with the cosine correction from my earlier post.\n\n\n\n\npattern2 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = acos(runif(n_points, -1, 1)) / pi * 180 - 90,\n                   generator = \"Uniform random cosine-corrected\")\nplot_pp(pattern2)\n\n\n\n\n\n\n\n\n\n\nFigure 3: Uniform random pattern with cosine correction\n\n\n\nIn later patterns I have applied this cosine correction where the underlying process generates uniformly distributed y coordinates.\n\n\nA pattern from spatial point process\nMy first thought here was to use a sequential spatial inhibition process. This is a spatial point process where points are generated at random locations, but rejected if they are closer than some inhibition distance to an existing point already in the pattern. This is easily done in Euclidean space using the spatstat::rSSI function. In latitude-longitude space this won’t work because distances are not calculable using the Pythagorean function on coordinates.\nInstead, I have opted for the uniform point process (just as above), but specifying a tiling (a tessellation) of the space with approximately equal-area tiles. Using the formula for the y coordinate of a cylindrical equal-area projection \\(y=sin\\phi\\), we can generate approximately equal-area rectangles in latitude-longitude space as below. I emphasise the approximation here, because it is only approximate. Rectangles in lat-lon space are not rectangles on the globe after all.\n1nx &lt;- sqrt(n_points / 2) * 2 + 1\nny &lt;- sqrt(n_points / 2) + 1\n2xg &lt;- seq(-1, 1, length.out = nx) * 180\n3yg &lt;- asin(seq(-1, 1, length.out = ny)) / pi * 180\n\npp &lt;- runifpoint(n = 1, win = tess(xgrid = xg, ygrid = yg))\npattern3 &lt;- pp |&gt;\n  as.data.frame() |&gt;\n  mutate(generator = \"Stratified point process\")\nplot_pp(pattern3) +\n  geom_point(data = expand_grid(xg, yg), aes(x = xg, y = yg), \n             colour = \"red\", shape = 3, size = 0.5)\n\n\n1\n\nThe number of gridlines we need in each direction is determined here.\n\n2\n\nx coordinates are trivially equally spaced.\n\n3\n\ny coordinates approximately equally spaced in the transformed space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Stratified random point process\n\n\n\nI’ve plotted the coordinates of the grid used to ‘thin’ the points at high latitudes for reference. The process generates only one point in each of these cells. However… the thinning is not continuous and there are a number of points very close to the poles, where of course the available area is zero!\n\n\nQuasi-random sequences\nThese are the processes that were new to me, which showed up when I started looking for R packages that could do even sampling in 2D spaces. The first package to show up was spacefillr. This implements a number of such sequences, but we’ll look here at the Halton sequence, because its workings are relatively easy to understand, and the implementation in the spbal package provides more options.\n:Halton sequences are generated using a pair of coprime numbers, i.e., two numbers with no common factors. The simplest example is 2 and 3. Each of the selected numbers specifies a sequence by repeated subdivision of the interval 0 to 1. So for 2 we get \\[\n\\frac{1}{2},\\frac{1}{4},\\frac{3}{4},\\frac{1}{8},\\frac{5}{8},\\frac{3}{8},\\frac{7}{8},\\ldots\n\\] and for 3 we get \\[\n\\frac{1}{3},\\frac{2}{3},\\frac{1}{9},\\frac{4}{9},\\frac{7}{9},\\frac{2}{9},\\frac{5}{9},\\ldots\n\\] Pairing these sequences gives us coordinates of points in the unit square. Different generating numbers can be chosen (provided they are coprime) and different starting points in each sequence can be paired, to give a wide variety of deterministically generated quasi-random patterns. The spbal package provides a highly configurable interface to generate such sequences using the cppRSHalton_br function. We can see the procedures inner workings clearly by examining the first few elements in the sequence:\n\ncppRSHalton_br(10, bases = 2:3, seeds = 0)$pts\n\n        [,1]       [,2]\n [1,] 0.0000 0.00000000\n [2,] 0.5000 0.33333333\n [3,] 0.2500 0.66666667\n [4,] 0.7500 0.11111111\n [5,] 0.1250 0.44444444\n [6,] 0.6250 0.77777778\n [7,] 0.3750 0.22222222\n [8,] 0.8750 0.55555556\n [9,] 0.0625 0.88888889\n[10,] 0.5625 0.03703704\n\n\nThese appear entirely regular, and some regularity is evident in the patterns generated (see Figure 5), although it is less apparent than might be anticipated on considering the numerical values alone. The main interest in Halton sequences is their desirable evenness of distribution for sampling purposes, which is apparent in Figure 5.\npattern4 &lt;- cppRSHalton_br(n_points, \n1                           bases = c(2, 3),\n2                           seeds = c(14, 21))$pts |&gt;\n  as.data.frame() |&gt;\n  rename(x = V1, y = V2) |&gt; \n  mutate(x = x * 360 - 180, \n         y = acos(y * 2 - 1) / pi * 180 - 90,\n         generator = \"Halton\")\nplot_pp(pattern4)\n\n\n1\n\nThe coprime generating values.\n\n2\n\nThe starting positions in the sequence for each generating value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Points generated by a Halton sequence\n\n\n\nDetails concerning generation of Halton sequences and their statistical properties are provided by Faure and Lemieux.2\n\n\nA ‘home-grown’ parameter-free pattern generator\nHere I use a seemingly trivial (but not very efficient!) algorithm to generate some home-made evenly distributed points, without the need to specify any (spatial) parameter like the inhibition distance required by rSSI. I found the inspiration for this in this detailed blog post about generating :blue noise, which was in turn based on an algorithm described by Mitchell.3\nMatters are complicated by having to calculate distances in latitude-longitude space, which we do using the :Haversine formula \\[\nd=2 r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\varphi_2 - \\varphi_1}{2}\\right) + \\cos(\\varphi_1) \\cos(\\varphi_2)\\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\n\\] for the distance between two lon-lat locations\\(\\left(\\lambda_1,\\varphi_1\\right)\\) and \\(\\left(\\lambda_2,\\varphi_2\\right)\\), although because we only need the relative distances we don’t use the \\(2r\\) scaling. The need to calculate toroidal distances described in the blogpost is obviated by calculating distances on the sphere which wraps in a similar way.\nThe simple idea of this algorithm is that each time we add a new point we generate a set of points (candidates) to choose from, and select the one with the largest minimum distance to an existing point in the pattern. Making the algorithm more efficient would involve only checking the distance to points known to be close to candidate points using some kind of spatial index or binning structure.\nAt a lower implementation level there’s likely a quicker way to do the distance calculations between every candidate point and every point already in the pattern than a nested loop in R (a construct generally best avoided…).\n\n1ll_distances &lt;- function(p1, p2) {\n  distances &lt;- matrix(0, nr = nrow(p1), nc = nrow(p2))\n  for (i in 1:nrow(p1)) {\n    for (j in 1:nrow(p2)) {\n      lon1 &lt;- p1[i, 1] \n      lat1 &lt;- p1[i, 2]\n      lon2 &lt;- p2[j, 1] \n      lat2 &lt;- p2[j, 2]\n      distances[i, j] &lt;- asin(sqrt(\n        (sin((lat2 - lat1) / 2)) ^ 2 +\n        cos(lat1) * cos(lat2) * sin((lon2 - lon1) / 2) ^ 2\n      ))\n    }\n  }\n  distances\n}\n\nrescale &lt;- function(x, x1min, x2min, x1max, x2max) {\n  x2min + (x - x1min) / (x1max - x1min) * (x2max - x2min)\n}\n\nspaced_points &lt;- function(n = 50, choice_scaling = 1.5,\n                          input_bb = c(0, 0, 1, 1),\n                          output_bb = c(0, 0, 1, 1), dist_fn) {\n  points &lt;- c(runif(1, input_bb[1], input_bb[3]), \n              runif(1, input_bb[2], input_bb[4])) |&gt; \n    matrix(ncol = 2)\n  for (i in 1:(n-1)) {\n2    n_candidates &lt;- ceiling(log(i * exp(1) * choice_scaling))\n    candidates &lt;- c(runif(n_candidates, input_bb[1], input_bb[3]), \n                    runif(n_candidates, input_bb[2], input_bb[4])) |&gt; \n      matrix(ncol = 2)\n    r_max &lt;- dist_fn(candidates, points) |&gt; \n3      apply(1, min) |&gt;\n      which.max()\n    points &lt;- rbind(points, candidates[r_max, ])\n  }\n  points |&gt; \n    as.data.frame() |&gt;\n    rename(x = V1, y = V2) |&gt;\n    mutate(x = rescale(x, input_bb[1], output_bb[1], input_bb[3], output_bb[3]),\n           y = rescale(y, input_bb[2], output_bb[2], input_bb[4], output_bb[4]),\n           generator = \"Blue noise\")\n}\n\n\n1\n\nRelative lat-lon distances determined using :Haversine formula.\n\n2\n\nThe choice_scaling parameter determines how rapidly the number of candidate points grows with the size of the existing data set. It should be strictly greater than 1 or no points will ever get added! Including a log factor stops the speed of the algorithm from falling too rapidly as points are added.\n\n3\n\nThe apply(1, min) operation finds the smallest distance in each row (i.e. distance to nearest neighbour in the existing set of points), and which.max() identifies the row with the largest minimum distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: A pattern generated using a ‘blue noise’ algorithm by iteratively choosing the most remote random additional point among a set of choices"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "title": "Random points on the globe revisited",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt is intriguing to me that blue noise is seemingly not a standard spatial point process. Unlike sequential spatial inhibition to which it is similar it requires no spatial parameter to be tuned to get a desired result, and it cannot fail to produce the requested number of points. It is also a reasonable plausible process from a ‘mechanism’ perspective. Imagine for example retailers considering premises in which to set up shop and examining a number of different sites, then choosing the one farthest from any potential competitor. It’s at least as compelling in that respect as a model of the outcome of competition between event locations as SSI.\nSince it seems useful, I provided some ‘hooks’ in the implementation above to allow use of different distance functions and ‘windows’ for point generation. Here’s an example in a simple Euclidean space, using :Manhattan distance to determine the best new point among candidates.\n\n\n\n\nabs_diffs &lt;- function(v1, v2) {\n  outer(v1, v2, \"-\") |&gt; abs()\n}\n\nmanhattan_distances &lt;- function(p1, p2) {\n  dx &lt;- abs_diffs(p1[, 1], p2[, 1])\n  dy &lt;- abs_diffs(p1[, 2], p2[, 2])\n  dx + dy\n}\n\nspaced_points(n_points, dist_fn = manhattan_distances) |&gt;\n  ggplot() +\n    geom_point(aes(x = x, y = y)) +\n    coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 9: A blue noise pattern in Euclidean space\n\n\n\nAn interesting rabbit hole indeed!"
  }
]