[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Note: some posts show code that has been superseded. Code ran as written at last modified dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Map Challenge 2025\n\n\nDay 26 Transport\n\n\n\n30 day maps\n\n\ncartography\n\n\nR\n\n\ntutorial\n\n\n\nI finally got around to ‘doing’ Wellington’s City to Sea Walkway. It provides some interesting data processing and mapping challenges. Debatable if it’s ‘transport’, but my journey, my rules.\n\n\n\n\n\nNov 26, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Map Challenge 2025\n\n\nDay 18 Out of this world\n\n\n\n30 day maps\n\n\norigami\n\n\ncartography\n\n\n\nI noted in a recent comment on someone else’s map back on day 3 how I’ve always loved Tanaka contours, and that I’d share how to make them in R. I this post I crowbar that promise into a map using Mars digital terrain model data.\n\n\n\n\n\nNov 18, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial autocorrelation: what’s the problem?\n\n\nIt depends what you mean by problem\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\ngeographic information analysis\n\n\n\nAn exploration of the impact of spatial autocorrelation on the results produced by different sampling strategies.\n\n\n\n\n\nNov 14, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Map Challenge 2025\n\n\nDay 9 Analogue\n\n\n\n30 day maps\n\n\norigami\n\n\ncartography\n\n\n\nIt’s hard to avoid digital completely, so I don’t know if this counts, but the final result is hand folded (more like pinched) so I’m going to say that it does. \n\n\n\n\n\nNov 9, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Map Challenge 2025\n\n\nDay 2 Lines\n\n\n\ncartography\n\n\nR\n\n\ntutorial\n\n\n30 day maps\n\n\n\nHere’s one I prepared earlier, or at any rate something I happened to be doing just a few days ago which fits nicely with the ‘lines’ theme. \n\n\n\n\n\nNov 2, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 4(B): are we there yet?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n30 day maps\n\n\n\nEighth in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nOct 31, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nXKCD 3122 ‘Interrupted spheres’\n\n\nNow with added New Zealand\n\n\n\nxkcd\n\n\ncartography\n\n\nR\n\n\ntutorial\n\n\nstuff\n\n\ngeospatial\n\n\n\nSoon after I posted my definitive ranking of xkcd’s bad map projections, another was added to the pile. I’ve reverse-engineered it in R, so you don’t have to. \n\n\n\n\n\nOct 28, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 4(A): got field data?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nSeventh in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nOct 24, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 3(C): pycno what was that you said?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nSixth in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nOct 14, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 3(B): still got areas?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nFifth in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nOct 5, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic urban networks\n\n\nMessing around with networks in R (and NetLogo, but mostly R)\n\n\n\nnetworks\n\n\nmodels\n\n\nR\n\n\nnetlogo\n\n\ntutorial\n\n\n\nAn interesting new paper out this week from Marc Barthelemy and Geoff Boeing got me hacking \n\n\n\n\n\nOct 4, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 3(A): got areas?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nFourth in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nSep 17, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 2: got lines?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nThird in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nSep 12, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGiscience 2025\n\n\nReflections on the meeting\n\n\n\ngeospatial\n\n\ngiscience\n\n\naotearoa\n\n\nconferences\n\n\n\nSome promising signs of a more reflective giscience emerging, but little doubt that AI/ML and movement analytics are the biggest games in town right now\n\n\n\n\n\nSep 10, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGIS, a transformational approach\n\n\nPart 1: got points?\n\n\n\ngeographic information analysis\n\n\ntransformational approach\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\n\nSecond in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nSep 9, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nRaster really is faster\n\n\n(Also vaster) but vector just seems more corrector\n\n\n\ngeographic information analysis\n\n\nfractals\n\n\nR\n\n\ntutorial\n\n\n\nFirst in a series of posts supporting Geographic Information Analysis \n\n\n\n\n\nAug 31, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nStreet based transit isochrones using city2graph and osmnx\n\n\nSliding doors\n\n\n\npython\n\n\ntutorial\n\n\n\nMy first published paper was on public transport isochrones. How might it have gone with GTFS, osmnx, and city2graph?\n\n\n\n\n\nAug 20, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMean temperature dials\n\n\nWeather data for all twelve months in a single map view\n\n\n\npython\n\n\ntutorial\n\n\ntiling\n\n\nvisualization\n\n\nmaps\n\n\n\nWe’ve been writing up the tiled map work from the last couple of years and along the way came up with a neat way to use the approach to show annual weather data in an interesting way.\n\n\n\n\n\nAug 15, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nDulux colours of Aotearoa New Zealand mapped\n\n\nThe ultimate categorical choropleth\n\n\n\nstuff\n\n\npython\n\n\ntutorial\n\n\n\nThere’s an earlier version of this map in R. We’re repainting a couple of rooms just now and were looking at colour charts, so I thought I’d revisit the map, this time using Python. \n\n\n\n\n\nAug 3, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nCounting points in polygons\n\n\nThis shouldn’t be so hard\n\n\n\ngeospatial\n\n\nR\n\n\npython\n\n\ntutorial\n\n\n\nFor unknown reasons neither R’s sf nor Python’s geopandas makes it especially obvious how to count points in polygons. Here’s what you need to know. \n\n\n\n\n\nJul 30, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA geometric origami classic\n\n\nFolding space\n\n\n\norigami\n\n\nlife\n\n\n\nNot geospatial stuff, but most certainly spatial stuff. Thomas Hull’s classic ‘Five intersecting tetrahedra’, which I’ve always wanted to make, but never quite got around to until now. \n\n\n\n\n\nJul 10, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA simple atlas of projections\n\n\nBlog post interactivity using marimo\n\n\n\npython\n\n\ntutorial\n\n\nmarimo\n\n\nprojections\n\n\n\nThe marimo quarto extension lets you make markdown posts with embedded reactive python code which can be written in any old order you like (just like observable notebooks). \n\n\n\n\n\nJul 7, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nWhat three letters\n\n\nWord games all the way to the ends of the earth\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nnetworks\n\n\nggplot\n\n\nlife\n\n\n\nMaps of all the possible air routes where the origin and destination airport IATA codes would differ by only one letter, and of the ones that actually do. Also, grobs. \n\n\n\n\n\nJun 27, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA population based binary space partition\n\n\nOf course there’s a point, I’m just not sure what\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\naotearoa\n\n\npopulation\n\n\ncartography\n\n\n\nA process for building a hierarchical partition of a gridded population dataset\n\n\n\n\n\nJun 20, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nHow to cut a cake into four equal slices\n\n\nIf the cake is a country made of people\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\naotearoa\n\n\n\nA procedure, including R code, for quadrisecting the population of New Zealand with two lines that intersect at 90°\n\n\n\n\n\nJun 12, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nNorth-south or east-west islands?\n\n\nAnd why both are wrong\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nstuff\n\n\naotearoa\n\n\n\nAn exploration of just how north is Te Ika-a-Maui and how south is Te Waipounamu.\n\n\n\n\n\nJun 1, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLook ma! (Almost) no javascript!\n\n\nI gave a talk at the local python meetup\n\n\n\ngeospatial\n\n\npython\n\n\nmaps\n\n\ntiling\n\n\ncartography\n\n\nweaving\n\n\nvisualization\n\n\n\nVideo of a talk about marimo that I gave to the local python meetup, and related links.\n\n\n\n\n\nMay 22, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nNine XKCD bad map projections: ranked!\n\n\nAlso: reverse-engineered\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nstuff\n\n\ncartography\n\n\nxkcd\n\n\n\nAn attempt to rank the nine examples of so-called ‘bad map projections’ published by the XKCD web comic over the last few years. Not all the projections are actually bad, and I managed to reproduce some in R.\n\n\n\n\n\nMay 16, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMostly tiles, but also glyphs\n\n\nAnd tiles as glyphs\n\n\n\npython\n\n\ncartography\n\n\nvisualization\n\n\ntiling\n\n\naotearoa\n\n\n\nAn exploration of some tile-based mapping options dealing with three variables, and a consideration of the relationship between tiles and glyphs in maps.\n\n\n\n\n\nMay 2, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nImagine setting up a Jupyter hub\n\n\nIt’s easy if you try\n\n\n\npython\n\n\ntraining\n\n\naotearoa\n\n\n\nTurns out it is actually really easy to setup a Jupyter hub! So easy, even I couldn’t mess it up, when I actually read the instructions.\n\n\n\n\n\nApr 30, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMapWeaver: tiled and woven multivariate maps without code\n\n\nHow can I make this better? And: is this anything?!\n\n\n\nmaps\n\n\ntmap\n\n\ntiling\n\n\nweaving\n\n\npython\n\n\n\nEarly experiences using Marimo to make a web app from the weavingspace code.\n\n\n\n\n\nMar 9, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMarimo notebooks\n\n\nReactive notebooks in python!\n\n\n\npython\n\n\ntutorial\n\n\n\nMarimo is a reactive notebook (like Observable) but for Python. It’s a really nice idea and I can imagine using it quite a lot. \n\n\n\n\n\nFeb 18, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nInto the (LiDAR) void\n\n\nOr: who’s afraid of the Tararuas?\n\n\n\naotearoa\n\n\nmaps\n\n\ntmap\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nlife\n\n\n\nReflections on tramping in the (very) gnarly Tararuas, with R code for making elevation transects.\n\n\n\n\n\nJan 30, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nRandom points on the globe revisited\n\n\nOf Christmas gifts and rabbit holes\n\n\n\ngeospatial\n\n\nR\n\n\ntutorial\n\n\nlife\n\n\n\nRevisiting the generation of evenly-space point patterns on the globe, sparked by a rather lovely board game.\n\n\n\n\n\nJan 14, 2025\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nA welcome (re)emergence of systems thinking\n\n\nAnd it’s not before time!\n\n\n\ncomplexity\n\n\naotearoa\n\n\nconferences\n\n\nsystems\n\n\n\nSome (positive) thoughts on a surprise encounter with systems thinking at a project away day.\n\n\n\n\n\nDec 1, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nALGIM 2024\n\n\nOn geospatial in local government\n\n\n\nlocal government\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\n\nI don’t think I ever imagined attending a local government information management conference, but I did, and here are some thoughts on the experience.\n\n\n\n\n\nNov 22, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\ntmap vs. ggplot2 for mapping\n\n\nI think I prefer…\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\ntmap\n\n\nggplot\n\n\n\nFor me at least the choice between ggplot2 and tmap is an ongoing question. Here are my latest thoughts on the subject (with code).\n\n\n\n\n\nNov 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of DuckDB\n\n\nInsert duck-related joke here\n\n\n\ngeospatial\n\n\nduckdb\n\n\nR\n\n\ntutorial\n\n\n\nDuckDB: for once a technology people are excited about that isn’t a disappointment!\n\n\n\n\n\nOct 25, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nIn praise of GeoPackages\n\n\nShapefiles… who needs ’em?\n\n\n\ngeospatial\n\n\nR\n\n\nqgis\n\n\ntutorial\n\n\n\nOne of the mysteries of the geospatial universe is the persistence of the shapefile. My thoughts, with example R code, on why we should all be using GeoPackages instead.\n\n\n\n\n\nOct 16, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGiscience 2025\n\n\nYou know you want to…\n\n\n\nconferences\n\n\ngeospatial\n\n\naotearoa\n\n\n\nCall for papers.\n\n\n\n\n\nOct 10, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nGeoCart’2024\n\n\nReflections on the meeting\n\n\n\ncartography\n\n\ngeospatial\n\n\naotearoa\n\n\nconferences\n\n\ntime-space\n\n\n\nGeoCart never disappoints, and this year was no exception. Even so the conference could probably do with a few minor adjustments to its format.\n\n\n\n\n\nAug 30, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nUseful utilities for NetLogo\n\n\nFor when you are missing R/python\n\n\n\nnetlogo\n\n\nsimulation\n\n\n\nI’ve made some useful utility functions for NetLogo that make it easier to manages lists and generate random numbers with different distributions.\n\n\n\n\n\nAug 15, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nThe model zoo\n\n\nKeeping up with NetLogo\n\n\n\nnetlogo\n\n\nbooks\n\n\nsimulation\n\n\n\nNotice of an update to the models associated with Spatial Simulation.\n\n\n\n\n\nAug 2, 2024\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments with R interpolators\n\n\nDoing projection using a triangulation interpolator\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nIt’s possible to generate arbitrary transformation of geographical space using the kinds of interpolators used by engineers for finite element analysis. Here’s how.\n\n\n\n\n\nOct 21, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nSpiral origami\n\n\nThings to make and do\n\n\n\njavascript\n\n\norigami\n\n\nstuff\n\n\n\nI made a weird javascript thing that produces crease patterns you can use to fold Tomoko Fuse’s spiral origami.\n\n\n\n\n\nAug 3, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nOK COVID, you win\n\n\nThe end of the line\n\n\n\nvisualization\n\n\ncovid\n\n\nR\n\n\nmaps\n\n\n\nThe sorry tale of how I (and the country) gave up on managing COVID.\n\n\n\n\n\nMar 9, 2022\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLow level handling of sf objects\n\n\nGoddamnit, floating point!\n\n\n\nR\n\n\ngeospatial\n\n\n\nProbably you won’t ever find yourself making simple features geometries from scratch, but if you do, here are some things you should know.\n\n\n\n\n\nDec 8, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nLocations of interest in 2021 delta outbreak\n\n\nWhen everything went pear-shaped\n\n\n\nmaps\n\n\nvisualization\n\n\ncovid\n\n\n\nThe delta outbreak of COVID in August 2021 was the beginning of the end for New Zealand’s pandemic response. Here’s an animated map timeline of the cases.\n\n\n\n\n\nOct 30, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nInverse distance weighted (IDW) interpolation using spatstat\n\n\nSo many packages so little time\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nYou can do inverse-distance weighted interpolation in the spatstat package, but it’s a little bit fiddly. This post has code examples to show you how.\n\n\n\n\n\nOct 22, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nKernel density estimation in R spatial\n\n\nHere’s one way to do kernel density estimation in R spatial\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nThe spatstat package is incredibly powerful but has no interest in dealing with your GIS data. Here’s how to use it to do kernel density estimation on ‘real’ data.\n\n\n\n\n\nOct 21, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nUniform random points on the globe\n\n\nThe earth’s a sphere: who knew?\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nGenerating randomly distributed evenly spaced points on the sphere takes some care. This post explores why and how.\n\n\n\n\n\nOct 20, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the Dulux colours\n\n\nThe ultimate in categorical mapping\n\n\n\nR\n\n\ngeospatial\n\n\nstuff\n\n\nmaps\n\n\naotearoa\n\n\n\nThe ‘Dulux colours of New Zealand’ are crying out to this geographer at least to be mapped, so I gave it a try.”\n\n\n\n\n\nSep 23, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nAffine transformations of sf objects\n\n\nManipulating simple features in sf is sorta simple, sorta not…\n\n\n\nR\n\n\ngeospatial\n\n\ntutorial\n\n\n\nDid you know the sf package lets you add vectors to geometries and multiply them by affine transformation matrices? Well, it does, and here are some examples.\n\n\n\n\n\nAug 12, 2021\n\n\nDavid O’Sullivan\n\n\n\n\n\n\n\n\n\n\n\n\nWhat three chords\n\n\nSeriously, WTC?!\n\n\n\njavascript\n\n\ngeospatial\n\n\nstuff\n\n\n\nA rather silly exploration of encoding spatial location using guitar chords (lots of them, but only three at a time).\n\n\n\n\n\nJul 3, 2020\n\n\nDavid O’Sullivan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Most talks are in web formats, and may include links that are now broken. Others are PDFs. Others are missing, but are listed because they give a sense of development over time.\n\n\n\nDate\nTitle\nNotes\n\n\n\n\n2025December\nSpatial data science: Just asking questions. Slides (Video to follow)\nSixth Spatial Data Science Symposium, Keynote presentation, (globally distributed remote meeting), 5 December.\n\n\n2025November\nWeavingspace: a new way to make multivariate maps\nFOSS4G International, Auckland, 20 November.\n\n\n2025September\nModelling potential environmental impacts of science activity in Antarctica (dark|light) theme\nRegional GIS Forum, Palmerston North, 26 September.\n\n\n2025July\nThe latest on tiled multivariate maps\nNew Zealand Cartographic Society, Te Herenga Waka, 4 July.\n\n\n2025June\nIt’s turtles all the way down: Simple models - complex outcomes\nCafé Complexité, Te Herenga Waka, 5 June.\n\n\n2025May\nLook ma! (Almost) no javascript!\nPython NZ Wellington Meetup, Sharesies Offices, Wellington, 15 May. Video\n\n\n2024August\nTime-space mapping in mountainous terrain\nGeoCart 2024, National Library, Wellington, 21-23 August\n\n\n2024April\nA spatially explicit agent-based model of on-farm environmental interventions\nAnnual Meeting of the American Association of Geographers, Honolulu, Hawai’i, United States\n\n\n2024March\n30 Day Map Challenge 2023\nMaptime! Wellington\n\n\n2023September\nFrom geographical information science via spatial data science to geographical computing\nFourth Spatial Data Science Symposium, University of Canterbury Hub, 7 September\n\n\n2023August\nComputing Geographically: Bridging Giscience and Geography\nUniversity of Canterbury, School of Earth and Environment, Research Seminar, 10 August\n\n\n2022November\nTiled & woven thematic maps\nRegional GIS Forum, Palmerston North, 11 November\n\n\n2022August\nComputing geographically: rethinking space and place in giscience\nKeynote at New Zealand Geospatial Research Colloquium, 29-30 August\n\n\n2022August\nTiled & woven thematic maps\nGeoCart 2022, National Library, Wellington, 24-26 August\n\n\n2022July\nR for geospatial\nSpatial Literacy User Group, Te Herenga Waka - Victoria University of Wellington\n\n\n2021November\nWeaving maps of multivariate data\nState of New Zealand Cartography, Special Meeting of the New Zealand Cartographic Society, Wellington\n\n\n2021September\nMapping the Dulux Colours of New Zealand Using R\nMaptime! Aotearoa, online\n\n\n2020November\nSpatially-explicit models for exploring COVID-19 lockdown strategies\nNew Zealand Geographical Society, Wellington\n\n\n2020November\nComputing geographically: rethinking giscience as geography\nUniversity of Utah, Department of Geography, Geography Awareness Week Colloquium\n\n\n2019September\nA spatial simulation model to explore the potential impact of gene drives as a control on invasive wasps\nGeocomputation 2019, Queenstown, 18-21 September\n\n\n2019August\nTheoretical geography: definitely harder than physics!\nVvoIP_Physics_Debates symposium\n\n\n2019July\nAvoiding the YAAWN syndrome\nAgents for Theory: From Cases to General Principles, Theory Development through Agent-based Modeling, International Workshop held at Herrenhäuser Palace, Hanover, Germany\n\n\n2018May\nSome translation required, or: A city is not a network either!\nInaugural Brian Coffey lecture and workshop in Geographical Information Science University of Washington, Tacoma\n\n\n2018April\nComputing with many spaces: Generalizing projections for the digital geohumanities and GIScience\n(with Luke Bergmann who presented) 114th Annual Meeting of the American Association of Geographers, New Orleans, LA\n\n\n2018March\nReimagining GIScience for relational spaces\nUniversity of Colorado Boulder, Department of Geography Colloquium Series\n\n\n2017December\nBridging GIScience and Geographical Thought\nGeographic Data Science Lab, University of Liverpool\n\n\n2017September\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nGraduate webinar series on Agent-based models, University of Minnesota. These are the same slides as a talk at Stanford a couple of years earlier…\n\n\n2017September\nSome translation required, or: A city is not a network either!\nInternational Symposium on The Future of Urban Network Research, University of Ghent, Belgium\n\n\n2017April\n‘Same only different’: rethinking the practice of digital urban geographies\n113th Meeting of the American Association of Geographers, Boston, MA\n\n\n2016November\nSimple spatial models: Building blocks for a process-based GIS?\nGeolunch Series, Geospatial Innovation Facility (GIF), University of California, Berkeley\n\n\n2016September\nSearching for common ground (again)\n(with Jim Thatcher and Luke Bergmann) Presented at 9th International Conference on Geographical Information Science (GIScience 2016), Montreal, Canada\n\n\n2016September\nSimple simulation models as a complexity ‘pattern language’\nLightning talk at Rethinking the ABCs: Agent-Based Models and Complexity Science in the age of Big Data, CyberGIS, and Sensor Networks pre-conference workshop at GIScience 2016, Montreal, Canada\n\n\n2016March\nSpatiality, maps, mathematics and critical human geography\nUniversity of Uppsala, Department of Social and Economic Geography\n\n\n2016January\nFuture GIS\n(with Matt Wilson) University of British Columbia, Department of Geography\n\n\n2015December\nThinking with and about models in geography\nUniversity of California, Davis. Geography Graduate Group seminar series.\n\n\n2015August\n(with Alex Singleton and Seth Spielman) Our town: How socioeconomics shape functional neighborhoods in American cities\nGeocomputation 2015, UT Dallas\n\n\n2015May\nSpatial simulation: Exploring pattern and process\nUNIGIS Salzburg Webinar\n\n\n2015May\nSimple spatial models: Building blocks for process-based GIS?\nStanford University\n\n\n2015May\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\nStanford University Libraries’ Center for Interdisciplinary Digital Research\n\n\n2015April\nIdentifying ‘narrative arcs’ to explain outcomes in an agent-based model of island resource exploitation\n(with George Perry) presented at the 110th Annual Meeting of the Association of American Geographers, Chicago, IL\n\n\n2014November\n‘Play well’: Learning about the world using spatial models\nUniversity of Oregon, Department of Geography and Complexity Science conference\n\n\n2014September\nUsing Personal Names to Explore Cultural, Ethnic and Linguistic Structure in Populations\nUC Berkeley, Department of Demography\n\n\n2014June\nSimple spatial models: building blocks for process-based GIS?\nInstitute of Australian Geographers – New Zealand Geographical Society Joint Conference, University of Melbourne\n\n\n2013August\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at SIRC NZ 2013, University of Otago, Dunedin, New Zealand. Extended abstract available here\n\n\n2013April\nTowards a ‘pattern language’ for spatial simulation models\n(with George Perry) presented at the 109th Annual Meeting of the Association of American Geographers, Los Angeles, CA\n\n\n2013February\nTowards a ‘pattern language’ for spatial simulation models\nDepartment of Geography, University of California Santa Barbara\n\n\n2012May\nNaming networks and population structure\nDepartment of Urban Engineering, University of Tokyo\n\n\n2012May\nSpatial Simulation: Exploring Pattern and Process – A Work in progress\nCentre for Spatial Information Science, University of Tokyo\n\n\n2012February\nAgent-based models: what are they good for? Or: did Schelling really need an ABM?\nPresented at the 108th Annual Meeting of the Association of American Geographers, New York, NY\n\n\n2011September\nModel Histories: The Generative Properties of Agent-Based Modelling\nPresented by James Millington (also with George Perry) at Annual Conference of the Royal Geographical Society with the Institute of British Geographers, Royal Geographical Society, London\n\n\n2011April\nNaming networks and population structure\n(with Pablo Mateos) presented at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA\n\n\n2011April\nDo physicists have geography envy?\nwith Steve Manson (who presented) at the 107th Annual Meeting of the Association of American Geographers, Seattle, WA. This paper was eventually published in much different form in the Annals of the American Association of Geographers.\n\n\n2009December\nSimulating long distance dispersal processes in spatially heterogeneous landscapes\n(with George Perry) presented at Geocomputation 2009, University of New South Wales, Sydney, Australia. A version of this work was published in Ecological Informatics."
  },
  {
    "objectID": "portfolio/04-antarctica.html",
    "href": "portfolio/04-antarctica.html",
    "title": "Science impacts in Antarctica",
    "section": "",
    "text": "This work aims to understand the potential for damaging environmental impacts in Antarctica. You can follow the work as it develops at this website, and also view slides from a recent talk here.\nThe approach I’m taking attempts to estimate ‘desire lines’ for human movement. Here’s a typical output:\n\nThis work is funded through Manaaki Whenua Landcare Research and Te Pūnaha Matatini."
  },
  {
    "objectID": "portfolio/01-1-wasp-control-model.html",
    "href": "portfolio/01-1-wasp-control-model.html",
    "title": "A model for gene drive control of wasps",
    "section": "",
    "text": "Over a number of years, I developed a model of the potential for gene-drives as a control on invasive wasp populations in New Zealand. This included a two year pause for COVID when the original paper was disappointingly rejected after revisions.\nAnyway, the model looks like this:\n\nThe general conclusion is that while the gene drive mechanism we explored would substantially reduce wasp populations this outcome would come with substantial downsides:\n\nIt would take years. This would make it highly likely that the modified genotype would make it back to the wasps home range in Eurasia, where while annoying (they are wasps after all!) they are an important part of ecosystems.\nIf control was discontinued before complete eradication, the wild population would rapidly recover, making accurate monitoring of the wild-GM population mix essential.\nEradication could only be achieved by releasing truly vast numbers of genetically modified wasps—many more than there are wild wasps out there! This result is summarised in the figure below\n\n\nwhere only very high intensity releases of GM wasps over many years are successful in eradicating wild wasps. This is Figure 5 from the paper explaining the inner workings and conclusions from the model:\n\nLester PJ, D O’Sullivan and GLW Perry. 2023. Gene drives for invasive wasp control: Extinction is unlikely, with suppression dependent on dispersal and growth rates. Ecological Applications 33(7) e2912. doi: 10.1002/eap.2912\n\nI presented an early version of the model at Geocomputation 2019 which is fondly remembered as the last conference before COVID reared its ugly head.\nAll the materials including the model are available here."
  },
  {
    "objectID": "portfolio/03-time-space-mapping.html",
    "href": "portfolio/03-time-space-mapping.html",
    "title": "Time-space mapping",
    "section": "",
    "text": "Another ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThis is one aspect of a wider project aimed at developing maps where the geometry is based not on ‘location’ but on the relationships — whether of time, money, people, or anything else of interest between places. Space conceived not as a fixed absolute background to things but as a living moving thing that both shapes and is shaped by unfolding events.\nTime-space mapping is a particular subset of this wider project focused on relations of estimated travel time between locations in a mountainous environment. There’s more detail on what we’re up to in this presentation I gave at GeoCart 2024.\nYou’ll find a video of dynamic time-space in the presentation and there’s another in my diary entry about GeoCart. Meanwhile, here’s a series of maps of estimated hiking times from a given starting location on a series of idealised conical mountains of increasing steepness."
  },
  {
    "objectID": "portfolio/00-weaving-and-tiling-maps.html",
    "href": "portfolio/00-weaving-and-tiling-maps.html",
    "title": "Tiled and woven maps",
    "section": "",
    "text": "This is an ongoing project in collaboration with Luke Bergmann at University of British Columbia.\nThe idea is to map many different variables by combining them by ‘weaving’ or ‘tiling’. Here’s an example\n\nDifferent strands in the woven pattern represent different socioeconomic (or other) variables in the mapped area, and are coloured in the usual ‘choroplethic’ manner. We’re still figuring out the details, but some of the resulting maps are quite striking. Here’s a tiled (not woven) one\n\nExplanations of exactly what the heck is going on here are at the project repo, where you’ll also find links to presentations, example notebooks, and the code to make maps like these yourself!"
  },
  {
    "objectID": "portfolio/02-moving-the-middle.html",
    "href": "portfolio/02-moving-the-middle.html",
    "title": "Agent modeling of land management",
    "section": "",
    "text": "This is part of a large MBIE funded project ‘Moving the middle’.\nThe part of the research I am most involved in is developing an agent-based model of how land managers (mostly farmers) make changes in landuse and farm management practices. Other parts of the wider project are examining the motivations and drivers of farmer behaviour at an individual level, whereas the model explores how individual actions might scale up across landscapes.\nSlides from a presentation I gave about the model can be viewed here.\nAnd here is a more recent screenshot of the model in action."
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "I offer training in any of the areas listed below. Contact me for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing spatial data science\n\n\nA suite of short courses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial analysis and modelling\n\n\nGeographic information analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeographical computing\n\n\nPython programming for geospatial\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "training/02-geographical-computing.html",
    "href": "training/02-geographical-computing.html",
    "title": "Geographical computing",
    "section": "",
    "text": "Go to the materials\nThese pages outline a one semester (36 contact hours) class in python programming for geospatial that was last taught at Victoria University of Wellington as GISC 420 in the first half of 2022.\nI am still in the process of cleaning the materials up for potential conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to python programming for geospatial but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html",
    "title": "ALGIM 2024",
    "section": "",
    "text": "What is ALGIM? According to the website,\nand\nSo ALGIM is a place where local government ICT people can pool resources and expertise to make the challenging business of doing local government a little bit easier. To that end ALGIM offers training, webinars, forums for the exchange of ideas. One of those forums is the annual conference.2"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#an-industry-conference-not-an-academic-conference",
    "title": "ALGIM 2024",
    "section": "An industry conference not an academic conference",
    "text": "An industry conference not an academic conference\nALGIM’s annual conference is a showpiece event. This year the conference theme was ‘Driving collaboration’, approached from a range of perspectives. Topics such as AI in local government, standardisation of processes and plaforms, and fostering collaboration loomed large. There were interesting keynotes from an eclectic mix of people such as :Audrey Tang, Nicole Skews-Poole, Julian Moore, and Michael Baker.\nBut the ALGIM conference is certainly not an academic conference. I’m a veteran of those having attended 50 or so in the last two decades, and if you add in symposia, workshops, and sundry other varieties of academic meeting, a few more even than that. The rhythms of these are familiar: 20 minute talks in sessions an hour or two long, with breaks for refreshments, and (for the most part) not much connection to the world of work beyond the meeting. Obviously everyone participating is at work, and people gossip about work in the breaks, but the subject matter under discussion in sessions is not work itself, but scientific and scholarly questions of one kind or another.\nAnyway, this was not that, but a rather different kind of event. It’s very much a networking meeting for people with shared professional interests3 and as such absolutely about work. I do have some previous experience of this kind of event4 and knew going in that in addition to keynotes and shorter presentations on more specialised topics, many technology vendors would be there hawking their wares, or perhaps more realistically simply reminding local government ICT managers that they exist, should they be considering revisiting their backup, comms, data security, disaster recovery, online payment systems, or whatever other ICT options."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#wot-no-gis",
    "title": "ALGIM 2024",
    "section": "Wot? No GIS?",
    "text": "Wot? No GIS?\nAnd that, to a ‘GISer’, was the most interesting aspect. I fully expected to see all the usual local geospatial suspects here. Esri’s local representative Eagle Technology was present, albeit not in force (mostly they were handing out ice cream!), along with one or two others, such as GBS and local FME partner Seamless, but that was more or less it. No Koordinates, Catalyst, Orbica, Lynker Analytics, or LINZ.\nNow… that might be because (i) nobody goes to a conference to buy enterprise GIS, or even to change their enterprise GIS plans, or (ii) Esri/Eagle’s position in GIS in local government in New Zealand is so strong that there isn’t much point in anybody else showing up. On the other hand, it might be because… well, because however often geospatial folks tell themselves (and anyone who will listen) that 80%—or some other high percentage!—of all data are spatial, and no matter how self-evidently spatial it seems that the job of local government might be, when it comes to information management in local government, GIS is not much of a thing.\nAnd yet, GIS clearly is important in local government, even if it is treated as slightly peripheral by ICT managers."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#what-was-i-doing-there-anyway",
    "title": "ALGIM 2024",
    "section": "What was I doing there anyway?",
    "text": "What was I doing there anyway?\nI was there to chair a panel discussion on current challenges/developments in geospatial (AI, digital twins, barriers to collaboration, convergence with data analytics, education and training), and to convene a ‘GIS Round Table discussion’.\nBoth sessions were interesting, although the panel discussion time frame was a little too short at only half an hour to really get into things with the excellent contributors Andrew Steffert from Horizons Regional Council, Kerri Gray from CreateBig, and Scott Campbell from Eagle.\nBut with all due deference to those experts, the Round Table was, I thought, more telling. It wasn’t very big with about a dozen in attendance, but it was fairly representative with ‘GIS officers’ (for want of a better term) from councils big and small. The striking thing was how quickly even this very limited forum became a source of support and consultation among the people around the table. The strong sense I got was of GIS officers overwhelmed by the numerous expectations around their role(s). There was also a sense, especially from smaller councils where there might be only one or two people ‘doing GIS’, that those people feel isolated with limited options to reach out for assistance. And so, within minutes of introductions, people were comparing notes on how others had approached various common tasks and what technologies or data resources they were using. There really is nothing quite like getting people in a room together for learning what their challenges are.\nIn short, however tacked on that “and GIS” might appear in ALGIM’s mission statement, the GIS officers in local government around New Zealand (and I imagine also Australia) seem more than ready to take advantage of whatever forums ALGIM might be able to offer for learning, exchanging ideas, sharing approaches, and collaborating. Hopefully, this can happen soon, since it doesn’t seem like the job of GIS officer in local government is going to get any easier any time in the near future!"
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#ps-not-forgetting",
    "title": "ALGIM 2024",
    "section": "PS Not forgetting",
    "text": "PS Not forgetting\n\n\n\nThe Hamilton City Kai Map\n\n\nI would be remiss if I didn’t also mention seeing some really nice presentations of award nominated geospatial projects such as the Hamilton City Kai Map, Northland Regional Council’s use of geospatial to integrate the Health and Safety processes associated with their numerous fieldwork activities, and a Christchurch City map-centred streamling of their building warrant of fitness process (i.e., building inspections). The latter two are internal work process streamlining projects so unfortunately don’t have public websites I can link to."
  },
  {
    "objectID": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "href": "posts/2024-11-22-algim-2024/algim-2024.html#footnotes",
    "title": "ALGIM 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomebody always has to come last, but park that slightly tacked on feeling “and GIS” just for now…↩︎\nALGIM now also has an Australian branch, which will see the organisation grow substantially in the next few years.↩︎\nAcademic conferences are also that, but with a different emphasis.↩︎\nAs I’ve recounted elsewhere it was at a meeting like this that I chanced upon GIS. If you’re interested it’s mentioned about 2 minutes into this podcast.↩︎"
  },
  {
    "objectID": "posts/2025-07-30-counting-points-in-polygons/index.html",
    "href": "posts/2025-07-30-counting-points-in-polygons/index.html",
    "title": "Counting points in polygons",
    "section": "",
    "text": "Putting materials together for a short training course on introductory python programming for GISers, and also for two sessions on making maps and doing GIS in R for ResBaz 2025 I was once again puzzled by how the generally great tools in these two free ecosystems for doing geospatial analysis make it at best non-obvious and at worst downright difficult to do some seemingly obvious things.\nA case in point is counting points in polygons. Herewith then, recipes in Python and in R for doing just that."
  },
  {
    "objectID": "posts/2025-07-30-counting-points-in-polygons/index.html#counting-points-in-polygons-using-geopandas",
    "href": "posts/2025-07-30-counting-points-in-polygons/index.html#counting-points-in-polygons-using-geopandas",
    "title": "Counting points in polygons",
    "section": "Counting points in polygons using geopandas",
    "text": "Counting points in polygons using geopandas\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\n1import matplotlib.pyplot as plt\n\n\n\n1\n\nFor making maps (albeit with some difficulty!)\n\n\n\n\nHere are the data we’ll use in both examples. It doesn’t much matter where they are from or what they relate to for present purposes.\n\n\nCode\npoints = gpd.read_file(\"points.gpkg\")\npolygons = gpd.read_file(\"polygons.gpkg\")\n\nax = polygons.plot(lw = 0.5, fc = \"lightgrey\", ec = \"w\", figsize = (10, 10))\npoints.plot(ax = ax, markersize = 1, color = \"k\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUsing the GeoSeries spatial predicate methods\nI’m not going to be nice about this. geopandas makes counting points in polygons unreasonably complicated. Maybe I am missing something. But I don’t think so.1\nThe heart of the problem is that the binary spatial predicate functions applicable to GeoSeries, for reasons that are unclear, are applied row-wise, not pair-wise between every possible combination of the two GeoSeries being compared. That’s what the various sf::st_* functions in R do, and is the basis for the approach available there (see below).\nThat means that what (say) polygons.contains(points) returns is a list of boolean True/False results for whether the first polygon contains the first point, the second polygon the second point, and so on. I have a hard time thinking of a situation where this would be useful information. You can get it to base the alignment between rows on an index variable, but it’s unclear how this makes the process any more useful. That leaves little choice, using the spatial predicate functions, but to iterate over the two GeoSeries and compile counts. You can halfway use the vectorised operation of geopandas by counting how many points are in each polygon, but iterating over the polygons in python seems to be unavoidable.\nWe can make this into a (near) one-liner, although it took me a while to figure out the code below, which is the same basic idea as the approach shown later for R.\n\n\nCode\nn_points = {\n  name: points.geometry.within(polygon).sum()\n  for name, polygon in zip(polygons.name, polygons.geometry)\n}\nn_points = pd.Series(n_points.values(), name = \"n_points\")\nn_points\n\n\n0      34\n1      26\n2       1\n3       0\n4       7\n     ... \n83      8\n84      0\n85    208\n86      6\n87      0\nName: n_points, Length: 88, dtype: int64\n\n\nAnd that’s a data series we can add to the original polygon dataset:\n\n\nCode\npd.concat([polygons, n_points], axis = \"columns\")\n\n\n                               name  ... n_points\n0                          Adelaide  ...       34\n1           Aro Street-Nairn Street  ...       26\n2                            Awarua  ...        1\n3                           Belmont  ...        0\n4                   Berhampore East  ...        7\n..                              ...  ...      ...\n83                        Wadestown  ...        8\n84          Wellington City-Marinas  ...        0\n85  Willis Street-Cambridge Terrace  ...      208\n86                           Wilton  ...        6\n87                        Woodridge  ...        0\n\n[88 rows x 3 columns]\n\n\nBut I don’t know. This just feels wrong, and I have no confidence it will scale well because we are iterating over the polygons in Python (even if I am using a comprehension).\nHere are a couple of other cunning plans to get the point counts in polygon data table order.\n\n\nCode\n# define a convenience function\ndef points_in_polygon(poly, pts):\n  return poly.contains(pts).sum()\n\npolygons.geometry.apply(points_in_polygon, pts = points.geometry).rename(\"n_points\")\n\n\n0      34\n1      26\n2       1\n3       0\n4       7\n     ... \n83      8\n84      0\n85    208\n86      6\n87      0\nName: n_points, Length: 88, dtype: int64\n\n\nOr even using a lambda function (if you are truly obsessed with one-liners).\n\n\nCode\npolygons.geometry.apply(lambda x: x.contains(points.geometry).sum()).rename(\"n_points\")\n\n\n0      34\n1      26\n2       1\n3       0\n4       7\n     ... \n83      8\n84      0\n85    208\n86      6\n87      0\nName: n_points, Length: 88, dtype: int64\n\n\nWhether or not these make the whole operation vector-based is unclear. The pandas apply() function should be, and the function we are applying is vectorised over the GeoSeries of points but limited testing the speed of all three approaches2 suggests there is not much to choose between them performance-wise.\n\n\nProbably the right approach: using GeoDataFrame.sjoin\nA more conservative approach is to use spatial join operations, then do some post-join clean up, then join the resulting point counts back to the original data. Here’s a sequence of steps I’ve settled on over time. Strap in…\nFirst spatially join the points to the polygons.\n\n\nCode\npolygons.sjoin(points, how = \"left\")\n\n\n         name  ... point_id\n0    Adelaide  ...    966.0\n0    Adelaide  ...   1154.0\n0    Adelaide  ...    710.0\n0    Adelaide  ...    155.0\n0    Adelaide  ...   1377.0\n..        ...  ...      ...\n86     Wilton  ...    272.0\n86     Wilton  ...    397.0\n86     Wilton  ...    170.0\n86     Wilton  ...   1502.0\n87  Woodridge  ...      NaN\n\n[1704 rows x 4 columns]\n\n\nWe have multiple copies of each polygon each associated with a single point, and because we used how = \"left\" we have retained even those polygons with no matching points. Now we can use groupby to group entries that share the same polygon name and count how many of each there are. Because the count() method ignores missing data it will count the polygons with no matching point as zeros (this is a rather finicky, and I would guess potentially fragile detail).\n\n\nCode\n(polygons.sjoin(points, how = \"left\")\n1     .loc[:, [\"name\", \"point_id\"]]\n     .groupby(\"name\", as_index = False)\n     .count())\n\n\n\n1\n\nRestrict to just the id variables of the two datasets\n\n\n\n\n                               name  point_id\n0                          Adelaide        34\n1           Aro Street-Nairn Street        26\n2                            Awarua         1\n3                           Belmont         0\n4                   Berhampore East         7\n..                              ...       ...\n83                        Wadestown         8\n84          Wellington City-Marinas         0\n85  Willis Street-Cambridge Terrace       208\n86                           Wilton         6\n87                        Woodridge         0\n\n[88 rows x 2 columns]\n\n\nWe should rename that column to reflect the information it now contains.\n\n\nCode\n(polygons.sjoin(points, how = \"left\")\n     .loc[:, [\"name\", \"point_id\"]]\n     .groupby(\"name\", as_index = False)\n     .count()\n     .rename(columns = {\"point_id\": \"n_points\"}))\n\n\n                               name  n_points\n0                          Adelaide        34\n1           Aro Street-Nairn Street        26\n2                            Awarua         1\n3                           Belmont         0\n4                   Berhampore East         7\n..                              ...       ...\n83                        Wadestown         8\n84          Wellington City-Marinas         0\n85  Willis Street-Cambridge Terrace       208\n86                           Wilton         6\n87                        Woodridge         0\n\n[88 rows x 2 columns]\n\n\nAnd finally, we’d want to join all this back to the original data.\n\n\nCode\npip = polygons.merge(\n  polygons.sjoin(points, how = \"left\")\n    .loc[:, [\"name\", \"point_id\"]]\n    .groupby(\"name\", as_index = False)\n    .count()\n    .rename(columns = {\"point_id\": \"n_points\"})\n  )\n  \npip\n\n\n                               name  ... n_points\n0                          Adelaide  ...       34\n1           Aro Street-Nairn Street  ...       26\n2                            Awarua  ...        1\n3                           Belmont  ...        0\n4                   Berhampore East  ...        7\n..                              ...  ...      ...\n83                        Wadestown  ...        8\n84          Wellington City-Marinas  ...        0\n85  Willis Street-Cambridge Terrace  ...      208\n86                           Wilton  ...        6\n87                        Woodridge  ...        0\n\n[88 rows x 3 columns]\n\n\nAll told, that’s quite the journey. Here it all is wrapped up in a reusable function, which you are free to cut out and keep!\n\n\nCode\ndef count_points_in_polygons(\n    polys:gpd.GeoDataFrame, \n    pts:gpd.GeoDataFrame, \n    id_var:str) -&gt; gpd.GeoDataFrame:\n    \"\"\"Counts points in polygons and appends new column to GeoDataFrame.\n\n    Args:\n        polys (gpd.GeoDataFrame): the polygons within which to count.\n        pts (gpd.GeoDataFrame): the points to count.\n        id_var (str): a variable that uniquely identifies the polygons.\n\n    Returns:\n        gpd.GeoDataFrame: polygon GeoDataFrame with added column 'n_points' \n            containing result.\n    \"\"\"\n    pt_var = pts.columns[0]\n    return polys.merge(\n        polys.sjoin(pts, how = \"left\")\n            .loc[:, [id_var, pt_var]]\n            .groupby(id_var, as_index = False)\n            .count()\n            .rename(columns = {pt_var: \"n_points\"}))\n\ncount_points_in_polygons(polygons, points, \"name\")\n\n\n                               name  ... n_points\n0                          Adelaide  ...       34\n1           Aro Street-Nairn Street  ...       26\n2                            Awarua  ...        1\n3                           Belmont  ...        0\n4                   Berhampore East  ...        7\n..                              ...  ...      ...\n83                        Wadestown  ...        8\n84          Wellington City-Marinas  ...        0\n85  Willis Street-Cambridge Terrace  ...      208\n86                           Wilton  ...        6\n87                        Woodridge  ...        0\n\n[88 rows x 3 columns]\n\n\nI’ve again done some limited performance testing of this method and it definitely seems to be faster than the GeoSeries spatial predicate methods approaches. I assume this is because it is fully vectorised, and perhaps even makes use internally of spatial indexes, although I have no detailed knowledge of the inner workings of geopandas on which to base that guess…\nAnyway, onwards to R!"
  },
  {
    "objectID": "posts/2025-07-30-counting-points-in-polygons/index.html#counting-points-in-polygons-using-sf",
    "href": "posts/2025-07-30-counting-points-in-polygons/index.html#counting-points-in-polygons-using-sf",
    "title": "Counting points in polygons",
    "section": "Counting points in polygons using sf",
    "text": "Counting points in polygons using sf\nsf, while still not having a true one-liner for this operation, at least makes things less difficult with pair-wise comparison based spatial predicate functions.\n\n\nCode\nlibrary(sf)\nlibrary(ggplot2)\n\npoints &lt;- st_read(\"points.gpkg\")\npolygons &lt;- st_read(\"polygons.gpkg\")\n\npoint_counts &lt;- polygons |&gt;\n  st_contains(points) |&gt;\n  lengths()\n\npolygons$n_points &lt;- point_counts\n\n\nThis is easier to make sense of than than the spatial join approach in geopandas, and because it’s based on a spatial predicate function that does pair-wise comparisons between geometries it’s also cleaner. The method relies on the base R lengths() function which reports the length of each vector of index numbers reported by st_contains in its default ‘sparse matrix’ mode.\nOverall, it’s not at all obvious starting from a blank screen that this is how you should go about counting points in polygons in sf, but it works, and until someone tells me otherwise this is how I’ll do it.\nOverall the sf approach isn’t so much work as to make it worth writing a function to apply it. Even so, like its geopandas equivalents it seems unnecessarily indirect!\nAnyway, if you have stuck around this long, you’ve earned another map.\n\n\nCode\nggplot(polygons) +\n  geom_sf(aes(fill = n_points)) +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +\n  theme_void()"
  },
  {
    "objectID": "posts/2025-07-30-counting-points-in-polygons/index.html#footnotes",
    "href": "posts/2025-07-30-counting-points-in-polygons/index.html#footnotes",
    "title": "Counting points in polygons",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI would be happy to hear otherwise.↩︎\nUsing the ipython %timeit magic↩︎"
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html",
    "href": "posts/2025-06-13-population-quadrants/index.html",
    "title": "How to cut a cake into four equal slices",
    "section": "",
    "text": "This journey into the heart of darkness post was inspired by a LinkedIn post from Alasdair Rae linking to a map he made cutting the UK into four equal population slices. The post led to a surprising amount of complicated mathematical discussion on math reddit1 concerning whether such a dissection should always be possible, under what conditions, and if the dissection could be extended to more slices. I think the conclusion there is that if the surface (considering the population map as a surface) is everywhere continuous—roughly speaking it varies smoothly from place to place—then such a dissection should always be possible.\nThis conclusion surprised me a little.2 Here’s my line of thinking: I can see that there should always be a line in any given direction that halves the population. Draw a line at one side of the map area. Slide it across the map area to the other side. At the start all of the population is on one side of the line and at the end it is all on the other side of the line. If the distribution is continuous, then by the intermediate value theorem(IVT) from calculus, the line must at some point on its journey have been at a point where it cut the population exactly in half. So, pick a direction, find the line in that direction that bisects the population. The problem is that there is no guarantee that the bisector at 90° in combination with the first one will divide the population into equal quarters. It might if you ‘get lucky’, but there’s no prior reason why it would.\nAt this point, the mathematicians redditors invoke IVT and argue that you can choose the second line at an angle so that you do get equal quarters and then slide the crossover point and angle around until you find the place and orientation where the two lines do quadrisect3 the population.\nAnyway, that’s all by way of background. Let’s see if we can find that uh… sweet spot for Aotearoa New Zealand!\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n1library(scales)\n2library(patchwork)\n3library(spatstat)\n\n\n\n1\n\nFor nicer numeric labels on some axes.\n\n2\n\nFor nice multipanel plots.\n\n3\n\nFor a weighted median function."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#data",
    "href": "posts/2025-06-13-population-quadrants/index.html#data",
    "title": "How to cut a cake into four equal slices",
    "section": "Data",
    "text": "Data\nI used gridded population data for New Zealand at 250m resolution. The exact dataset I used was the early release data sitting around on my hard drive from when I made this map. You can get current releases of the same data here. I converted the gridded data to an x, y, population CSV file because that’s all I need for the process I followed. I also adjusted the coordinates by centring them on their mean centre. This is to keep the coordinate values in check when we start rotating points around an origin.\n\n\nCode\npop_grid &lt;- read.csv(\"nz-pop-grid-250m.csv\")\ncx &lt;- mean(pop_grid$x)\ncy &lt;- mean(pop_grid$y)\npop_grid &lt;- pop_grid |&gt;\n  mutate(x = x - cx, y = y - cy)\n\nnz &lt;- st_read(\"nz.gpkg\") |&gt;\n  mutate(geom = geom - c(cx, cy))\n\n\nAnd here is a basemap to get everyone oriented.\n\n\nCode\nbasemap &lt;- ggplot(nz) + \n  geom_sf(lwd = 0) +\n  geom_tile(data = pop_grid, \n            aes(x = x, y = y, fill = pop_250m_grid)) +\n  scale_fill_distiller(palette = \"Reds\", direction = -1) +\n  theme_void() +\n  theme(legend.position.inside = c(0.2, 0.7),\n        legend.position = \"inside\")\nbasemap\n\n\n\n\n\n\n\n\nFigure 1: Base map showing distribution of Aotearoa New Zealand population.\n\n\n\n\n\nThe Chatham Islands are included here, although, since this problem is all about medians not means, it’s unlikely that outlier population makes much difference to these calculations. In any case, to a first order approximation everyone is in Auckland—that concentration of population at the narrowest point of the northern isthmus of Te Ika-a-Māui.4 Not really, but Auckland does loom large in what follows, as it does in cartograms of New Zealand population."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#a-concept-of-a-plan-of-an-algorithm",
    "href": "posts/2025-06-13-population-quadrants/index.html#a-concept-of-a-plan-of-an-algorithm",
    "title": "How to cut a cake into four equal slices",
    "section": "A concept of a plan of an algorithm",
    "text": "A concept of a plan of an algorithm\nI briefly contemplated taking an approach similar to that outlined in the mathematical background above, but decided against it. Partly this was because building it would involve (broadly) two steps:\n\nA function to find a bisector;\nGiven a bisector, a function to find another bisector at 90°, that, given the first one, quadrisects the population. This would likely involve a complicated iterative procedure sliding and rotating the pair of bisectors around until the sweet spot is found.\n\nMy short-circuiting brain kicked in after item (1) to say, “well, if you’ve found a bisector, just find all the bisectors5 and then test to see if any pair at 90° meet the criteria.” This would in no sense be a complete or perfect solution, but it would at least start to give some sense of how sensitive to a precise choice of locations and angles the problem is.6 Also, it would avoid having to deal with (2) which sounds, well… tricky.\nSo the plan now was\n\nA function to find a bisector; and\nMake a bunch of bisectors and test to see if any pairs at 90° are quadrisectors.7\n\nThis is kind of an accept-reject algorithm. It’s obviously inefficient, since we generate many incorrect candidate solutions, but if I have a reasonably quick way to find bisectors the cost of those incorrect solutions shouldn’t be too high.\nConcept of a plan in place, let’s go!"
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#a-population-bisector-function",
    "href": "posts/2025-06-13-population-quadrants/index.html#a-population-bisector-function",
    "title": "How to cut a cake into four equal slices",
    "section": "A population bisector function",
    "text": "A population bisector function\nHere comes the science…\nI could search for the bisector at a given angle, by sliding a line across a map and moving it forward or backwards depending on the population on either side of it, until I found the desired spot. That would certainly be computationally satisfying, but it’s possible to derive the line directly using a weighted median function. The weighted median of a set of numbers that have an associated weight, is the value at which the total weight associated with numbers above and below the median is equal. spatstat provides a convenient implementation, so we don’t need to do that part.\nWe know the slope of the desired straight line. So we rotate our x, y coordinates such that the bisector would lie flat along the horizontal axis and determine the weighted median in the vertical direction. This is the offset from the origin of the bisector perpendicular to its direction of slope. Some mathematical manipulation based on the standard form equation of a straight line \\(Ax+By+C=0\\) given that we know the line’s slope, and its \\(x\\)- and \\(y\\)-intercepts (based on the calculated perpendicular offset) allows us to get the \\(A\\), \\(B\\), and \\(C\\) coefficients we need. If the desired angle of the bisector is either horizontal or vertical no rotation is needed and we can handle these special cases easily.\nSo here’s code for all that, along with a couple of convenience functions to return the slope and \\(y\\) intercept of the straight line.\n\n\nCode\nrotation_matrix &lt;- function(a) {\n  matrix(c(cos(a), -sin(a), sin(a), cos(a)), 2, 2, byrow = TRUE)\n}\n\nbisector &lt;- function(pts, angle) {\n  if (angle == 0) {\n    median_y &lt;- weighted.median(pts$y, pts$pop_250m_grid)\n    A &lt;- 0; B &lt;- 1\n1    C &lt;- -weighted.median(pts$y, pts$pop_250m_grid)\n  } else if (angle == 90) {\n    A &lt;- 1; B &lt;- 0\n2    C &lt;- -weighted.median(pts$x, pts$pop_250m_grid)\n  } else {\n    a &lt;- angle * pi / 180\n3    pts_r &lt;- rotation_matrix(-a) %*%\n             matrix(c(pts$x, pts$y), nrow = 2, byrow = TRUE)\n    median_y &lt;- weighted.median(pts_r[2, ], pts$pop_250m_grid)\n4    A &lt;-  median_y / cos(a)\n    B &lt;- -median_y / sin(a)\n    C &lt;- -A * B\n  }\n  list(A = A, B = B, C = C)\n}\n\nget_slope &lt;- function(straight_line) {\n  -straight_line$A / straight_line$B\n}\n\nget_intercept &lt;- function(straight_line) {\n  -straight_line$C / straight_line$B\n}\n\n5get_ggline &lt;- function(straight_line, ...) {\n6  if (straight_line$B == 0) {\n    geom_vline(aes(xintercept = -straight_line$C / straight_line$A), ...)\n  } else {\n    geom_abline(aes(intercept = get_intercept(straight_line), \n                    slope = get_slope(straight_line)), ...)\n  }\n}\n\n\n\n1\n\nFor the horizontal bisector we want the weighted median of the y coordinates.\n\n2\n\nAnd for the vertical bisector, the weighted median of the x coordinates.\n\n3\n\nRotate coordinates by the negative of the line’s slope angle so we can find the weighted median perpendicular to the line.\n\n4\n\nYou’ll have to trust me on the calculation for A, B, and C, but keep in mind that since \\(Ax+By+C=0\\), the gradient of the line is given by \\(-A/B\\) and by inspection that’s true of these calculations, so that part checks out!\n\n5\n\n... means we can pass additional plot options in from the calling code.\n\n6\n\nWhen B == 0 the line is vertical so we need a geom_vline\n\n\n\n\nFor a given bisector we can determine which cells are either side of it by checking if \\(Ax+By+C\\) is positive or negative. Given that result, we can also easily determine the population either side of such a line.\n\n\nCode\n1get_cells_one_side_of_line &lt;- function(pts, sl, above = TRUE) {\n  if (above) {\n    pts |&gt; mutate(chosen = sl$A * x + sl$B * y + sl$C &gt; 0) |&gt;\n      pull(chosen)\n  } else {\n    pts |&gt; mutate(chosen = sl$A * x + sl$B * y + sl$C &lt;= 0) |&gt;\n      pull(chosen)\n  }\n}\n\nget_pop_one_side_of_line &lt;- function(pts, sl, above = TRUE) {\n  sum(pts$pop_250m_grid[get_cells_one_side_of_line(pts, sl, above)])\n}\n\n\n\n1\n\nget_cells_one_side_of_line reports a logical TRUE/FALSE vector for which cells are on the requested side of the line. We do it this way to make it easier to combine the intersection of lines to determine quadrant populations later.\n\n\n\n\n\nSanity check\nOK. So let’s see if all that has worked by running an example for a line at 30°\n\n\nCode\nx30 &lt;- bisector(pop_grid, 30)\nbasemap + get_ggline(x30, linetype = \"dashed\", lwd = 0.5) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nFigure 2: A map to sanity check the methods developed so far.\n\n\n\n\n\nSo we get a line at 30° as required. What are the populations either side of it?\n\n\nCode\nc(get_pop_one_side_of_line(pop_grid, x30),\n  get_pop_one_side_of_line(pop_grid, x30, FALSE))\n\n\n[1] 2542977 2543049\n\n\nNot precisely equal, but pretty close. And of course, because we are not subdividing population if the line passes through a raster cell, but assigning all of the population of a cell to whichever side of the line its centre falls in, we wouldn’t expect an exact answer."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#but-i-wanted-quadrisection",
    "href": "posts/2025-06-13-population-quadrants/index.html#but-i-wanted-quadrisection",
    "title": "How to cut a cake into four equal slices",
    "section": "But I wanted quadrisection",
    "text": "But I wanted quadrisection\nIt has just occurred to me that they called this quartering in medieval times, as in “hung, drawn and quartered”… but let’s not dwell on that, and move on.\nTo get quadrant populations we use two lines and get the various combinations of population above/below each.\n\n\nCode\nget_quadrant_pops &lt;- function(pts, sl1, sl2) {\n1  above1 &lt;- get_cells_one_side_of_line(pts, sl1)\n  above2 &lt;- get_cells_one_side_of_line(pts, sl2)\n2  c(sum(pts$pop_250m_grid[ above1 &  above2]),\n    sum(pts$pop_250m_grid[ above1 & !above2]),\n    sum(pts$pop_250m_grid[!above1 & !above2]),\n    sum(pts$pop_250m_grid[!above1 &  above2]))\n}\n\n\n\n1\n\nGet logical vectors of which cells are above the two lines.\n\n2\n\nAnd then we can combine them to get all the pairwise combinations above/below the lines and calculate the populations.\n\n\n\n\nA function to plot the quadrant populations at a range of angles is convenient. Details in the cell below if you are interested.\n\n\nCode\nplot_range &lt;- function(angles) {\n  bisectors      = lapply(angles     , bisector, pts = pop_grid)\n  perp_bisectors = lapply(angles + 90, bisector, pts = pop_grid)\n  df &lt;- data.frame(angle = angles)\n1  df[, c(\"pop1\", \"pop2\", \"pop3\", \"pop4\")] &lt;-\n        mapply(get_quadrant_pops, bisectors, perp_bisectors,\n               MoreArgs = list(pts = pop_grid)) |&gt; t()\n\n  ggplot(df |&gt; select(angle, pop1:pop4) |&gt; \n               pivot_longer(cols = pop1:pop4)) +\n    geom_line(aes(x = angle, y = value, group = name), \n              lwd = 0.2, alpha = 0.35) +\n    geom_point(aes(x = angle, y = value, colour = name), \n               size = 0.5) +\n    scale_colour_brewer(palette = \"Set1\", name = \"Quadrant\") +\n    ylab(\"Estimated population\") +\n    scale_x_continuous(breaks = angles[seq(1, length(angles), 10)],\n                       minor_breaks = angles) +\n    scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n    theme_minimal()\n}\n\n\n\n1\n\nAssigning the four numbers produced by get_quadrant_pops in this way seems to be the fastest approach, and preferable to a for loop. If you are writing for loops in R or Python, you are losing.\n\n\n\n\nAnd now we can plot the quadrant populations across a range of angles from 0 to 90°.\n\n\nCode\ng1 &lt;- plot_range(-1:90) \ng1 + annotate(\"rect\", xmin = c(-1, 76.5, 79), \n                      xmax = c(1, 78.5, 81), \n                      ymin = -Inf, ymax = Inf, \n              fill = \"goldenrod\", alpha = 0.3, lwd = 0)\n\n\n\n\n\n\n\n\nFigure 3: Variation in quadrant populations as the orientation of the first bisector is varied.\n\n\n\n\n\nAs expected these are actually two pairs of equal sums given that the two lines we are using are bisectors, so we expect diagonally opposite quadrants to have equal population, but not necessarily all four to be equal. There are three highlighted orientations where all four populations seem to converge, at around 0°, 77°, and 80°.\n\n\nCode\ng2 &lt;- plot_range(seq(-0.5,  0.5, 0.025)) + guides(colour = \"none\")\ng3 &lt;- plot_range(seq(  77,   78, 0.025)) + guides(colour = \"none\")\ng4 &lt;- plot_range(seq(79.5, 80.5, 0.025)) + guides(colour = \"none\")\n(g2 + g3 + plot_layout(axes = \"collect\")) / g4\n\n\n\n\n\n\n\n\nFigure 4: A closer look at the quadrant populations at angles where they appear equal.\n\n\n\n\n\nProbably for a continuous, infinitely divisible population surface these would all be solutions to our quartering problem, but if we have a closer look (below) we can see that only the solution close to 80° actually converges for our discretised data. The other two cases have (I assume) some population grid cells jumping back and forward across our lines as we shift them slightly. On a continuous population surface I assume the behaviour at these angles would be more like the smooth variation we see in the 80° case.8"
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#so-finally",
    "href": "posts/2025-06-13-population-quadrants/index.html#so-finally",
    "title": "How to cut a cake into four equal slices",
    "section": "So finally…",
    "text": "So finally…\nThe zoomed in view around 80° suggests that bisectors at angles of 79.975° and 169.975° are pretty much the ‘answer’.\nHere’s a map, and the estimated populations in each quadrant, which, given the discretisation of our population data are very close to equal.\n\n\nCode\nxs &lt;- c(79.975, 169.975) |&gt; lapply(bisector, pts = pop_grid)\nx_df &lt;- data.frame(\n  slope     = xs |&gt; sapply(get_slope), \n  intercept = xs |&gt; sapply(get_intercept))\n\nbasemap + \n  geom_abline(data = x_df, \n              aes(slope = slope, intercept = intercept),\n              linetype = \"dashed\", lwd = 0.5) +\n  guides(fill = \"none\")\n\nget_quadrant_pops(pop_grid, xs[[1]], xs[[2]])\n\n\n[1] 1271354 1271442 1271630 1271600\n\n\n\n\n\n\n\n\nFigure 5: The best fit equal population quadrants.\n\n\n\n\n\nIt’s unsurprising as I hinted at the start, that one of the bisectors passes through Auckland, with over a third of the country’s population living there. Going anti-clockwise, starting from the northeast quadrant, these four ‘regions’ might be called Part of Auckland and Waikato-Bay of Plenty, Part of Auckland and Northland, Most of Te Waipounamu and Taranaki, and What’s Left of Te Waipounamu and Te Ika-a-Māui."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#final-thoughts",
    "href": "posts/2025-06-13-population-quadrants/index.html#final-thoughts",
    "title": "How to cut a cake into four equal slices",
    "section": "Final thoughts",
    "text": "Final thoughts\nWorking on this problem was instructive.\nPerhaps most telling is how the conclusion that a pure mathematical approach leads to—that there is always a precise solution— depends heavily on an assumption that’s difficult to realise in practice. There’s no such thing as a continuous distribution of population, so the idealised mathematics only plays out approximately in practice. Something to keep in mind in many problem settings. Even so, the mathematics is useful: it tells us that there should be a solution even if that seems unlikely given real world data.\nTo find that ‘precise’ solution, we’d have to do the dissections in a way that yielded continuously varying estimates of the populations either side of a line as it is infinitesimally shifted around. For example, we could use an areal interpolation method, assigning the proportion of population in a grid cell based on the fraction of its area on each side of the line. This seems likely to be quite a lot slower than my admittedly approximate approach.\nA further thought is that lurking in here are almost certainly the ingredients for a treemap/quadtree approach to cartograms. This is not an entirely new idea. For example in this paper\n\nSlingsby A, J Wood, and J Dykes. 2010. Treemap Cartography for showing Spatial and Temporal Traffic Patterns. Journal of Maps 6(1) 135-146. doi: 10.4113/jom.2010.1071\n\nspatialised treemaps are applied to visualize complex data.\nI am envisaging bisecting horizontally, then bisecting each half vertically, then each quarter horizontally, and so on. The resulting rectangular regions would be of equal population, and could be rescaled to be of equal size after each cut. Something like that. The idea definitely needs work, but I think it has potential.\nBut that’s another puzzle for another time."
  },
  {
    "objectID": "posts/2025-06-13-population-quadrants/index.html#footnotes",
    "href": "posts/2025-06-13-population-quadrants/index.html#footnotes",
    "title": "How to cut a cake into four equal slices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, that’s a thing.↩︎\nNotwithstanding the counterintuitive wonders of the ham sandwich theorem invoked by many of the math redditors.↩︎\nYes, it’s a word, I checked.↩︎\nBack in the day, this made a lot of sense as an overland shortcut between a harbour on the Pacific and another on the Tasman. Now as a place to concentrate your population it’s not so obviously a good choice.↩︎\nAt some angular resolution.↩︎\nI’d call it an agile approach if I were being kind. A little more seriously, I’d call it getting a feel for the problem before bothering to tackle it in full.↩︎\nI’m less certain this is a word.↩︎\nProbably… I can’t be absolutely certain of this. We’d have to look more closely to investigate which grid cells are ‘jumping around’ at these values to confirm this assumption.↩︎"
  },
  {
    "objectID": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "href": "posts/2022-08-03-spiral-folds/spiral-folds.html",
    "title": "Spiral origami",
    "section": "",
    "text": "I made an observable notebook to generate crease patterns for the flat-foldable origami ‘whirpool spirals’ designed by Tomoko Fuse and presented in chapter 3 of her amazing book Spiral: Origami|Art|Design. You’ll find more details in the notebook.\n\nThe picture is an example of the folds it produces, or at any rate of the things you can fold by cutting out and following the crease patterns it produces. You’ll have to do the folding yourself, I’m afraid.\nIf you’d like an idea of how the fold works without going to all the trouble of actually making one, then visit Amanda Ghassaei’s Origami Simulator and navigate to Examples - Tessellations - Whirlpool Spiral."
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html",
    "title": "Affine transformations of sf objects",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#packages",
    "title": "Affine transformations of sf objects",
    "section": "Packages",
    "text": "Packages\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(wk)\n\nsf::sf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#a-simple-square",
    "title": "Affine transformations of sf objects",
    "section": "A simple square",
    "text": "A simple square\nJust to get things set up let’s make a simple square.\n\nsquare &lt;- (st_polygon(list(matrix(c(-1, -1, 1, -1, 1, 1, -1, 1, -1, -1), \n                                 5, 2, byrow = TRUE))) * 0.5 + c(1, 0)) |&gt;\n  st_sfc()\n\ntm_shape(square) + \n  tm_borders(col = \"red\") + \n  tm_grid()"
  },
  {
    "objectID": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "href": "posts/2021-12-08-affine-transformation-of-sf-objects/affine-transformations-of-sf-objects.html#simple-transformations",
    "title": "Affine transformations of sf objects",
    "section": "Simple transformations",
    "text": "Simple transformations\nIn the code above, we made a polygon and multipled it by 0.5, then added c(1,0) to it. This had the effect of scaling it by 0.5 andthen translating it by the vector \\[\\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\]\nThese unlikely looking operations are perfectly valid, although they feel a bit ‘off’.\nEven more unlikely is that you can multiply an sf object by a matrix…\n\nang &lt;- pi / 6\nmat &lt;- matrix(c(cos(ang), -sin(ang), \n                sin(ang),  cos(ang)), 2, 2, byrow = TRUE)\n(square * mat) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThis is very handy… but probably also a bad idea! Because you have to post-multiply by the matrix, the sense of many affine transformations is reversed and construction of the matrix is not ‘by the book’. Usually the affine transformation matrix \\(\\mathbf{A}\\) for an anti-clockwise rotation by angle \\(\\theta\\) around the origin, would be\n\\[\n\\mathbf{A} =\n\\left[\\begin{array}{cc}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{array}\\right]\n\\]\nHere, because we are post-multiplying the rotation will be in the other direction… and to rotate anti-clockwise, you use the \\(-\\mathbf{A}=\\mathbf{A}^T\\)\n\\[\n-\\mathbf{A} =\n\\left[\\begin{array}{cc}\n-\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & -\\cos\\theta\n\\end{array}\\right] =\n\\left[\\begin{array}{cc}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{array}\\right] = \\mathbf{A}^\\mathrm{T}\n\\]\nThis means that if you are doing any serious affine transforming of sf shapes at a low-level in R spatial, I recommend either writing some wrapper functions that generate and apply the necessary matrices on the fly, or, probably better yet, using the wk package which has proper support for affine transformations."
  },
  {
    "objectID": "posts/2025-07-07-projections-explorer/index.html",
    "href": "posts/2025-07-07-projections-explorer/index.html",
    "title": "A simple atlas of projections",
    "section": "",
    "text": "As I’ve posted previously (see here, here, and here) Marimo is a promising new kind of reactive Python notebook. There’s a Quarto extension for it that makes it possible to embed reactive notebook interactivity into a blog post like this one.\nBy the time you have finished reading that paragraph, I hope the interactive bits of this post will have appeared below… if not, then just hang on a little longer.\n[Unfortunately, Marimo doesn’t prevent programmers from making mistakes, and the original version of this post was a bit sloppy in how it generated the circles in Tissot’s indicatrix. I’ve now corrected that issue. The Marimo extension for Quarto makes some odd design choices that account for some of the ugliness in the page below, although on the plus side it has made integrating an interactive notebook in to a blog post very easy, so I’ll take it.]\n\n    \n    \n    \n    get_plot()\n\nPick a new projection and marvel as the map of the world changes!\n\n    \n    \n    \n    \n\n \nThere’s not really very much more to say than that. The code driving this interaction is all pure python, but compiled to WASM so that it runs in the client on a browser.\nThe code that makes the UI elements like the drop-down and buttons is part of the marimo API, so for example, the code to make the drop-down and buttons looks like this:\n\n    \n    \n    \n    \n\n \nEach of these is a mo.ui.&lt;something&gt; element (mo is the recommended marimo import alias) and they are embedded in markdown to provide labels and so on.\nEverything else is standard Python whether base python or whatever modules you need to do the analysis and display the results your page is showing. Here’s the get_plot() function invoked above to plot the projected world map. You can see it gets the values of a dropdown called proj and two switches show_graticule and show_indicatrix to control the plot output.\n\n    \n    \n    \n    \n\n \nThe other thing to notice here is that due to marimo’s reactive nature, all this python code is ‘out of sequence’. The call to get_plot() comes before the function’s definition. For that matter, all the code to generate the graticule and so on is still to come:\n\n    \n    \n    \n    \n\n \nThis kind of thing is what drives me nuts any time I try to figure out what’s going on with javascript code, but somehow it seems kind of obvious in this setting.\nAnyway, here, finally, is the code to provide the drop-down with its list of projections.\n\n    \n    \n    \n    \n\n \nActually… not quite. The next cell defines the data layers and the very last cell imports all the Python modules that everything runs on!\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\n \nThe clever thing that marimo does is to impose a couple of minor restrictions on your Python code which makes it (relatively!) easy to infer a directed acyclic graph(DAG) of the dependencies between cells, so that it can decide which cells to re-run when something updates. Unfortunately, you can’t really see that in this post. That kind of meta-revelation is beyond my skillset. But you can get a feel for it by looking at the python source for a version of this page here.\nEach cell in a notebook is wrapped in a function and added to an app object by the @app.cell decorator. You can only define any variable in a notebook in a single cell. This can be awkward making throwaway variable names like i, j, k, x, and y unuseable, so marimo considers any variable that starts with a _ limited to the scope of the cell it is used in (so you can use _i, _j and so on for those convenience variables anywhere you like).\nAll the ‘public’ variables with plain names in your notebook are considered return values of the cells (wrapped as functions) that define them, and as parameters of the cells (wrapped as functions) that consume them. This makes building the dependency graph fairly easy and allows marimo to do its reactive thing without you having to think too hard about ‘callbacks’ and all the other stuff you probably associate with trying to code reactively in other settings.\nHere’a screenshot of the DAG for an earlier version of the code on this page.\n\nAnyway, regardless of how it works, it just does and one outcome of that is the potential for web interactivity baked into blog posts like this one!"
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html",
    "href": "posts/2024-10-17-duckdb/duckdb.html",
    "title": "The joy of DuckDB",
    "section": "",
    "text": "Just recently as part of this project I finally got around to putting some properly big data into an actual database and OMG! For the kind of situation I was in, I can’t recommend giving this a try enough."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "href": "posts/2024-10-17-duckdb/duckdb.html#some-background",
    "title": "The joy of DuckDB",
    "section": "Some background",
    "text": "Some background\nThe project in question as one component involves developing or at least exploring building :species distribution models for a large number of the bird species present in Aotearoa New Zealand. To that end we’ve obtained the latest eBird data collected for the New Zealand Bird Atlas. This is a phenomenal resource which includes the accumulated observations of thousands of citizen science volunteers, accumulated over several years.\nThe raw .txt file containing the observational data is a chunky 2.8GB with 7 million rows of data. Seven million rows isn’t so bad, right? Right, it really isn’t that bad. There is even an R package (of course there is), cheekily called auk1, for massaging the raw data down to the data you actually want.\nIn my case relevant data pertain only to the NZ Bird Atlas effort, and to complete checklists as only these provide the absence data required for occupancy modelling. The complexity of the data makes running auk to filter the raw data down slow, but it’s a one-time-only operation, so that’s OK, and now I have a 3.7 million row table, and we’re in business, no need to worry about setting anything up.\nThe problem comes when I have to blow those 3.7 million rows back up again for occupancy modelling to 110 million rows."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "href": "posts/2024-10-17-duckdb/duckdb.html#wait-what-110-million-rows",
    "title": "The joy of DuckDB",
    "section": "Wait, what? 110 million rows?!",
    "text": "Wait, what? 110 million rows?!\nYes, 110 million rows.\nHow it works is that each species (a little over 300) requires an entry in each complete checklist (around 360,000 of these) recording whether that species was observed (present) or not (absent) in that checklist. That results in 110 million row table. You can keep the two tables separate but then you have to keep joining them every time you go to use them. And if you save the 110 million table to disk it takes up around 8GB, and also takes a noticeable length of time to open and close for analysis in R.\nIt was about this time that I thought I should consider my options."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "href": "posts/2024-10-17-duckdb/duckdb.html#enter-duckdb",
    "title": "The joy of DuckDB",
    "section": "Enter DuckDB",
    "text": "Enter DuckDB\nDuckDB is an easy to install columnar database that you can drive using :SQL. It’s particularly easy to install because the R and Python APIs come bundled with the database itself. So, if you don’t want to, you don’t even have to install it, your platform of choice will do that for you.\nAnyway, if you do install it, which I did, so I could poke it around a little before going further, then to start it up from the command line type\n% duckdb\nAnd if you want to create a new database in the folder you are running from then it’s\n% duckdb my-new-database.db\nThe file extension is optional. Once in the session you can stash an existing CSV file in the database as a table with the command (D is the DuckDB command line prompt):\nD CREATE TABLE letters AS FROM 'letters.csv';\nand to see the results of your handiwork:\nD SELECT * FROM letters;\n┌─────────┬───────┬─────────┐\n│ column0 │  id   │ letter  │\n│  int64  │ int64 │ varchar │\n├─────────┼───────┼─────────┤\n│       1 │     1 │ a       │\n│       2 │     2 │ b       │\n│       3 │     3 │ c       │\n│       4 │     4 │ d       │\n│       5 │     5 │ e       │\n│       6 │     6 │ f       │\n│       7 │     7 │ g       │\n│       8 │     8 │ h       │\n│       9 │     9 │ i       │\n│      10 │    10 │ j       │\n├─────────┴───────┴─────────┤\n│ 10 rows         3 columns │\n└───────────────────────────┘\nD \nSatisfied it was this easy, I typed .exit to shut DuckDB down and moved on to consider how to use DuckDB from R. I should mention at this point that I’ve bounced off PostgreSQL a couple of times in the past when considering using it in classroom situations because it’s just not as easy to get into as this."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "href": "posts/2024-10-17-duckdb/duckdb.html#duckdb-in-r",
    "title": "The joy of DuckDB",
    "section": "DuckDB in R",
    "text": "DuckDB in R\nThe R package you need is duckdb, so\n\ninstall.packages(\"duckdb\")\nlibrary(duckdb)\n\nand you are ready to go (no other installation of DuckDB required).\nNow if you have a giant dataframe called say my_giant_df, that you need to deal with, open a connection to a new database (or an existing one if you’ve been here before) with\n\ncon &lt;- dbConnect(duckdb(), \"my-giant-dataframe.db\")\n\nand write your dataframe into it as a table called giant_df with\n\ndbWriteTable(con, \"giant_df\", my_giant_df)\n\nIf that’s all you plan on doing then you should shut down the connection\n\ndbDisconnect(con, shutdown = TRUE)\n\nWhen I did this I was agreeably surprised to find that my 8GB file had shrunk down to a mere 750MB.\nBut there’s more. The reason I went down this route at all is that I generally only want to work with the data for one bird species at a time — data which come in handy packets of only 360,000 rows or so. Here’s how that works in practice. First open a connection to the database\n\ncon &lt;- dbConnect(duckdb(), dbdir = str_glue(\"the-birds.db\"))\n\nThe database has a table called observations containing the aforementioned 110 million rows. Each row includes among other things the common_name of a bird. We can get a vector containing those using\n\ncommon_names &lt;- dbGetQuery(con, \"SELECT DISTINCT common_name FROM observations\") |&gt;\n  pull(common_name) |&gt;\n  sort()\n\nNow we can iterate over each species by doing\n\nfor (common_name in common_names) {\n  sql_name &lt;- str_replace_all(common_name, \"'\", \"''\")\n  query &lt;- str_glue(str_glue(\"SELECT * FROM observations WHERE common_name = '{sql_name}'\"))\n  this_bird_df &lt;- dbGetQuery(con, query)\n  # ...\n  # do stuff with this_bird_df\n  # ...\n}\n\nThe only wrinkles here, for those paying attention, are using str_glue from the stringr package to form the SQL query I need, and related to that a str_replace_all to double up any single-quotes ' that happen to appear in those common names to '' so that they can be passed into an SQL query.\nIn general you query the database using dbGetQuery(&lt;connection&gt;, &lt;SQL&gt;) and you can execute a command with dbExecute(con, &lt;command&gt;).\nOf course, you have to know a bit of SQL, but for this kind of simple (local) data warehousing, there’s nothing you are likely to need that a quick google DuckDuckGo search won’t unearth.\nThis approach enabled me to iterate over all 300 species in the data and assemble a ‘mini-atlas’ of ggplot maps of each bird’s range in under a minute (snippet below), which is about how long it was previously taking R just to open the 8GB CSV file. Not to mention that the giant data table is never in working memory, only the chunks I need one at a time.\n\n\n\nYes, of course, Mallard is there for a reason\n\n\nIt’s safe to say, I’ll be using DuckDB a lot in many projects to come. There seem to be some wrinkles in relation to handling spatial data, specifically from R’s sf package but there’s a package for that,2 and it’s nothing to get too alarmed about."
  },
  {
    "objectID": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "href": "posts/2024-10-17-duckdb/duckdb.html#footnotes",
    "title": "The joy of DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA very nerdy deep cut from the Cornell Ornithology Lab. Respect.↩︎\n‘We have a package for that’ should be R’s tagline.↩︎"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "",
    "text": "I wrote a paper waaaay back in 20041 which among other things was a plea for qualitatively inclined geographers to take complexity science seriously, as a sincere attempt to understand the world as it is, in all its, uh… complexity. It’s a tricky argument to make because the preferred tools of complexity science are computational or mathematical models, which are necessarily simplifications. Models, after all, are only useful if we can make sense of them and that’s only likely to be possible if they simplify the complex realities we are trying to understand.2\nThe paper has been widely cited, but a glance at where it has been cited suggests that my its readership has mostly been already sympathetic fellow-travellers in quantitative geography, not the more diverse audience I was hoping for, from across the discipline and beyond. While writing the paper I sent a draft to :Doreen Massey who was enthusiastic. The paper is framed as a response to her calls around that time for more dialogue across the divide between human and physical geography. The emergence of critical physical geography3 suggests that Massey’s call has not gone unheard.\nIt would be absurd to suggest that my paper has had even a fraction of that impact. Equally, it would be wrong to think that geography as a discipline has enthusiastically embraced the more holistic and open attitude to methods that taking seriously Massey’s call (or my paper) would entail.\nBut wait!"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#systems-thinking-spotted-in-the-wild",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Systems thinking spotted in the wild",
    "text": "Systems thinking spotted in the wild\nA workshop I was at last week suggests that maybe (just maybe) under the guise of systems thinking, complexity science might be starting to have an impact on more qualitative approaches.\nThe workshop was a gathering of some of the many researchers involved in the Moving the Middle (MtM) project down in Christchurch, ahead of the Agents of Change team’s Know Your Place environment + art event in Lyttelton Harbour. One of the presentations to the assembled team on the first morning of the workshop took me by (pleasant) surprise, as it was on :systems thinking. And one of the first things to be put on the screen was some version of the diagram below.\n\n\n\n\n\n\nFigure 1: A particularly complicated systems diagram—which is nevertheless a simplification4\n\n\n\nThis was part of an enjoyable talk by Nick Cradock-Henry summarising the elements of systems thinking, which the Agents of Change team have really picked up and run with in the last year as a framework for organising their thinking around the wider somewhat disparate MtM project.\nAs a long-time fan of :complexity science, which I consider to be either an evolution from systems thinking or a broader framework within which systems thinking sits,5 this was a big deal. I’ve wanted for years to see qualitatively inclined social scientists—the kind that talk about more-than-human geographies and such—to whole-heartedly engage with systems thinking!\nI’m not really sure what’s brought this on. Maybe it’s recent excitement about the :circular economy? Or a delayed reaction to COVID models? (Unlikely) Or maybe :Te Pūnaha Matatini is getting some traction with a wider audience? Whatever the reason, I’m glad it’s happening."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#making-models-and-having-conversations",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Making models and having conversations",
    "text": "Making models and having conversations\nThe primary tool of systems thinking the Agents of Change team seem to have been working with so far is causal loop diagrams (see Figure 1), which were ably explained both by Nick Cradock-Henry and by Justin Connolly. What this take on complexity/systems thinking brings to the fore is the importance of creating models (in this case primarily a visual model) collaboratively as things to talk about in a constructive way as groups of people try to understand some problem at hand.\nThe point of such models is not simulation or prediction, but understanding. Not even understanding necessarily, but arriving at sufficient agreement in a particular problem solving context about what makes things tick, so that useful conversations can be had.\nIf visual models eventually form a starting point for building system dynamics models, or agent-based models, or other kinds of computational models, that’s fine. But it’s also fine if that doesn’t happen. In fact, computational models can muddy the waters. They are expected or required to be predictive, and everyone becomes fixated on prediction and stops thinking. Or the model becomes a fall-guy (‘the model told me to do it!’). Or models are dragged into a role in monitoring and management that they weren’t designed for.6 When an organisation invests in building a simulation model, the chance of it being drafted into use for purposes well beyond its original scope are high, often with unintended consequences.\nOnce a computational model exists, there’s a danger of thinking that the topic at hand is now well enough understood that we have everything under control. But often it’s not really the model as end-product that is important, it’s the focus for conversation and discussion provided by collaboratively creating a model (informed by complexity/systems thinking that’s the important step), and that’s true whether the model is visual, computational, or even statistical."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#postscript-uncomplex-thinking-and-big-data-analytics",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Postscript: uncomplex thinking and big data analytics",
    "text": "Postscript: uncomplex thinking and big data analytics\nAnother reason I am happy to see a new audience get excited about the complexity/systems thinking approach to model-building is because there is far too much excitement about a very linear approach to model-building these days, in the multi-headed shape of big data analytics, machine-learning, and AI.7\nIt’s not that these methods aren’t useful. Of course they are! The problem is that they too often skip the collaborative model-building, or leave the model-building to machines, so that the most important opportunity for learning is lost. That’s not quite correct. The analyst developing such models often learns a lot in the process. The problem is that the point of the exercise is often not the learning along the way, but the final model that results, and once that’s done the assumption is that now we understand, and can predict and manage the problem at hand. And of course, for as long as the world continues to be open, interconnected, and processual (i.e., forever), that kind of model will inevitably be wrong if not today, then some day very soon.8"
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#looking-ahead",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe qualitative social scientists I was hanging out with last week are rightly skeptical of such uncomplex models. Their enthusiasm for complexity/systems thinking models on the other hand reflects how such approaches really do give us a better chance of getting a handle on the world. I really hope their enthusiasm isn’t a one-off but a harbinger of many more fruitful conversations ahead, across what have often seemed unbridgeable divides."
  },
  {
    "objectID": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "href": "posts/2024-11-27-systems-thinking/systems-thinking.html#footnotes",
    "title": "A welcome (re)emergence of systems thinking",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. It’s paywalled, but get in touch if you’d like a copy.↩︎\nI’ll note here that models “are never true, but fortunately it is only necessary that they be useful”, p.2 in Box GP. 1979. Some problems of statistics and everyday life. Journal of the American Statistical Association 74(365) 1-4. Paywalled, I’m afraid.↩︎\nThe original paper on this is Lave R et al. 2014. Intervention: Critical physical geography. The Canadian Geographer / Le Géographe canadien 58(1) 1–10. But that is paywalled. An open access paper which gives a sense of what critical physical geography aims to accomplish is Lave R. 2015. Engaging within the Academy: A Call for Critical Physical Geography. ACME: An International Journal for Critical Geographies 13(4) 508-515.↩︎\nFigure from Monat JP and TF Gannon. 2015. Using systems thinking to analyze ISIS. American Journal of Systems Science 4(2) 36–49.↩︎\nDepending on my mood, I can go either way, but if you need convincing of the links, see Merali Y and PM Allen. 2011. Complexity and Systems Thinking, 31-52 in The Sage Handbook of Complexity and Management PM Allen, S Maguire, and B McKelvey (eds) Sage.↩︎\nYes, I’m looking at you, Overseer.↩︎\nThis is something I’ve written about before: O’Sullivan D 2018. Big data … why (oh why?) this computational social science? In Thinking Big Data in Geography: New Regimes, New Research eds. JE Thatcher, J Eckert, and A Shears, 21–38. University of Nebraska Press. Again, paywalled: let me know if you’re interested to read a copy.↩︎\nIronically, models are most urgently needed for prediction precisely when prediction is difficult or even impossible.↩︎"
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "",
    "text": "I’ve just spent much of a not especially nice Saturday (weather-wise) tidying up some loose ends on the Computing Geographically website. In particular, I’ve been updating the R code snippets that make many of the figures. I started out intending to update the tmap v3 code to v4, since the latter is now recommended by the developer. It has, for example, been adopted in the latest edition of the excellent Geocomputation with R.\nA little way into the process, I realised that since doing last year’s 30 Day Map Challenge (see my efforts here) I use ggplot2 a lot more for my everyday mapping work than I do tmap. That’s in spite of having taught tmap for several years, a choice I made because its learning curve is less steep than ggplot2’s. Anyway, long story short, migrating the code on my book website from tmap v3 to v4 turned into more of a mixed bag: migrating some code to tmap v4, and some to ggplot2.\nIn this post I discuss some things to consider if you are choosing which of these two excellent packages—that’s important: they are both excellent packages—to use in various situations."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#same-only-different",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Same only different",
    "text": "Same only different\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tmaptools)\nlibrary(tmap)\nlibrary(htmlwidgets)\nlibrary(cols4all)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(dplyr)\n\n\nWhat the packages have in common is that they implement the idea of a grammar of graphics (hence ggplot2’s name) where we progressively add layers to a graphic, specifying for each layer how data attributes are scaled to give values of :visual variables (colour, line width, line style, symbol size and so on). There’s a graphic (in German) showing this idea here.\nSo the packages are similar at a high-level. However, I’m not going to delve into details of the differences between these packages. For that, you should explore the extensive online resources available. The best places to start are hard to pick. Maybe here for tmap and here for geospatial stuff in ggplot2. Just be aware that tmap is undergoing a major upheaval as it transitions to v4 and while older code should still work, you’ll see a lot of warnings. Meanwhile, ggplot2 has a habit of changing things without preserving backwards compatibility, so it’s advisable to be wary of any code snippets more than 3 or 4 years old when you are looking for help.\nRather than the details, I want to explore the packages ‘in use’ by looking at four broad aspects of mapping that speak to the advantages of a package specifically designed for mapping (tmap) over a more general purpose visualization tool (ggplot2), which nevertheless holds it own. Those four things are choropleth maps, raster data, web maps, and ‘map junk’ (north arrows and the like).\n\nChoropleth maps: tmap’s killer app\nMaking ‘proper’ choropleth maps in ggplot2 is no fun at all. See my Day 13 experience in the 2023 30 Day Map Challenge. The difficulty is that the central tenet of classified choropleth mapping is controlling how you relate data to colour fills. It’s not that ggplot2 won’t allow you fine control over this aspect of your choropleth map, it’s just that it really, really wants you to map your data linearly, and continuously to a colour ramp. That’s a reasonable design decision for a general scientific visualization tool. It’s just not what cartographers do in choropleth mapping.\nTo illustrate, here’s an old dataset I often use: TB cases in Auckland City in 2006 by Census Area Unit:\n\nak &lt;- st_read(\"ak-tb.gpkg\")\n\nAnd here’s the most basic ggplot2 choropleth map of the TB_RATE variable (which is cases per 100,000 population):\n\nggplot() +\n1  geom_sf(data = ak, aes(fill = TB_RATE))\n\n\n1\n\naes(fill = ...) specifies which variable maps to the colour fill\n\n\n\n\n\n\n\n\n\n\n\nA few things stand out here (to my eye at least):\n\nyou get a graticule in latitude-longitude even if the data are in a projected coordinate system (to be clear, the map is in the projected coordinates);\nthe default colour ramp goes from dark for low values to light for high values; and\nthe default colour ramp is black-to-blue, which might be preferable to the default muddy browns we’ve become accustomed to, but is an unusual choice for a map.\n\nOf course we can fix all these things:\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_continuous_c4a_seq(\n2    palette = \"brewer.reds\") +\n3  coord_sf(datum = st_crs(ak))\n\n\n1\n\nscale_fill_continuous_c4a_seq() is in the cols4all package which includes a very wide range of palettes\n\n2\n\nUse Brewer palette ‘Reds’\n\n3\n\nGet the coordinate system of the data using st_crs() and apply to the coordinate frame using coord_sf()\n\n\n\n\n\n\n\n\n\n\n\nNote that I am using a cols4all scale, because this is the preferred colour palettes package for tmap, and provides a wide array of options. For casual mapping, we likely don’t care about the grid, and we can get rid of that too using theme_void():\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"brewer.reds\") +\n  theme_void()\n\n\n\n\n\n\n\n\nSomething like the above, which is three lines of code after the introductory ggplot() call, is what I use most of the time. And the map is good enough, especially if it’s temporary and a stepping stone on the way to some other analysis.\nWhat does the same map look like in tmap v4?\n\ntm_shape(ak) +\n  tm_polygons(\n1    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(\n2      values = \"brewer.reds\")) +\n  tm_layout(\n    frame = FALSE, \n3    legend.frame = FALSE)\n\n\n1\n\nVariable name for fill in quotes\n\n2\n\nBrewer ‘Reds’ again\n\n3\n\nRemove annoying frames around map and legend\n\n\n\n\n\n\n\n\n\n\n\nEssentially the same map, up to minor aesthetic details. These are easily tweaked in either package, so we won’t worry about them too much here.\nWhere tmap wins out is if you want to experiment with classic choropleth map classification schemes, for example:\n\nm1 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\")) +\n1  tm_layout(title = \"Pretty\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\nm2 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n2      values = \"brewer.reds\", style = \"equal\", n = 6)) +\n  tm_layout(\n3    title = \"Equal intervals\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE) \n\nm3 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n4      values = \"brewer.reds\", style = \"quantile\", n = 6)) +\n  tm_layout(\n    title = \"Quantiles\", title.size = 0.8,\n    frame = FALSE, legend.frame = FALSE)\n\nm4 &lt;- tm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(\n5      values = \"brewer.reds\", style = \"sd\")) +\n  tm_layout(title = \"Std. Dev.\", title.size = 0.8,\n            frame = FALSE, legend.frame = FALSE)\n\ntmap_arrange(m1, m2, m3, m4, ncol = 2)\n\n\n1\n\nThe default classification style is \"pretty\", i.e. human-friendly round numbers\n\n2\n\nstyle = \"equal\" to get equal intervals\n\n3\n\nadd a title to show the classification style\n\n4\n\nstyle = \"quantile\" for quantiles\n\n5\n\nstyle = \"sd\" for standard deviation breaks\n\n\n\n\n\n\n\n\n\n\n\nForget the minor issue with aligning these maps exactly, the point here is that these are all classified choropleth maps, with the classification method specified by the style parameter (more options are available than shown here). This is an established way to make choropleth colours easier to parse, or put another way, to make it easier to highlight the specific features of the data you want readers to focus on.\nThe magic here is that behind the scenes tmap is using the classInt package, and if we want to make similar maps in ggplot2 we have to do that work ourselves:\n\nlibrary(classInt)\n\nclass_breaks &lt;- ak$TB_RATE |&gt;\n  classIntervals(6, \"quantile\", digits = 1)\nbrks &lt;- round(class_breaks$brks, 1)\n\nggplot() + \n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n1  scale_fill_binned_c4a_seq(\n    palette = \"brewer.reds\", breaks = brks) + \n  theme_void()\n\n\n1\n\nNote the change to a binned scale, controlled by the breaks\n\n\n\n\n\n\n\n\n\n\n\nThe above is the minimal ggplot2 version of a quantile map I can come up with. If you want the nice class labels then you have to make those labels yourself and do more work on the legend. You can see an example of this in my Day 13 map in the 2023 30 Day Map Challenge.\nOverall, if you are making a lot of classified choropleth maps then tmap is your friend.\n\n\nRaster data: tmap’s other killer app\ntmap is comfortable dealing with raster data. Here’s a simple example:\n\ndem &lt;- rast(\"dem.tif\")\n\ntm_shape(dem) + \n1  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_intervals(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n1\n\ntm_raster() ‘announces’ that these data are raster so that some visual variables might be interpreted differently, e.g. col is now what fill is on a polygon layer, where col is interpreted as linework or outline colour\n\n\n\n\n\n\n\n\n\n\n\nNice! Note that the same tm_scale_intervals() function is used here to specify colours in this case as it was to specify fill colours in the choropleth map case. We can also have a continuous colour ramp, and layer on top a hillshade. First make the hillshade using some terra functions:\n\nslope &lt;- dem |&gt; terrain(\"slope\", unit = \"radians\")\naspect &lt;- dem |&gt; terrain(\"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, \n                   angle = 35, direction = 135)\n\nThen just add it as another layer also using the col aesthetic with a semi-transparent grey palette.\n\ntm_shape(dem) + \n  tm_raster(\n    col = \"dem\",\n    col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n    col.legend = tm_legend(title = \"Elevation\")) +\n  tm_shape(hillshade) +\n  tm_raster(\n1    col = \"hillshade\",\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col_alpha = 0.35\n  ) +\n  tm_layout(\n    legend.frame = FALSE, legend.show = FALSE)\n\n\n1\n\ntmap is OK about adding a second layer using the col visual variable\n\n\n\n\n\n\n\n\n\n\n\nNicer! If I have one issue here it’s with using the parameter name values for the palette name, which takes a little bit of getting used to. Of course, the rationale for this is that the palette specifies where the colour visual variable is to get its values which in this context, are colours.\nggplot2 doesn’t really deal in rasters. We can make similar maps pretty easily, by converting to an x, y, attribute dataframe:\n\ndem_df &lt;- dem |&gt; as.data.frame(xy = TRUE)\nggplot(dem_df) +\n1  geom_raster(aes(x = x, y = y, fill = dem)) +\n  scale_fill_continuous_c4a_seq(\n    palette = \"hcl.terrain2\", name = \"Elevation\") +\n2  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(\n3    panel.border = element_rect(fill = NA, linewidth = 1))\n\n\n1\n\nNote that for ggplot2 the colours are still a fill here\n\n2\n\nexpand = FALSE prevents a margin around the data\n\n3\n\nggplot2’s theming system has a lot going on…\n\n\n\n\n\n\n\n\n\n\n\nYou can probably tell I’ve done a fair bit of this kind of thing: I didn’t just magic that expand = FALSE option and the panel.border thing out of nowhere.\nAnyway, while this approach is fine for a small raster like this one, even here the x-y dataframe equivalent to the raster has 35,000 or so rows, so this won’t scale well.\nAnd we haven’t even got into the headaches involved if you want to layer something else on top of a raster dataset where you need to use the fill aesthetic again, like say… a hillshade. Let’s just say the ggnewscale package is your friend. I leave that as an exercise for the reader…\nAgain… I think it’s clear that tmap is a better choice if you are making a lot of maps of raster data.\n\n\nWeb maps: tmap’s oth… OK, this is just getting silly\nWhen it comes to web maps ggplot2 doesn’t even try. tmap on the other hand has its ‘mode switch’ option. Set the mode to view and you are making web maps!\n\n1tmap_mode(\"view\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\", \n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"), \n    fill_alpha = 0.5)\n\n\n1\n\nNow you’re making web maps! Use tmap_mode(\"plot\") to switch back\n\n\n\n\n\n\n\n\n\n \nSo, yeah, if you are making bona fide web maps, tmap every time. Switch back to plot mode, and tmap also does a good job of allowing you to use a web map as a basemap layer:\n\ntmap_mode(\"plot\")\ntm_shape(ak) +\n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_intervals(values = \"brewer.reds\"),\n    fill_alpha = 0.5) +\n1  tm_basemap(server = \"OpenStreetMap\") +\n  tm_layout(frame = FALSE, legend.frame = FALSE)\n\n\n1\n\nAdds a static web map as a basemap\n\n\n\n\n\n\n\n\n\n\n\nggplot2 can get the same effect using the ggspatial::annotation_maptile() function (you will also need to have the prettymapr package installed).\n\nggplot() + \n1  annotation_map_tile(zoomin = 1) +\n  geom_sf(data = ak, aes(fill = TB_RATE), alpha = 0.5) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  theme_void()\n\n\n1\n\nannotation_map_tile() is a function from ggspatial\n\n\n\n\n\n\n\n\n\n\n\nOverall though, another win for tmap.\n\n\nMap junk: whatever\nAs you might guess from the title of this section, I am not much concerned with north arrows and scalebars and such-like. tmap has them built in.\n\ntm_shape(ak) + \n  tm_polygons(\n    fill = \"TB_RATE\",\n    fill.scale = tm_scale_continuous(values = \"brewer.reds\")) +\n  tm_compass() +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.frame = FALSE)\n\n\n\n\n\n\n\n\nTo get the same things in ggplot2 you need the ggspatial package, and they’re perfectly serviceable. I am about as excited about this as I sound.\n\nggplot() +\n  geom_sf(data = ak, aes(fill = TB_RATE)) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n1  annotation_north_arrow(location = \"br\") +\n2  annotation_scale(location = \"bl\") +\n  theme_void() + \n  theme(panel.border = element_rect(fill = NA))\n\n\n1\n\nAnother ggspatial function\n\n2\n\nAnd another\n\n\n\n\n\n\n\n\n\n\n\nNeedless to say, both tmap and ggplot2 offer lots of flexibility for adding titles and subtitles and positioning all these elements around the map wherever you want them.\nAll in all, this one is probably a tie, assuming you’ve installed ggspatial."
  },
  {
    "objectID": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "href": "posts/2024-11-16-tmap-vs-ggplot/tmap4-vs-ggplot2.html#final-thoughts",
    "title": "tmap vs. ggplot2 for mapping",
    "section": "Final thoughts",
    "text": "Final thoughts\nIn brief then: tmap is better at:\n\nclassified choropleth maps;\ndealing with geospatial rasters; and\nweb maps,\n\nand about equal on\n\nweb map derived basemaps; and\nmap junk;\n\nThe latter two assuming that you have installed ggspatial. The funny thing is, I still find myself using ggplot2 more! For me the counterpoints to the above are:\n\nunclassified choropleths are often fine for a quick look-see at your data;\nI don’t make many raster-based maps, and ggplot2::geom_tile is often good enough for my purposes;\nWhere I previously used web maps in tmap to get an overview of my data, now I tend to use QGIS. If I really need a web map, I will certainly use tmap; and\nI am not a fan of map junk (hence: junk). Don’t get me wrong, I’ll but a north arrow and scalebar on a map if necessary, I just don’t make many maps like that.\n\nIf your mix of use-cases is different, especially if choropleth maps, raster data, and web maps loom larger in your world than they do in mine, then you might wind up making a different choice.\nI think the reason I end up gravitating to ggplot2 is that it is the entry point into a much wider visualization world. The idiom it has popularised, which tmap has reworked for mapping, of layering a series of aesthetics each linking a data variable to a visual variable (that so-called grammar of graphics) is more cleanly implemented in ggplot2, as you’d expect it to be. And the exact same semantics apply to scatterplots, boxplots, violin plots, histograms, bar charts, and so on. The ggplot2 ecosystem is sprawling and extremely powerful, and making maps is just one small corner of it.\nBecause of that, I am always making charts using ggplot(data) + geom_*() and it feels very natural to make maps the same way.\nHaving said that, for R coded maps that cover all the mapping bases, you should probably also get to grips with tmap. I use both regularly, even if I do use ggplot2 a little more regularly!"
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html",
    "href": "posts/2025-06-27-iata-codes/index.html",
    "title": "What three letters",
    "section": "",
    "text": "Last year some time I found myself on a flight from Dubai (DXB) to Dublin (DUB) and of course my pattern detecting brain noted the off-by-one-letter difference in the International Air Transport Association (IATA) codes of the two airports. It got in my head at the time and between watching bad movies and dozing and wondering if the flight would ever end (it was the third leg of a WLG - AKL - DXB - DUB endurance test) I found myself idly wondering, just how many off-by-one-letter flights there are. Are they rare or commonplace? Where are they? How many could there possibly be?\nSo, for all those similarly afflicted by such questions, here’s a post with more than you ever wanted to know about this deeply trivial matter. As always, there are coding tricks to be learned along the way. The code I think is less interesting can also be viewed by clicking on the ▸ Code links."
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#libraries-and-data",
    "href": "posts/2025-06-27-iata-codes/index.html#libraries-and-data",
    "title": "What three letters",
    "section": "Libraries and data",
    "text": "Libraries and data\nThe most interesting libraries we need are geosphere, which does all manner of great circle calculations (it could use an update to more fully integrate with sf, but otherwise is pretty great) ; and stringdist, which does the needed calculation of differences between strings.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n1library(patchwork)\n2library(geosphere)\n3library(stringdist)\n4library(smoothr)\n\n\n\n1\n\nFor assembling multi-panel figures.\n\n2\n\nFor making great circle arcs.\n\n3\n\nFor determining similarities between strings.\n\n4\n\nFor densifying points along meridians to make a ‘globe’ polygon.\n\n\n\n\nWe need some base world data, and I’ve also made a ‘globe’ so we can colour in the background sensibly in a non-rectangular projection. I’ve chosen 30°E for the central meridian of my world maps because1 that allows all the off-by-one routes that exist to be shown without breaks.\n\n\nCode\n1c_meridian &lt;- 30\nprojection &lt;- str_glue(\"+proj=natearth lon_0={c_meridian}\")\n\nworld &lt;- spData::world\nworld_p &lt;- world |&gt; \n2  st_break_antimeridian(lon_0 = c_meridian) |&gt;\n  st_transform(projection)\n\nglobe &lt;- st_polygon(\n  list(matrix(c(c(-1, -1, 1,  1, -1) * 179.999 + c_meridian, \n                c(-1,  1, 1, -1, -1) * 90), ncol = 2))) |&gt;\n  st_sfc() |&gt;\n  as.data.frame() |&gt;\n  st_sf(crs = 4326) |&gt;\n3  densify(100)\nglobe_p &lt;- globe |&gt;\n  st_transform(projection)\n\n\n\n1\n\nUsing a central meridian 30°E gives a better centred final map.\n\n2\n\nBreak things based on a central meridian 30°E\n\n3\n\nFor rounded ‘edges’ to the globe when projected."
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#aviation-data",
    "href": "posts/2025-06-27-iata-codes/index.html#aviation-data",
    "title": "What three letters",
    "section": "Aviation data",
    "text": "Aviation data\nVia ourairports.com2 we can get pretty comprehensive data on airports, large and small. So much data in fact (some 83,210 airports) that I’ve limited the focus for this post to a mere 482 ‘large’ airports that also have an IATA code.\n\n\nCode\nairports &lt;- read.csv(\"https://davidmegginson.github.io/ourairports-data/airports.csv\") |&gt; \n  filter(type %in% c(\"large_airport\"), iata_code != \"\") |&gt;\n  select(name, iata_code, longitude_deg, latitude_deg) |&gt;\n  rename(x = longitude_deg, y = latitude_deg)"
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#all-possible-off-by-one-routes",
    "href": "posts/2025-06-27-iata-codes/index.html#all-possible-off-by-one-routes",
    "title": "What three letters",
    "section": "All possible off-by-one routes",
    "text": "All possible off-by-one routes\nFinding the best way to get all the two airport combinations proved trickier than I expected, at least it did for as long as I persisted with trying to do it ‘tidily’. I also took a detour via igraph which allows for a nice graph-based approach, round-tripping airport-pairs via a simple undirected graph. That’s appropriate in some respects, but really overkill for this problem. Knowing when to give up is half the battle, and switching to using base R’s combn is simple, give or take a bit of matrix conversion to get the results into a dataframe.\nAfter that it’s a simple matter to filter for routes with a Hamming distance between codes as calculated by stringdist of exactly one, and then join coordinate data for the two airports.\n\n\nCode\nroutes &lt;- combn(airports$iata_code, 2) |&gt; \n  matrix(ncol = 2, byrow = TRUE, \n         dimnames = list(NULL, paste(\"iata\", 1:2, sep = \"\"))) |&gt; \n  as.data.frame() |&gt;\n  filter(stringdist(iata1, iata2, method = \"hamming\") == 1) |&gt;\n  left_join(airports, by = join_by(iata1 == iata_code)) |&gt;\n  rename(name1 = name, x1 = x, y1 = y) |&gt;\n  left_join(airports, by = join_by(iata2 == iata_code)) |&gt;\n  rename(name2 = name, x2 = x, y2 = y) |&gt;\n  select(iata1, iata2, name1, name2, x1, y1, x2, y2)\n\n\n\nGreat circle routes\nWhile flights might very well not follow great circle routes in all cases, they are the most appropriate general way to represent them on a world map, which is where geosphere comes in.\nThe tricky part here was to break the great circle linestrings into multistrings at my chosen antimeridian, 150°W. While sf::st_break_antimeridian does this correctly, for reasons that I think are associated with the various format conversions from old style spatial data generated by geosphere to sf I wound up with duplicate copies of every flight that crosses the anti-meridian, in addition to a multilinestring broken at the meridian. Fixing this requires a couple of additional steps.\n\n\nCode\nroutes_sf &lt;- gcIntermediate(\n    routes |&gt; select(x1:y1), routes |&gt; select(x2:y2), \n    n = 100, addStartEnd = TRUE, breakAtDateLine = TRUE, sp = TRUE) |&gt;\n  st_as_sf() |&gt;\n  st_set_crs(4326) |&gt;\n  bind_cols(routes) |&gt;\n  st_break_antimeridian(lon_0 = c_meridian) |&gt;\n1  mutate(route = str_c(iata1, \"-\", iata2))\n\n2routes_multi &lt;- routes_sf |&gt;\n  group_by(route) |&gt;\n  summarise()\n\n3routes_sf &lt;- routes_sf |&gt;\n  st_drop_geometry() |&gt;\n  distinct(iata1, iata2, .keep_all = TRUE) |&gt;\n  left_join(routes_multi, by = join_by(route)) |&gt;\n  st_as_sf()\n\n\n\n1\n\nAdd a \"AKL-WLG\" style route attribute to help in identifying duplicate routes.\n\n2\n\nGroup on the route attribute and summarise to create multilinestrings. This is the retrospective fix for extra linestrings that showed up in the previous step.\n\n3\n\nJoin the multilinestrings back to the data."
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#a-map-of-all-the-potential-off-by-one-routes",
    "href": "posts/2025-06-27-iata-codes/index.html#a-map-of-all-the-potential-off-by-one-routes",
    "title": "What three letters",
    "section": "A map of all the potential off-by-one routes",
    "text": "A map of all the potential off-by-one routes\n\n\nCode\nggplot() +\n  geom_sf(data = globe_p, fill = \"#ddeeff\", colour = NA) +\n  geom_sf(data = world_p, fill = \"#dddddd\", colour = NA) +\n  geom_sf(data = routes_sf, lwd = 0.05, colour = \"#666666\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 1: All possible routes between major airports differing by one letter in their IATA codes.\n\n\n\n\n\nJust to emphasise: this is not a map of actually existing off-by-one routes, it’s a map of all the 782 possible such routes between our 482 large airports. That’s from a possible 116,402 routes, or about 0.67%. This is more than half as many again as we’d expect if the codes were random. There are up to 26-cubed or 17,576 codes. Any given code can connect to as many as 17,575 other codes, and of these only 75 can be off-by-one (there are 25 possible different letters in each of the three available positions). That’s 75 / 17,575, which is only 0.43%\nOf course the codes aren’t random, they generally bear some relation to the names of actual places3 and we’d therefore expect the distribution of letters in the codes to be uneven. We can confirm this very easily:\n\n\nCode\nletter_freqs = data.frame(\n1  Letter = LETTERS,\n  Count = airports |&gt; \n    pull(iata_code) |&gt;\n    str_c(collapse = \"\") |&gt;\n    str_count(LETTERS)\n)\nggplot(letter_freqs) +\n  geom_col(aes(x = Letter, y = Count), fill = \"darkgrey\") +\n  theme_minimal()\n\n\n\n1\n\nLETTERS and letters are vectors of the alphabet in upper and lower case. A handy thing to know.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Counts of letters in the IATA codes of 482 airports\n\n\n\n\n\nI checked for the 9000 or so airports that have codes and the pattern is not so different, with A and S still the most favoured letters."
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#what-about-actually-existing-routes",
    "href": "posts/2025-06-27-iata-codes/index.html#what-about-actually-existing-routes",
    "title": "What three letters",
    "section": "What about actually existing routes?",
    "text": "What about actually existing routes?\nAmong other reasons, actually existing routes are different from all possible ones because the longest scheduled flights max out at a little over 15,000km4\nSadly, it’s difficult to source open data on regularly scheduled flights, because it’s commercially valuable information. The best I could do was from openflights.org, but as they say “The third-party that OpenFlights uses for route data ceased providing updates in June 2014”.5 Anyway, we can filter out routes from our off-by-one dataset that don’t appear in this list.\n\n\nCode\nsched &lt;- read.csv(\n  \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\", \n  header = FALSE) |&gt;\n  select(3, 5) |&gt; \n  rename(iata1 = V3, iata2 = V5) |&gt;\n1  mutate(ab = str_c(iata1, \"-\", iata2),\n         ba = str_c(iata2, \"-\", iata1))\n\nroutes_sf_actual &lt;- routes_sf |&gt;\n  mutate(ab = str_c(iata1, \"-\", iata2)) |&gt;\n2  filter((ab %in% sched$ab) | (ab %in% sched$ba))\n\n\n\n1\n\nMake a route label in both directions since it might appear in either direction in the data.\n\n2\n\nCheck for both directions in the filter.\n\n\n\n\nAnd we can map those.\n\n\nCode\nggplot() +\n  geom_sf(data = globe_p, fill = \"#ddeeff\", colour = NA) +\n  geom_sf(data = world_p, fill = \"#dddddd\", colour = NA) +\n  geom_sf(data = routes_sf, lwd = 0.05, colour = \"#666666\") +\n  geom_sf(data = routes_sf_actual, lwd = 0.5, colour = \"red\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 3: Actually existing routes off-by-one in their IATA codes highlighted in red.\n\n\n\n\n\nThe longest of these routes is Melbourne (MEL) to Delhi (DEL) from where you could get an onward connection to Helsinki (HEL). Surprisingly New Zealand’s only entrant in this list is the relatively local Auckland (AKL) to Adelaide (ADL). Amusingly, the shortest off-by-one route at 324km is between King Abdulaziz International Airport (JED) and Prince Mohammad Bin Abdulaziz Airport (MED) both in Saudi Arabia, and keeping airport naming rights in the family.\n\nMore local maps\nIn lieu of a zoomable web map6 below are four more localised views.\n\n\nCode\nairport_labels &lt;- airports |&gt;\n  filter(iata_code %in% c(routes_sf_actual$iata1, \n                          routes_sf_actual$iata2)) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\"), crs = 4326)\n\nget_proj_string &lt;- function(bounds) {\n  lon_0 &lt;- (bounds[1] + bounds[3]) / 2\n  lats &lt;- quantile(bounds[c(2, 4)], c(0.25, 0.5, 0.75))\n  lat_1 &lt;- lats[1]\n1  lat_0 &lt;- lats[2]\n  lat_2 &lt;- lats[3]\n  str_glue(\"+proj=aea +lat_1={lat_1} +lat_2={lat_2} +lon_0={lon_0}\")\n}\n\nget_ll_bbox &lt;- function(bounds) {\n  st_polygon(list(\n    matrix(c(bounds[1], bounds[1], bounds[3], bounds[3], bounds[1],\n             bounds[2], bounds[4], bounds[4], bounds[2], bounds[2]), ncol = 2))) |&gt;\n    st_sfc(crs = 4326) |&gt; \n    as.data.frame() |&gt;\n    st_sf() |&gt;\n    densify(100)  \n}\n\nget_ll_limited_gdf &lt;- function(gdf, ll_bbox, crs) {\n  gdf |&gt; \n    st_filter(ll_bbox, \n              .predicate = st_is_within_distance, \n2              dist = 1e6) |&gt;\n    st_transform(crs) |&gt;\n    st_make_valid()\n}\n\nget_regional_map &lt;- function(limits) {\n  proj     &lt;- get_proj_string(limits$lims)\n  bb       &lt;- get_ll_bbox(limits$lims)\n  world_p  &lt;- get_ll_limited_gdf(world, bb, proj)\n  routes_p &lt;- routes_sf |&gt; st_transform(proj)\n  actual_p &lt;- routes_sf_actual |&gt; st_transform(proj)\n  labels_p &lt;- get_ll_limited_gdf(airport_labels, bb, proj)\n  plot_lims &lt;- bb |&gt;\n    st_transform(proj) |&gt; \n    st_bbox()\n3  ggplotGrob(\n    ggplot() +\n      geom_sf(data = world_p, fill = \"#d0d0d0\", colour = \"white\", lwd = 0.25) +\n      geom_sf(data = routes_p, lwd = 0.04, colour = \"#666666\") +\n      geom_sf(data = actual_p, lwd = 0.35, colour = \"red\") +\n      geom_sf_label(data = labels_p, aes(label = iata_code), size = 1.5) +\n      coord_sf(xlim = plot_lims[c(1, 3)], \n               ylim = plot_lims[c(2, 4)], expand = FALSE) +\n      ggtitle(limits$region) +\n      theme_void() +\n      theme(panel.border = element_rect(fill = NA, linewidth = 0.5),\n            plot.title = element_text(size = 10))\n    )\n}\n\nregional_limits &lt;- list(\n  list(region = \"North America\", lims = c(-115, 15, -65, 55)),\n  list(region = \"Europe\",        lims = c( -15, 30,  45, 70)),\n  list(region = \"Middle East\",   lims = c(  30, 10,  80, 50)),\n  list(region = \"East Asia\",     lims = c(  80,  0, 130, 40))\n)\n\nwrap_plots(lapply(regional_limits, get_regional_map))\n\n\n\n1\n\nNo lat_0 is required for this projection, but might be if I change it some time.\n\n2\n\nIt’s necessary to ‘cast the net wide’ to make sure all countries within a window in lat-lon are on the map in the projected space, which is quite different than the lat-lon bounding box.\n\n3\n\nReturning plots as plots caused me some memory issues, which seem to be resolved by returning them as ‘grobs’. There’s a first time for everything in my R journey.\n\n\n\n\n\n\n\n\n\n\nFigure 4: Regionally focused maps with IATA codes shown."
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#finally-are-the-off-by-one-routes-clustered",
    "href": "posts/2025-06-27-iata-codes/index.html#finally-are-the-off-by-one-routes-clustered",
    "title": "What three letters",
    "section": "Finally: are the off-by-one routes clustered?",
    "text": "Finally: are the off-by-one routes clustered?\nIt’s not easy to know exactly how to approach this question. But we might expect, due to geographical patterns in naming and language, that there would be some clustering of the actual off-by-one routes. A rough and ready approach to testing this idea is applied in the code cell below, based on the lengths in km of the various subsets of flights. The idea is that shorter length routes might be more common among off-by-one routes than in scheduled flights in general.\n\n\nCode\nrandom_xy &lt;- world |&gt; \n  filter(continent != \"Antarctica\") |&gt;\n  st_sample(250) |&gt;\n  st_coordinates()\npairs &lt;- combn(1:250, 2) |&gt; t()\n\nsched_xy &lt;- sched |&gt;\n  inner_join(airports, by = join_by(iata1 == iata_code)) |&gt;\n  rename(name1 = name, x1 = x, y1 = y) |&gt;\n  inner_join(airports, by = join_by(iata2 == iata_code)) |&gt;\n  rename(name2 = name, x2 = x, y2 = y)\n\ndf &lt;- list(\n  random = distGeo(random_xy[pairs[, 1], ], random_xy[pairs[, 2], ]),\n  scheduled = distGeo(sched_xy |&gt; select(x1, y1),\n                      sched_xy |&gt; select(x2, y2)),\n  possible = distGeo(routes_sf |&gt; st_drop_geometry() |&gt; select(x1, y1),\n                     routes_sf |&gt; st_drop_geometry() |&gt; select(x2, y2)),\n  actual = distGeo(routes_sf_actual |&gt; st_drop_geometry() |&gt;select(x1, y1),\n                   routes_sf_actual |&gt; st_drop_geometry() |&gt; select(x2, y2))) |&gt;\n  stack() |&gt;\n  as.data.frame() |&gt; \n  rename(Distance = values, Type = ind) |&gt;\n  mutate(Distance = Distance / 1e3,\n         Type = ordered(Type, levels = c(\"random\", \"scheduled\", \n                                         \"possible\", \"actual\"),\n                              labels = c(\"Random\", \"Scheduled\", \n                                         \"Possible off-by-one\",\n                                         \"Actual off-by-one\")))\n\nggplot(df) +\n  geom_histogram(aes(x = Distance), binwidth = 2000, \n                 fill = \"grey\", colour = \"black\", linewidth = 0.25) +\n  facet_wrap( ~ Type, scales = \"free_y\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 5: A comparison of the flight length distributions of each category of flight.\n\n\n\n\n\nLooking at these distributions there’s no reason to think there’s any geographical patterning, in spite of Canada’s attempt to tip the scales with its weird Y– codes.7\nCompletely random flights with origins and destinations randomly located on land (excluding Antarctica) and our all possible off-by-one flights are similarly distributed, although the latter has a spike in shorter (sub-5000 km) flights and another around 8000 km. These are most likely due to unevenness in population densities and hence airports relative to my naïve null model. For example, larger numbers of short flights than the null model shows are likely within-region flights, while the spike around 8000 km is probably trans-oceanic flights between densely populated coastal areas. A more sophisticated ‘null’ model would position random airports based on population densities, and perhaps demonstrate this characteristic, but that seems a bit like overkill in this context! Both these distributions include entirely unrealistic and irrelevant flights longer than 15,000 km which simply don’t exist.8\nMeanwhile, the lengths of actually existing off-by-one flights look very much like a random sample from the lengths of all scheduled flights. Based on this evidence it would be hard to make a claim that there’s any geographical pattern to where you are most likely to find yourself on one of these flights. A code after all is just a code, especially when, like IATA codes it has developed in an ad hoc manner with a need for consistency locking in many early decisions, unlike, for example, the numbers assigned to highways in the United States and roads in the United Kingdom based on explicitly geographical schemes.\nSo next time, if ever, you find yourself on one of these rare-ish 1-in-150 flights, do your best to appreciate it. But there’s really no need to add this to your bucket list. Saying which, MEL-DEL-HEL seems like it could be… fun?"
  },
  {
    "objectID": "posts/2025-06-27-iata-codes/index.html#footnotes",
    "href": "posts/2025-06-27-iata-codes/index.html#footnotes",
    "title": "What three letters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpoilers…↩︎\nAlbeit hosted by David Megginson, who appears to be FlightGear flight simulator enthusiast.↩︎\nCanada’s weird system stretches this claim pretty far.↩︎\nI regret to say I’ve been on a couple of these, a side-effect of being from Ireland and living in New Zealand. I can’t recommend such super-long flights in economy unless it’s in an A380. But can I just say? Those things are awesome.↩︎\nOh well, never mind. This is a fun little project but not one worth spending any money on.↩︎\nWhich comes with its own challenges with respect to meridians and projections, etc.↩︎\nThen again, Canada is BIG, so… maybe we need a different definition of distance. But that would be a whole other story.↩︎\nAt least not yet.↩︎"
  },
  {
    "objectID": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "href": "posts/2024-10-10-giscience-2025/giscience-2025.html",
    "title": "Giscience 2025",
    "section": "",
    "text": "The first Call for Submissions to GIScience 2025 has just come out (deadline for proceedings papers Jan 31 2025; for other submissions Apr 4 2025).\nWe’re excited in Aotearoa New Zealand to be hosting this one, even if I think it should be called giscience 2025 (note the non-capitalisation). I’m on the program committee so look forward (a little nervously) to a flood of submissions. It’s nice for a change that it won’t be us Aotearoans making the long trip.\nThe last big geospatial conference in NZ was Geocomputation 2019 which was greatly enjoyed by all who made it, and this one will be even better, I’m certain of it!"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html",
    "href": "posts/2021-10-21-kde/kde.html",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#packages",
    "href": "posts/2021-10-21-kde/kde.html#packages",
    "title": "Kernel density estimation in R spatial",
    "section": "",
    "text": "This requires a surprising number of moving parts (at least the way I did it):\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#data",
    "href": "posts/2021-10-21-kde/kde.html#data",
    "title": "Kernel density estimation in R spatial",
    "section": "Data",
    "text": "Data\nThe data are some point data (Airbnb listings from here) and some polygon data (NZ census Statistical Area 2 data).\n\nLoad the data\n\npolys &lt;- st_read(\"sa2.gpkg\")\n\nReading layer `sa2' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/sa2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 78 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1735096 ymin: 5419590 xmax: 1759041 ymax: 5443768\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\npts &lt;- st_read(\"abb.gpkg\")\n\nReading layer `abb' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-kde/abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1254 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1742685 ymin: 5420357 xmax: 1755385 ymax: 5442630\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAnd have a look\n\ntm_shape(polys) +\n  tm_polygons() + \n  tm_shape(pts) + \n  tm_dots()"
  },
  {
    "objectID": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "href": "posts/2021-10-21-kde/kde.html#spatstat-for-density-estimation",
    "title": "Kernel density estimation in R spatial",
    "section": "spatstat for density estimation",
    "text": "spatstat for density estimation\nThe best way I know to do density estimation in the R ecosystem is using the spatstat library’s specialisation of base R’s density function. That means converting the point data to a spatstat planar point pattern (ppp) object, which involves a couple of steps.\n\npts.ppp &lt;- pts$geom %&gt;% \n  as.ppp()\n\nA point pattern also needs a ‘window’, which we’ll make from the polygons.\n\npts.ppp$window &lt;- polys %&gt;%\n  st_union() %&gt;%       # combine all the polygons into a single shape\n  as.owin()            # convert to spatstat owin - again maptools...\n\n\nNow the kernel density\nWe need some bounding box info to manage the density estimation resolution\n\nbb &lt;- st_bbox(polys)\ncellsize &lt;- 100\nheight &lt;- (bb$ymax - bb$ymin) / cellsize\nwidth &lt;- (bb$xmax - bb$xmin) / cellsize\n\nNow we specify the size of the raster we want with dimyx (note the order, y then x) using height and width.\nWe can convert this directly to a raster, but have to supply a CRS which we pull from the original points input dataset. At the time of writing (August 2021) you’ll get a complaint about the New Zealand Geodetic Datum 2000 because recent changes in how projections and datums are handled are still working themselves out.\n\nkde &lt;- density(pts.ppp, sigma = 500, dimyx = c(height, width)) %&gt;%\n  raster() \ncrs(kde) = st_crs(pts)$wkt  # a ppp has no CRS information so add it\n\n\n\nLet’s see what we got\nWe can map this using tmap.\n\ntm_shape(kde) +\n  tm_raster(palette =  \"Reds\")\n\n\n\n\n\n\n\n\n\n\nA fallback sanity check\nTo give us an alternative view of the data, let’s just count points in polygons\n\npolys$n &lt;- polys %&gt;%\n  st_contains(pts) %&gt;%\n  lengths()\n\nAnd map the result\n\ntm_shape(polys) +\n  tm_polygons(col = \"n\", palette = \"Reds\", title = \"Points in polygons\")\n\n\n\n\n\n\n\n\n\n\nAggregate the density surface pixels to polygons\nThis isn’t at all necessary, but is also useful to know. This is also a relatively slow operation. Note that we add together the density estimates in the pixels contained by each polygon.\n\nsummed_densities &lt;- raster::extract(kde, polys, fun = sum)\n\nAppend this to the polygons and rescale so the result is an estimate of the original count. We multiply by cellsize^2 because each cell contains an estimate of the per sq metre (in this case, but per sq distance unit in general) density, so multiplying by the area of the cells gives an estimated count.\n\npolys$estimated_count = summed_densities[, 1] * cellsize ^ 2\n\nAnd now we can make another map\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\",\n              title = \"500m KDE summed\")\n\n\n\n\n\n\n\n\nSpot the deliberate mistake?!\nSomething doesn’t seem quite right! What’s with the large numbers in the large rural area to the west of the city? Thing is, you shouldn’t really map count data like this, but should instead convert to densities. If we include that option in the tm_polygons function, then order is restored.\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\", convert2density = TRUE,\n              title = \"500m KDE estimate\")\n\n\n\n\n\n\n\n\nReally, this should be done with the earlier map of points in polygons too, so let’s show all three side by side. tmap_arrange is nice for this, although it has trouble making legend title font sizes match, unless you do some creative renaming. I’ve also multiplied the KDE result by 1,000,000 to convert the density to listings per sq. km, and we can see that the three maps are comparable.\n\nm1 &lt;- tm_shape(kde * 1000000) + \n  tm_raster(palette = \"Reds\", title = \"500m KDE\")\nm2 &lt;- tm_shape(polys) + \n  tm_fill(col = \"n\", palette = \"Blues\", convert2density = TRUE,\n              title = \"Point density\")\nm3 &lt;- tm_shape(polys) + \n  tm_fill(col = \"estimated_count\", palette = \"Greens\", convert2density = TRUE,\n              title = \"KDE summed\")\ntmap_arrange(m1, m2, m3, nrow = 1)"
  },
  {
    "objectID": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html",
    "href": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "This quick post rounds out the GIS transformations from area data to other spatial types. It turns out—to my relief, if not my surprise—that a package is available that implements the suggested transformation from areas to field data in the table on page 26 of Geographic Information Analysis, making this one of the simpler posts in the series so far.\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(predicts)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(ggspatial)\n\ntheme &lt;- theme_void() +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.5, 0.075),\n        legend.direction = \"horizontal\",\n        legend.title.position = \"top\",\n        legend.key.width = unit(0.8, \"cm\"),\n        legend.key.height = unit(0.5, \"cm\"),\n        legend.text = element_text(angle = 45, hjust = 1),\n        legend.background = element_rect(fill = \"#ffffff80\", colour = NA), \n        panel.background = element_rect(fill = NA),\n        panel.ontop = TRUE)"
  },
  {
    "objectID": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#from-areas",
    "href": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#from-areas",
    "title": "GIS, a transformational approach",
    "section": "From areas…",
    "text": "From areas…\nAs always, we need some data. It’s surprisingly tricky to find an area of New Zealand where the population density is high(ish) and somewhat variable over an extensive area such that the result of density smoothing are likely to be interesting. Best I can do is some old census data by 100 or so Census Area Units for central Auckland back in 2006.\n\n\nCode\ncontext &lt;- st_read(\"ak-context.gpkg\")\npolygons &lt;- st_read(\"ak-2006-ethnicity-and-tb.gpkg\") |&gt; \n  mutate(\n    area_km2 = (st_area(geom) |&gt; units::drop_units()) / 1e6,\n    pop_density_km2 = population / area_km2)\n\n\nHere’s what those data look like.\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_sf(data = context, colour = NA) + \n  geom_sf(data = polygons, aes(fill = pop_density_km2), colour = NA) +\n  scale_fill_distiller(\"Population density sq.km\",\n                       palette = \"Purples\", direction = 1) +\n  coord_sf(expand = FALSE) +\n  annotation_scale() +\n  theme\ng1\n\n\n\n\n\n\n\n\nFigure 1: The SA2 population data, by count, and by density\n\n\n\n\n\nThere are a lot of people (many of them students) in apartments living at somewhat urban densities in the central city area, but the rest of this fairly extensive area (about 155 sq.km) is fairly suburban. Perhaps recent developments in Auckland are set to change that, although New Zealand has resisted densification for so long I wouldn’t bet the house on it."
  },
  {
    "objectID": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#to-fields",
    "href": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#to-fields",
    "title": "GIS, a transformational approach",
    "section": "… To fields",
    "text": "… To fields\nAnyway, we are here to create a pycnophylactic-ally smoothed surface from this. Pycnophylactic smoothing seems to me like a word that Waldo Tobler made up for a bet. In the original paper1 he suggests that the term means ‘mass preserving’, although the prefix ‘pycno-’ apparently refers to thickness or density in the Greek from whence it came. Either way it’s easier to think of it as meaning ‘volume-preserving’, as the volume under the surface produced by this transformation is equal to that of prisms extruded to a height given by the areal density of the countable phenomenon of interest. This is a desirable property for any smoothing process to have as it means that the sum across all cells in the resulting raster surface will match the total across all regions in the source data.\nAnyway, the mechanics of the process consist of repeated local averaging and rescaling to ensure that the quantity attributed to each region after each round of averaging matches the starting quantity. Repeated application of averaging and rescaling sees the population of denser regions migrate towards those regions’ centres and of less dense regions towards more densely populated neighbouring regions.\nA function to perform this iterative process is provided by the predicts package. The pycnophy function seems like a bit of an afterthought in the package, which is mostly concerned with species distribution models. Running the function is straightforward. Convergence can take a while if you go for a very small cell size. In this case 100m seems about right given the extent of the data.\n\n\nCode\npop_raster &lt;- polygons |&gt;\n  as(\"SpatVector\") |&gt;\n  rast(res = 100)\n\npycno &lt;- pop_raster |&gt;\n  pycnophy(as(polygons, \"SpatVector\"), \"population\") |&gt;\n  terra::as.data.frame(xy = TRUE) |&gt;\n  rename(population_est = layer)\n\n\nThat’s it. Here’s a map of the result. Showing the boundaries of the regions makes clear how volume has migrated towards the centre of dense areas and away from the centre of low density areas.\n\n\nCode\ng2 &lt;- ggplot() +\n  geom_sf(data = context, colour = NA) +\n  geom_raster(data = pycno, aes(x = x, y = y, fill = population_est)) +\n  scale_fill_distiller(\"Smoothed population per ha.\",\n                       palette = \"Purples\", direction = 1) +\n  geom_sf(data = polygons, fill = NA, lwd = 0.1) +\n  coord_sf(expand = FALSE) +\n  theme\ng2\n\n\n\n\n\n\n\n\nFigure 2: Pycnophylactic smoothed map of population\n\n\n\n\n\nAnd we can confirm that the smoothing has preserved volume by summing the population from the source regions, and from the smoothed surface.\n\n\nCode\npolygons$population |&gt; sum()\n\n\n[1] 409941\n\n\nCode\npycno$population_est |&gt; sum()\n\n\n[1] 409941\n\n\nHurrah! They are exactly equal.\nThere’s not a lot more to be said about this, other than perhaps to acknowledge yet another contribution by Waldo Tobler. I can’t help adding, in passing that in the opening paragraph of the paper he states that, “[A] common fact of geography is that places influence each other”,2 a rather less snappy take on his eponymous ‘first law’."
  },
  {
    "objectID": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#footnotes",
    "href": "posts/2025-10-14-gia-chapter-1B-part-3C/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTobler WR. 1979. Smooth pycnophylactic interpolation for geographical regions. Journal of the American Statistical Association *74**(367) 519–530.↩︎\nibid. page 519↩︎"
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html",
    "title": "In praise of GeoPackages",
    "section": "",
    "text": "In the shapefiles must die wars I’ve been smugly using GeoPackages for several years now. Here are some reasons why."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#one-file-no-really-its-just-one-file",
    "title": "In praise of GeoPackages",
    "section": "One file: no really, it’s just one file!",
    "text": "One file: no really, it’s just one file!\nIf there is one thing above all others to love about GeoPackages it’s this. When teaching newcomers, the standout advantage over shapefiles is simple: there is only one file, and not some number between three and seven (or is it eight? I’m just not sure).\nI’ve lost count of how often I had to disable the increasingly hard to find Hide file extensions option on a baffled student’s computer to reveal the disturbing truth that there were several identical-except-for-the-extension files that together formed a so-called shapefile. Or how when supplying data for lab assignments I had to include instructions about unzipping files to a known folder and so on (a seemingly simple requirement made much more complicated by more recent releases of Windows allowing the user to look inside a .zip file without actually unpacking the contents…)."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#understandable-attribute_",
    "title": "In praise of GeoPackages",
    "section": "Understandable ATTRIBUTE_",
    "text": "Understandable ATTRIBUTE_\nWhat I meant to say was: understandable attribute_names because you can have attribute names longer than 10 characters."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-layers",
    "title": "In praise of GeoPackages",
    "section": "Many layers",
    "text": "Many layers\nI’ve tended to shy away from packaging multiple datasets in GeoPackages as support for this feature has at times been uncertain and confusing.\nNow that I am less beholden to not confusing beginners where ‘one file = one layer’ is a useful rule to live by, I’ve started to look more closely at what’s going on here. It still has the potential to confuse — especially in QGIS’s right-click Export → Save Features As… — but there is untapped potential here for making life easier when it comes to sharing bundles of related data with a minimum of fuss.\nI’ll explain using my go to tool for general data wrangling, R’s sf package.\n\nMulti-layer geopackages in R\nAssuming you have a locally stored simple GeoPackage nz.gpkg, you read it using:\n\nlibrary(sf)\nnz &lt;- st_read(\"nz.gpkg\")\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow, if you’d like to add another dataset to that file, you can specify a layer to put it in. Before doing that, it’s probably best to check what layers are already there using st_layers():\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nAs we might expect (and if you’ll excuse the awkward formatting due to the long crs_name) a single layer with the same layer name as the file itself. Say we buffer our data and want to store it back into the same file, then we can do the below, as long as we provide a new layer name to store it in:\n\nnz |&gt; \n  st_buffer(12000) |&gt; \n  st_write(\"nz.gpkg\", layer = \"coast\")\n\nWriting layer `coast' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nAnd now we can see that both layers are present in the file:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1         nz Multi Polygon        1      1\n2      coast Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n2 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nNow if you open this file in R unless you specify the layer you want, you’ll just get the first one:\n\nst_read(\"nz.gpkg\")\n\nMultiple layers are present in data source /Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg, reading layer `nz'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `nz' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2024-10-16-geopackages/nz.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748531 xmax: 2463348 ymax: 6191876\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nand one of those warning messages it’s tempting not to read, but really should.\nAnd that’s it really, for multiple vector layers in GeoPackages in R.\n\n\nMulti-layer geopackages in QGIS\nMeanwhile, if you open a two-layer GPKG in QGIS you’ll see this:\n\n\n\n\n\nThat’s pretty clear. What’s unfortunately less clear than in R is the sequence of operations that will safely add a layer to an existing GeoPackage. That’s not quite fair: what is unclear is the warning message you get if you choose an existing .gpkg file as the destination for a dataset you’d like to save. The warning message looks like this:\n\n\n\n\n\nThis seems pretty scary. Before trying this at home I suggest you make a copy of the target geopackage if you are worried about losing your data, but if you steel yourself, and against every instinct hit Replace, then as long as you set a different name in the Layer name option of the Save Vector Layer as… dialog\n\n\n\n\n\nit will be fine, and you’ll end up with an additional layer in the target GeoPackage.\nYou can also manage the component layers of GeoPackages in QGIS’s Browser panel.\n\n\nOther platforms are available\nI should note that similar to R, the Python geopandas module through its read_file() and list_layers() functions, and its GeoDataframe’s to_file() method offers the same functionality as discussed above."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#many-formats",
    "title": "In praise of GeoPackages",
    "section": "Many formats",
    "text": "Many formats\nSo if you can put many different layers in a GeoPackage, can you mix vector and raster datasets in there too?\nTurns out you can, although, at least for R’s sf this is where things get a bit messy. terra is the package for dealing with raster data, so let’s load that and read in a raster dataset. Before doing that, I’ll clean up nz.gpkg so it only has one layer again:\n\nnz |&gt; st_write(\"nz.gpkg\", layer = \"vector\", delete_dsn = TRUE)\n\nDeleting source `nz.gpkg' using driver `GPKG'\nWriting layer `vector' to data source `nz.gpkg' using driver `GPKG'\nWriting 1 features with 1 fields and geometry type Multi Polygon.\n\n\nNow load the raster layer\n\nlibrary(terra)\nnz_r &lt;- rast(\"nz.tif\")\nnz_r\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.tif \nname        : layer \nmin value   :     1 \nmax value   :     1 \n\n\nCrowbarring this thing into our GeoPackage is certainly possible, but it’s far from intuitive, and involves invoking some GDAL options.\n\nnz_r |&gt; \n  writeRaster(\"nz.gpkg\", \n              gdal = c(\"RASTER_TABLE=raster\", \"APPEND_SUBDATASET=YES\"))\n\nThe GDAL options are all documented, but applying them using terra::writeRaster is finicky, and clearly this is not for the faint-hearted!\nFurthermore… sf can’t ‘see’ the raster layer:\n\nst_layers(\"nz.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields\n1     vector Multi Polygon        1      1\n                                         crs_name\n1 NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nI guess if you don’t ‘do’ raster layers then there is no point in being able to see them either ¯\\_(ツ)_/¯. terra is similarly see-no-evil about things and just reads in the raster layer that is in the file without commenting on other layers that might be present:\n\nrast(\"nz.gpkg\")\n\nclass       : SpatRaster \ndimensions  : 144, 137, 1  (nrow, ncol, nlyr)\nresolution  : 10000, 10000  (x, y)\nextent      : 1090144, 2460144, 4748531, 6188531  (xmin, xmax, ymin, ymax)\ncoord. ref. : NZGD2000 / New Zealand Transverse Mercator 2000 (EPSG:2193) \nsource      : nz.gpkg \nname        : Height \n\n\nQGIS actually does better here. It certainly sees both layers:\n\n\n\n\n\nIt’s worth noting that the QGIS Browser panel makes mixing raster and vector layers into your GeoPackages straightforward.\n\nOverall, I’ve been aware that I can bundle raster and vector layers in GeoPackages like this but haven’t used the capability. In part because I’ve only just figured out how to do it using the R tools(!), but mostly because I prefer to keep raster data in GeoTIFFs and vector data in GeoPackages so I can tell which is which at a glance."
  },
  {
    "objectID": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "href": "posts/2024-10-16-geopackages/in-praise-of-the-geopackage.html#so-are-geopackages-perfect",
    "title": "In praise of GeoPackages",
    "section": "So, are GeoPackages perfect?",
    "text": "So, are GeoPackages perfect?\nOf course not. There’s an argument to be made that every format that perpetuates the simple features paradigm is a bad as every other. I even wrote a book that is — kind of — all about this. It’s one of the mysteries of the evolution of geospatial that topology was embedded in the ‘standard’ formats, until it wasn’t. For what it’s worth, I think we have relational databases to blame for that.\nGeoPackages don’t get us out of floating point geometry hell either.\nThere are also better formats for particular applications. GeoJSON is web-native in a way that GeoPackages never will be, and newer formats such as FlatGeoBuf and GeoParquet, and more recent approaches like Discrete Global Grids certainly have their place.\nBut in a world still dominated by relational DBMS, a geospatial format that is basically a wrapper around SQLite tables was almost certain to emerge eventually, and GeoPackages are that format. They’re vastly preferable to shapefiles, and it’s good to see them slowly (more quickly would be better) replacing them.\nThe shapefile is (almost) dead, long live the GeoPackage!"
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "In this post and this one I covered various GIS transformations from respectively points and lines. Here, we carry on down this path with some of the transformations available when your geospatial data consist of areas.\nAs it turns out, area data are complicated, and some of the associated transformations have more than a few wrinkles, so one post has become two, and this post only covers polygons → points, and polygons → lines.1\nAs will become clear, that’s more than enough to be going on with. Check out the number of packages imported below, if you’re not convinced.\nCode\nlibrary(sf)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(sfnetworks)\nlibrary(tidygraph)\nlibrary(raybevel)\nlibrary(rmapshaper)\nlibrary(spatstat)\nlibrary(terra)\n\ntheme_set(theme_void())"
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#from-areas",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#from-areas",
    "title": "GIS, a transformational approach",
    "section": "From areas…",
    "text": "From areas…\nAs always, we need some data. This time it’s useful to have data with associated attributes, so we’ll use some real data, conveniently to hand, namely New Zealand 2018 census Statistical Areas, with populations.\n\n\nCode\npolygons &lt;- st_read(\"sa2-generalised.gpkg\")\npolygon1 &lt;- (polygons |&gt; \n1  st_cast(\"POLYGON\"))$geom[1]\npolygon2 &lt;- (polygons |&gt;\n  filter(SA22018_V1_00_NAME == \"Mangatainoka\"))$geom[1]\nwellington &lt;- polygons |&gt;\n  filter(TA2018_V1_00_NAME == \"Wellington City\") |&gt;\n  select(population)\n\n\n\n1\n\nThe first polygon in the data is actually a multipolygon, so we cast to polygons before selecting it.\n\n\n\n\nAnd here are what those look like. The two single polygons have been picked out for their complexity, and in the case of the second one, for its having a couple of holes, which makes some transformations trickier.\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_sf(data = wellington, aes(fill = population)) +\n  scale_fill_distiller(palette = \"Reds\") +\n  guides(fill = \"none\") +\n  ggtitle(\"Wellington\")\ng2 &lt;- ggplot(polygon1) + geom_sf() + ggtitle(\"Polygon #1\")\ng3 &lt;- ggplot(polygon2) + geom_sf() + ggtitle(\"Mangatainoka\")\n\ng1 | g2 | g3\n\n\n\n\n\n\n\n\nFigure 1: The sample data sets (note that these are not at the same scale)"
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-points",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-points",
    "title": "GIS, a transformational approach",
    "section": "… to points",
    "text": "… to points\nThe table of transformations™ calls for ‘Centroid’, which is easily accomplished with the st_centroid function.\n\n\nCode\ncentroids &lt;- wellington |&gt;\n  st_centroid()\n\nggplot() + \n  geom_sf(data = wellington) +\n  geom_sf(data = centroids)\n\n\n\n\n\n\n\n\nFigure 2: Centroids for all the Wellington SA2s\n\n\n\n\n\nAs is likely well-known, the centroid of an area is not guaranteed to be inside the area. There are a couple of examples of that in these data. A function that returns a point guaranteed to be inside the area is st_point_on_surface and this should be used in preference to st_centroid when the point2 is to enable data to be reliably joined between different polygon layers—say, for example, a set of polygons, and a set of the ‘same’ polygons which have been simplified or generalised.\nThe difference is shown in the figure below.\n\n\nCode\npolygons_with_no_centroid &lt;- wellington |&gt;\n  slice(((wellington |&gt;\n            st_contains(centroids) |&gt; \n            lengths()) == 0) |&gt; which())\n\nbb &lt;- st_bbox(polygons_with_no_centroid)\n\npoints_on_surface &lt;- polygons_with_no_centroid |&gt;\n  st_point_on_surface()\n\nggplot() + \n  geom_sf(data = wellington, fill = \"lightgrey\") + \n  geom_sf(data = polygons_with_no_centroid,\n          colour = \"pink\", lwd = 1) +\n  geom_sf(data = centroids) + \n  geom_sf(data = points_on_surface, colour = \"red\") +\n  coord_sf(xlim = bb[c(1, 3)], ylim = bb[c(2, 4)])\n\n\n\n\n\n\n\n\nFigure 3: Three centroids (black points) that are not inside their ‘parent’ polygons (outline in pink), and the corresponding points on surface (red points)\n\n\n\n\n\nThis case is relatively simple, insofar as the polygons that do not contain a centroid have not ‘acquired’ one from a neighbour, so checking for zero centroids correctly identifies ‘problem’ polygons.\nIt is easy to imagine this not working, however. For example, two intertwined U-shaped polygons, can ‘swap’ centroids, as shown below. If you were to use these centroids to move data between layers then the data associated with these polygons would get swapped in the process.\n\n\nCode\nsquare &lt;- function(dx, dy) {\n  (st_linestring(\n    matrix(c(0, 0, 1, 1, 0,\n             0, 1, 1, 0, 0), ncol = 2) \n  ) + c(dx, dy)) |&gt; \n    st_cast(\"POLYGON\")\n}\n\nu_polygons &lt;- \n  mapply(square, \n         dx = rep(0:3, 4), \n         dy = rep(3:0, each = 4),\n         SIMPLIFY = FALSE) |&gt;\n  st_sfc() |&gt;\n  data.frame() |&gt;\n  st_sf(crs = 2193) |&gt;\n  mutate(\n    ID = as.factor(c(1, 1, 1, 1,\n                     1, 2, 2, 2,\n                     1, 1, 1, 2,\n                     2, 2, 2, 2))) |&gt;\n  group_by(ID) |&gt;\n  summarise() |&gt;\n  mutate(geometry = st_simplify(geometry, dTolerance = 1e-6))\n\nus_centroids &lt;- u_polygons |&gt; st_centroid()\n\nggplot() +\n  geom_sf(data = u_polygons) +\n  geom_sf_label(data = u_polygons, aes(label = ID)) +\n  geom_sf(data = us_centroids) +\n  geom_sf_label(data = us_centroids, aes(label = ID), colour = \"red\")\n\n\n\n\n\n\n\n\nFigure 4: Two u-shaped polygons with IDs shown in black, and their associated centroids correspondingly labelled in red\n\n\n\n\n\nTL;DR: be careful using centroids as a convenient way to summarise the location of polygons!"
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-lines",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-lines",
    "title": "GIS, a transformational approach",
    "section": "… to lines",
    "text": "… to lines\nThe straight skeleton of a polygon is the set of lines that it collapses down along as it is negatively buffered and eventually disappears. We can see how this works for a simple convex shape without too much trouble.\n\n\nCode\nshape &lt;- st_polygon(\n    list(matrix(c(0, 0, 1, 2, 2, 1, 0,\n                  0, 1, 2, 2, 1, 0, 0), ncol = 2))) |&gt;\n  st_sfc(crs = 2193)\n\ncollapsed_polys &lt;- \n  mapply(st_buffer, shape, dist = -(0:7/10), nQuadSegs = 0,\n    SIMPLIFY = FALSE) |&gt;\n  st_sfc()\n\nggplot() +\n  geom_sf(data = collapsed_polys, fill = NA)\n\n\n\n\n\n\n\n\nFigure 5: An illustration of the skeleton of a simple polygon\n\n\n\n\n\nUnsurprisingly, things get complicated quickly when we move away from simple convex shapes, especially when we add the additional complication of holes. Sadly, there is no st_skeleton function in sf. Happily, this being R, there’s a package for that™. raybevel is a package for generating straight skeletons including extensions into 3D and roof models. If you squint at the simple example, you can see the relevance of roof models to this topic. In fact, according to wikipedia, this is the context in which the straight skeleton of a polygon first appeared as far back as 1877. The picture below (snipped from a recent property listing in Wellington reinforces this point.\n\n\n\n\n\n\nFigure 6: A straight skeleton roofline in the wild\n\n\n\nThe raybevel::skeletonize function meets our requirements. The result from that function is a collection of points, and collection of source-destination pairs of point ids, which we can link to one another to create the skeleton. The sfnetworks package is a good way to do the linking to make the result into a sf dataset.\n\n\nCode\nsk &lt;- st_coordinates(shape)[, 1:2] |&gt;\n  skeletonize()\n\nnw &lt;- sfnetwork(sk$nodes |&gt; st_as_sf(coords = 2:3), \n                edges = sk$links |&gt; filter(edge == FALSE), \n                directed = FALSE, \n                node_key = \"id\",\n                edges_as_lines = TRUE)\n\nggplot() +\n  geom_sf(data = shape) +\n  geom_sf(data = nw |&gt; st_as_sf(\"edges\") |&gt;\n            st_set_crs(2193), colour = \"red\", lwd = 0.2)\n\n\n\n\n\n\n\n\nFigure 7: Skeleton of a simple polygon\n\n\n\n\n\nHere is a more taxing example:\n\n\nCode\nsk &lt;- st_coordinates(polygon1)[, 1:2] |&gt;\n  skeletonize()\n\nnw &lt;- sfnetwork(sk$nodes |&gt; st_as_sf(coords = 2:3), \n                edges = sk$links |&gt; filter(edge == FALSE), \n                directed = FALSE, \n                node_key = \"id\",\n                edges_as_lines = TRUE)\n\ng1 &lt;- ggplot() +\n  geom_sf(data = polygon1) +\n  geom_sf(data = nw |&gt; st_as_sf(\"edges\") |&gt;\n            st_set_crs(2193), colour = \"red\", lwd = 0.2)\ng1\n\n\n\n\n\n\n\n\nFigure 8: Straight skeleton of a more complicated polygon\n\n\n\n\n\nIf we need to handle holes in the polygon, we have to also supply skeletonize with a list of the hole polygons, which requires us to do a bit more work:\n\n\nCode\nexterior &lt;- polygon2 |&gt; \n  st_exterior_ring()\n\nholes &lt;- exterior |&gt;\n  st_difference(polygon2) |&gt;\n  st_cast(\"POLYGON\")\n\nxy &lt;- st_coordinates(exterior)[, 1:2]\nxy_holes &lt;- holes |&gt;\n  lapply(st_coordinates) |&gt;\n1  lapply(subset, select = 1:2)\n\nsk &lt;- skeletonize(xy, xy_holes)\n\n\n\n1\n\nst_coordinates produces additional columns that skeletonize will choke on if we don’t remove them.\n\n\n\n\nAfter that the further steps required are the same as before and the resulting skeleton is shown below.\n\n\nCode\nnodes &lt;- sk$nodes |&gt; st_as_sf(coords = 2:3)\nedges &lt;- sk$links |&gt; rename(from = source, to = destination)\n\nnw &lt;- sfnetwork(nodes, \n                edges = edges |&gt; filter(edge == FALSE), \n                directed = FALSE, \n                node_key = \"id\",\n                edges_as_lines = TRUE)\n\nggplot() +\n  geom_sf(data = polygon2) +\n  geom_sf(data = nw |&gt; st_as_sf(\"edges\") |&gt;\n            st_set_crs(2193), colour = \"red\", lwd = 0.2)\n\n\n\n\n\n\n\n\nFigure 9: Straight skeleton of a polygon with holes\n\n\n\n\n\nWhile I am here, I might as well show the related medial axis, which is the set of points in a polygon that have two or more equidistant closest points on the polygon boundary. Perhaps the most obvious example where this might be of interest is tracing a possible centre-line of a river.\nThe medial axis of a polygon can be approximated as the edges of the Voronoi tessellation of the polygon vertices.\n\n\nCode\nbb &lt;- polygon1 |&gt; st_buffer(1000) |&gt; st_bbox() |&gt; st_as_sfc()\n\nmedial_axis &lt;- polygon1 |&gt;\n  st_union() |&gt;\n  st_cast(\"MULTIPOINT\") |&gt;\n  st_voronoi() |&gt;\n  st_cast() |&gt;\n  st_intersection(bb) |&gt;\n1  rmapshaper::ms_lines() |&gt;\n  st_intersection(polygon1) |&gt;\n  data.frame() |&gt;\n  st_sf() |&gt;\n  filter(st_geometry_type(geometry) == \"LINESTRING\" |\n2         st_geometry_type(geometry) == \"MULTILINESTRING\")\n\ng2 &lt;- ggplot() +\n  geom_sf(data = polygon1) +\n  geom_sf(data = medial_axis, colour = \"red\", lwd = 0.2) +\n  ggtitle(\"Medial axis\")\n\ng1 + ggtitle(\"Straight skeleton\") | g2\n\n\n\n1\n\nThis is the cleanest way I know to make polygons into a set of lines. Other approaches can struggle to remove duplicate edges derived shared sides of neighbouring polygons.\n\n2\n\nThe st_intersection step produces a jumble of geometry types including points, and multipoints, so this filter step removes those.\n\n\n\n\n\n\n\n\n\n\nFigure 10: The straight skeleton and medial axis compared\n\n\n\n\n\nBecause the sf::st_voronoi function produces a set of polygons, we have to convert these to a set of lines, which is most conveniently accomplished via the rmapshaper::ms_lines function. Even so, there’s still a need for several st_cast steps along the way, which doesn’t make for the cleanest code.\nPerhaps surprisingly, a cleaner, less confusing sequence of operations uses the dirichletEdges function in spatstat to get the same result.3\n\n\nCode\nxy &lt;- polygon1 |&gt; \n  st_sf() |&gt; \n  st_cast(\"POINT\") |&gt; \n1  slice(-1) |&gt;\n  st_coordinates()\n\n2pp &lt;- ppp(xy[, 1], xy[, 2], as.owin(bb))\n\nedges &lt;- dirichletEdges(pp) |&gt;\n  st_as_sf() |&gt;\n3  filter(label == \"segment\") |&gt;\n  select(-label) |&gt;\n  st_set_crs(2193) |&gt;\n  st_intersection(polygon1)\n\nggplot() +\n  geom_sf(data = polygon1) +\n  geom_sf(data = edges, colour = \"red\", lwd = 0.2)\n\n\n\n1\n\nRepeated points are not welcome in spatstat point patterns.\n\n2\n\nThis makes a spatstat point pattern from our polygon coordinates.\n\n3\n\nThe output includes the point pattern window as a polygon, which this filter step removes.\n\n\n\n\n\n\n\n\n\n\nFigure 11: The medial axis as determined by spatstat\n\n\n\n\n\nBoth these results, because they are based only on the vertices of the polygon are only approximations to the true medial axis, which should be derived from the Voronoi diagram of the polygon’s edges. Such diagrams may contain segments that are parabolic arcs, and not only straight line segments as is the case for a Voronoi diagram derived from point data. You can get some feel for the complexities that might be involved in the precise calculation from this post of mine, and from this blog post by a computational geometer who knows what they are doing.\n\nAn aside: line distance surface and the medial axis\nI mentioned that the distance surface in the lines → fields section of the previous post might contain ‘spoilers’ for polygon skeletons. This wasn’t quite correct. However there’s certainly a way to derive the medial axis of a polygon by going via a line distance surface.\nHere’s the code, which is adapted from that previous post. The additional step of deriving a slope layer from the distance surface helps to illustrate the relationship even more clearly.\n\n\nCode\ntarget_raster &lt;- polygon1 |&gt;\n  as(\"SpatVector\") |&gt;\n  rast(res = 100)\n\ntarget_points &lt;- target_raster |&gt;\n  as.points() |&gt;\n  st_as_sf()\n\nd1 &lt;- target_points |&gt;\n  mutate(\n    distance = st_distance(geometry, polygon1 |&gt; st_cast(\"LINESTRING\")) |&gt; \n      apply(MARGIN = 1, min)) |&gt;\n  as(\"SpatVector\") |&gt;\n  rasterize(target_raster, field = \"distance\") |&gt;\n  mask(polygon1 |&gt; as(\"SpatVector\"))\n\nd2 &lt;- d1 |&gt; terrain(neighbors = 4)\n\nnames(d1) &lt;- \"z\"\nnames(d2) &lt;- \"z\"\n\n\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_raster(\n    data = d1 |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = z)) +\n  geom_sf(data = polygon1, fill = NA) +\n  scale_fill_viridis_c(option = \"B\", direction = -1) +\n  guides(fill = \"none\") +\n  ggtitle(\"Distance surface\")\n\ng2 &lt;- ggplot() +\n  geom_raster(\n    data = d2 |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = z)) +\n  geom_sf(data = polygon1, fill = NA) +\n  scale_fill_viridis_c(option = \"B\") +\n  guides(fill = \"none\") +\n  ggtitle(\"Distance surface slope\")\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 12: The distance surface and slope of distance surface for a polygon boundary\n\n\n\n\n\nAnd here they are plotted together showing the expected match.\n\n\nCode\ng1 + geom_sf(data = medial_axis, colour = \"white\", lwd = 0.2)\n\n\n\n\n\n\n\n\nFigure 13: The vector and raster generated medial axis results compared\n\n\n\n\n\nCalculated at sufficient precision this raster approach may be a more useful way to find the medial axis."
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-be-continued",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#to-be-continued",
    "title": "GIS, a transformational approach",
    "section": "To be continued…",
    "text": "To be continued…\nThe transformations of area objects, particularly to lines, led down deeper rabbit holes than expected, so that’s it for now. Tune in again soon for transformations to areas and to fields."
  },
  {
    "objectID": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#footnotes",
    "href": "posts/2025-09-17-gia-chapter-1B-part-3A/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe post titles and URLs are starting to read like complex case law. My apologies.↩︎\nExcuse the pun.↩︎\nDirichlet tessellation is yet another name for the variously named Voronoi, Thiessen, and proximity polygons. Great ideas have many owners, I guess.↩︎"
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "Figure 1: Kākā (in Zealandia) by russellstreet CC BY-SA 2.0\nWhat’s with the kākā (Nestor meridionalis)?1 Well, they are one of the native bird species now relatively commonly seen in Wellington, largely due to the conservation efforts of the Zealandia wildlife sanctuary, and Zealandia is where the data for this post are from.\nBut I digress… the point of this post is to round out the GIS transformations with Field → Area and Field → Field examples.\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tmap)\nlibrary(whitebox)\nlibrary(cols4all)\nlibrary(ggplot2)\nlibrary(ggnewscale)\nlibrary(patchwork)\nlibrary(SAiVE)"
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#from-fields",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#from-fields",
    "title": "GIS, a transformational approach",
    "section": "From fields…",
    "text": "From fields…\n\n\nCode\ndem       &lt;- rast(\"zealandia-5m.tif\")\n1dem       &lt;- dem |&gt; crop(ext(dem) + c(0, 0, -5, 0))\nstreams   &lt;- st_read(\"streams.gpkg\") |&gt; select()\nlake      &lt;- st_read(\"lake.gpkg\") |&gt; select(name)\nslope     &lt;- terrain(dem, unit = \"radians\")\naspect    &lt;- terrain(dem, v = \"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, angle = 30, direction = 135)\n\n\n\n1\n\nTurns out there is a row of NAs at the southern end of this DEM.\n\n\n\n\nAs noted, we are using the same study area as in the previous post. For some context (and to learn about surface networks) go there and take a look. Meanwhile, here’s a map for orientation again.\n\n\n\n\n\n\nFigure 2: Location map of the Zealandia study area"
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#to-areas",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#to-areas",
    "title": "GIS, a transformational approach",
    "section": "… To areas",
    "text": "… To areas\nRasters are already area data—what else are we supposed to call lots of little squares?—so any operation that somehow or other aggregates those little squares into larger groups2 on some basis is a Field→Area transformation. Herewith one interesting way to combine raster pixels, and one slightly less interesting one.\n\nWatershed delineation\nDelineating drainage basins is one of those GIS operations that we’ve come to take for granted, although it seems clear even to this outsider that the ‘hydrological modelling’ you can do in a GIS, and ‘real’ hydrological modelling remain distinct domains. This was written in 2025:\n\nResearch gaps and challenges in existing studies are identified, emphasizing the lack of standardized methodologies, limited integration between hydrological models and GIS technologies, and difficulties in scaling GIS-based strategies to broader regional or national levels.3\n\nMeanwhile, in 1999, Sui and Maggio had this to say:\n\nWe believe that there are problems in both hydrological models and the current generation of GIS. These problems must be addressed before we can make the integration of GIS with hydrological modeling theoretically consistent, scientifically rigorous, and technologically interoperable.4\n\nSo… plus ça change, plus c’est le même chose. Without getting too deeply into it, the fundamental difference between the hydrological modelling readily available in GIS and hydrological modelling proper is that the former is generally static, while the latter is dynamic.\nHaving said that, watersheds are relatively static and a by now well-established workflow can derive, for a given terrain, with relative ease, plausible drainage basins, or perhaps more correctly, the upstream watershed of a specified location.\nProbably the best freely available tools for this kind of work are John Lindsay’s Whitebox Tools although the options are somewhat bewildering. I found the drainageBasins() function available in the SAiVE package from Clement Bataille’s lab of the same name a handy one-liner solution for my purposes. It has the slightly unnerving side-effect of creating lots of files, and (like) Whitebox Tools, which it relies on for the analysis, often requiring that you work with shapefiles,5 but that’s a small price to pay for not having to think too hard about all the options. The options are certainly there in the Whitebox suite, and if you are doing this sort of stuff seriously, you should certainly explore them.\nAnyway, here’s that one-liner.\n\n\nCode\ndrainageBasins(\n  \"./zealandia-5m.tif\",\n  points = \"./basin-centre.shp\",\n  points_name_col = \"name\",\n  save_path = \"./watersheds\"\n)\n\n\nThe first argument is the file containing our DEM. The points argument specifies a file with points for which we want the upstream watershed, that is, all the land upstream that drains to that point. The points file should have a name column which is used to name resulting output files and folders. Even SAiVE::drainageBasins has more options than this, but let’s just go with the basic configuration.\nThe analysis generates a bunch of output raster layers:\n\n\nCode\noutput_rasters &lt;- dir(\"./watersheds\", pattern = \"*.tif\", full.names = TRUE)\noutput_rasters\n\n\n[1] \"./watersheds/D8fac.tif\"           \"./watersheds/D8pointer.tif\"      \n[3] \"./watersheds/FilledDEM.tif\"       \"./watersheds/streams_derived.tif\"\n\n\nwhich we can stack together:\n\n\nCode\nstack &lt;- rast(output_rasters)\n\n\nHere’s what three of those outputs look like:\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_raster(\n    data = stack$D8pointer |&gt; \n      as.data.frame(xy = TRUE) |&gt;\n      mutate(D8pointer = ordered(D8pointer)),\n    aes(x = x, y = y, fill = D8pointer)) +\n  scale_fill_discrete_c4a_seq(palette = \"brewer.spectral\") +\n  coord_sf() +\n  guides(fill = \"none\") +\n  ggtitle(\"Flow direction\") +\n  theme_void()\n\ng2 &lt;- ggplot() +\n  geom_raster(\n    data = stack$D8fac |&gt;\n      as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = sqrt(D8fac))) +\n  scale_fill_continuous_c4a_seq(palette = \"-kovesi.blue_cyan\") +\n  coord_sf() +\n  guides(fill = \"none\") +\n  ggtitle(\"square-root Flow accumulation\") +\n  theme_void()\n\ng3 &lt;- ggplot() +\n  geom_raster(\n    data = stack$streams_derived |&gt; \n      as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = streams_derived)) +\n  coord_sf() +\n  guides(fill = \"none\") +\n  ggtitle(\"Derived streams\") +\n  theme_void()\n\ng1 | g2 | g3\n\n\n\n\n\n\n\n\nFigure 3: Three intermediate outputs in watershed delineation\n\n\n\n\n\nThe flow direction raster records in which of the 8 directions out of each cell in the grid surface water will flow. The flow accumulation raster is derived from the flow direction raster and accumulates a count of how many cells are upstream from each cell. Finally, cells above some accumulated flow are designated as streams.\nOne critical layer not shown here is the original DEM with depressions removed. This pre-processing step fills depressions in the DEM which would otherwise end up as sinks where streams would terminate. In the real world, such ‘sinks’ fill up with water, which continues to flow downhill. In effect the static flow direction analysis needs some ‘nudging’ to account for this dynamic process.\nAnyway, all that done, this workflow can identify for a given location or locations the associated upstream watershed.\n\n\nCode\nfence &lt;- st_read(\"fences.gpkg\")\nbasin_centre &lt;- st_read(\"./watersheds/basin-centre.shp\")\nbasin &lt;- st_read(\n  \"./watersheds/watersheds_2025-10-31/Lower Reservoir/Lower Reservoir_drainage_basin.shp\")\n\n\nAnd here it is:\n\n\nCode\ntm_shape(hillshade) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col.legend = tm_legend_hide(), col_alpha = 0.5) +\n  tm_shape(basin) +\n  tm_fill(fill = \"orange\", fill_alpha = 0.35) +\n  tm_shape(streams) + \n  tm_lines(col = \"dodgerblue\") +\n  tm_shape(lake) + \n  tm_fill(fill = \"dodgerblue\") +\n  tm_shape(fence) +\n  tm_lines(col = \"purple\", lwd = 1) +\n  tm_shape(basin_centre) +\n  tm_dots()\n\n\n\n\n\n\n\n\nFigure 4: The output watershed (‘drainage basin’) associated with the point shown at the north end of the lower reservoir.\n\n\n\n\n\nAs is apparent, the Zealandia reserve is more or less congruent with the watershed of the reservoir near its (human6) entrance in the north.\n\n\nHill masses\nOur lodestar table on page 26 of Geographic Information Analysis suggests hill masses as another possible outcome of transforming fields to areas. I am not 100% sure what that means,7 but I am going to go with areas enclosed by a given contour. So…\nThe obvious approach here might be to extract contour lines then, convert those to polygons. Unfortunately, this fails, because within the extent of a given DEM contours may not be closed shapes and so they don’t form proper polygons:\n\n\nCode\nline &lt;- dem |&gt; \n  as.contour(levels = c(250)) |&gt; \n  st_as_sf()\narea &lt;- line |&gt; \n  st_cast(\"LINESTRING\") |&gt; \n  st_cast(\"POLYGON\")\n\ng1   &lt;- ggplot(line) + geom_sf() + theme_void()\ng2   &lt;- ggplot(area) + geom_sf() + theme_void()\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 5: How a contour line fails to yield a proper polygon\n\n\n\n\n\nConceptually, the proper way to resolve this might be by finding the full extent of a contour, and intersecting it with the extent of the DEM. All contours close eventually. Unfortunately, it’s hard to know where they might close and how far away that might be.8 Furthermore, wherever it is, it’s guaranteed to be beyond the scope of our current DEM.\nMore pragmatically, we might attempt to split a rectangular extent polygon of the study area by the contour lines. Using sf this this turns out to be a trickier proposition than you might assume, even if you enroll lwgeom::st_split(). Given just a line, you don’t know which side of the line is inside the larger closed shape of which it is a part, and you end up with a subdivision at any given level of the extent, and the problem of figuring out which subdivisions are above or below the contour. Ultimately the problem here is the artificiality of the study area extent and there’s not a lot we can do about that.9\nFortunately, smarter tools resolve the problem. In QGIS gdal_contour is invoked in the Contour polygons geoprocessing tool and it comes as no surprise to learn there is also a wrapper for this in the R ecosystem in the stars package, which handles this problem with aplomb!10 I’ve tended to avoid stars because generally I don’t need its complexity, and the learning curve is rather steep. But baby steps… here is some code that converts our DEM to a stars object, then extracts the contour bands as polygons.\n\n\nCode\nlevels &lt;- seq(60, 400, 20)\ncontour_bands &lt;- dem |&gt;\n  st_as_stars() |&gt;\n  st_contour(breaks = levels)\n\n\nHere’s what we get:\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_sf(data = contour_bands, aes(fill = Min), colour = NA) +\n  scale_fill_continuous_c4a_seq(palette = \"hcl.terrain2\") +\n  theme_void()\n\ng2 &lt;- ggplot() +\n  geom_sf(data = contour_bands, aes(fill = Max), colour = NA) +\n  scale_fill_continuous_c4a_seq(palette = \"hcl.terrain2\") +\n  theme_void()\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 6: The contour bands each have a lower and upper value\n\n\n\n\n\nThen we can assemble these height bands into regions delineating all the land above each level.\n\n\nCode\nhill_masses &lt;- list()\nlevel_names &lt;- str_c(levels, \"m\")\nfor (i in seq_along(levels)) {\n  hill_masses[[i]] &lt;- \n    contour_bands |&gt;\n    filter(Min &gt;= levels[i]) |&gt;\n    mutate(level = level_names[i])\n}\nhill_masses &lt;- \n  hill_masses |&gt;\n  bind_rows() |&gt;\n  mutate(level = ordered(level, level_names))\n\n\nAnd just for the hell of it, here’s a small multiple map series of ‘hill masses’ at 20m intervals.\n\n\nCode\ntm_shape(hillshade) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col.legend = tm_legend_hide(), col_alpha = 0.5) +\n  tm_shape(hill_masses) +\n  tm_fill(fill = \"#ff000060\") +\n  tm_facets(by = \"level\", ncol = 6) +\n  tm_layout(\n    frame.lwd = 0,\n    panel.label.bg.color = \"white\",\n    panel.label.frame.color = NA,\n    panel.label.frame.lwd = 0\n  )\n\n\n\n\n\n\n\n\nFigure 7: All the hill masses!"
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#to-fields",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#to-fields",
    "title": "GIS, a transformational approach",
    "section": "… To fields",
    "text": "… To fields\nAnd for my last trick11 deriving the vector field from a surface.\nEvery scalar surface has an associated vector field. At any point on the surface we can find the gradient vector, which is oriented downhill in the direction of steepest slope at that location. Derived slope and aspect surfaces can be used here, along with a little bit of trigonometry to create an array of line segments to represent the vector field.\nThe final visualization works better with some smoothing of the source slope, and some of the additional processing operations are sensitive to NA values, so before going further we run that smoothing and trim the datasets to a common size with no NA values.\n\n\nCode\ngauss &lt;- focalMat(dem, 4, \"Gauss\")\ndem    &lt;- dem    |&gt; focal(gauss, mean, expand = TRUE)\nslope  &lt;- dem    |&gt; terrain(unit = \"radians\")\naspect &lt;- dem    |&gt; terrain(v = \"aspect\", unit = \"radians\")\ndem    &lt;- dem    |&gt; crop(ext(dem) + rep(-5, 4))\nslope  &lt;- slope  |&gt; crop(dem)\naspect &lt;- aspect |&gt; crop(dem)\n\n\n\n\nCode\ncellsize &lt;- res(dem)[1]\n\ndf &lt;- slope |&gt; \n  as.data.frame(xy = TRUE) |&gt;\n  bind_cols(aspect |&gt; as.data.frame()) |&gt;\n1  mutate(dx = sin(aspect) * tan(slope),\n         dy = cos(aspect) * tan(slope))\n\nn &lt;- nrow(df)\n2offsets &lt;- rnorm(n, 0, cellsize / 5)\ndf &lt;- df |&gt; \n  mutate(x0 = x + sample(offsets, n) - dx * cellsize * 1.5,\n         x1 = x + sample(offsets, n) + dx * cellsize * 2.5,\n         y0 = y + sample(offsets, n) - dy * cellsize * 1.5,\n         y1 = y + sample(offsets, n) + dy * cellsize * 2.5)\n\nvecs &lt;- df |&gt;\n  select(x0, x1, y0, y1) |&gt;\n  apply(1, matrix, ncol = 2, simplify = FALSE) |&gt;\n  lapply(st_linestring) |&gt;\n  st_sfc() |&gt;\n  as.data.frame() |&gt;\n  st_sf(crs = 2193) |&gt;\n  bind_cols(df |&gt; select(slope))\n\n\n\n1\n\nThe usual mapping of cosine to x-component and sine to y-component breaks down here because aspect is expressed as azimuth which starts at north for 0° and proceeds clockwise, whereas mathematical angles start at east for 0° and proceed counter-clockwise.\n\n2\n\nThis is to add a little bit of jitter to the vectors to reduce the visual impact of them being arranged in a strict grid.\n\n\n\n\n\n\nCode\ncxy &lt;- dem |&gt;\n  ext() |&gt;\n  matrix(ncol = 2) |&gt; \n  apply(2, mean)\n\nggplot() +\n  geom_sf(data = vecs |&gt; filter(slope &gt; 0.1),\n          aes(linewidth = slope / 2)) +\n  scale_linewidth_identity() +\n  coord_sf(xlim = cxy[1] + c(-500, 500),\n           ylim = cxy[2] + c(-500, 500)) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 8: The vector field associated with this terrain, which is a crude hachure map\n\n\n\n\n\nHachure mapping has been having a moment in the last few years, and this is certainly not a map that advances developments in that art, although it does show how the idea works. The most obvious way to improve it would to be a bit more selective about which field lines to show and thin things out accordingly. That’s a big focus of the method just linked.\nIt’s fitting to wrap up this sequence of posts based on a table in Geographic Information Analysis co-authored with Dave Unwin with such a map. Dave has always insisted on the usefulness of the vector field associated with any scalar field, and way back in 1981 in Introductory Spatial Analysis12 he suggests that\n\na map of the vector field will always give a moderately good visualization of the surface relief and is the essence of the method of hachures used in early relief maps.13\n\nFor my crude example ‘moderately good’ would be a generous assessment, but hopefully it demonstrates the value of the vector field perspective on the more familiar scalar fields."
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#et-fin-for-now",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#et-fin-for-now",
    "title": "GIS, a transformational approach",
    "section": "Et fin… for now",
    "text": "Et fin… for now\nSo that’s it for posts about a transformational approach to GIS and also for the supplementary material to Chapter 1 of Geographic Information Analysis. Next in that larger series will be material related to Chapter 2 ‘The Pitfalls and Potential of Spatial Data’.\nMeanwhile, never forget: everything’s better with a bird on it."
  },
  {
    "objectID": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#footnotes",
    "href": "posts/2025-11-03-gia-chapter-1B-part-4B/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe collective noun for kākā is allegedly a hoon—at least according to this podcast, and this website—although the former feels like wry comedy, and the latter doesn’t find much support anywhere else on the internet. More generally hoon, in New Zealand and Australia, according to the OED means “n lout or idiot’; v behave like a hoon”. The last thing kākā could be accused of being is idiotic, although like their more celebrated brethren the kea, while smart, they often hang around in groups, and look like they are up to no good. Seeing them in large groups in inner suburban Wellington is a blessing we largely owe to Zealandia.↩︎\nSee e.g., dissolve↩︎\nPage 537, Pal D, S Saha, A Mukherjee, P Sarkar, S Banerjee and A Mukherjee. 2025. GIS-Based Modeling for Water Resource Monitoring and Management: A Critical Review. Pages 537-561 in SC Pal and U Chatterjee (eds) Surface, Sub-Surface Hydrology and Management Application of Geospatial and Geostatistical Techniques. Springer.↩︎\nPage 38, Sui DZ and RC Maggio. 1999. Integrating GIS with hydrological modeling: practices, problems, and prospects. Computers, Environment and Urban Systems 23 33-51.↩︎\nBoo! Hiss! See this post.↩︎\nBirds can come and go as they please.↩︎\nKeep in mind that the book is co-authored…↩︎\nAs a regular walker in Wellington’s hilly terrain, I can attest to the truth of this comment.↩︎\nInterestingly, a related issue arises when we deal with large polygons on the Earth’s spherical surface, where every line defines two polygons. When the polygons are small, it’s perhaps obvious which one we mean, but when they are large it’s less clear. See this notebook for more on the problem on the sphere.↩︎\nTIL…↩︎\nDrum roll please!↩︎\nTo which Geographic Information Analysis is effectively an expanded sequel.↩︎\nPage 156 in Unwin D. 1981. Introductory Spatial Analysis. Methuen. See also the rest of Chapter 6.↩︎"
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html",
    "title": "30 Day Map Challenge 2025",
    "section": "",
    "text": "On Day 3 of this year’s 30 Day Map Challenge Sam Keast posted a nice Tanaka illuminated contours map, which he made in Arc loosely follow a John Nelson video.\nIn a comment I mentioned how much I’ve always loved this method of terrain representation ever since encountering it during my Masters in Glasgow back in 1996-7. I liked it so much that I leaned into it hard for one coursework assignment to make a tourist promotional map of a chosen location. Donegal being very much our go-to family holiday spot I chose the area around Sheephaven Bay and the Rosguill Peninsula for my subject matter. That meant that I had to manually digitise contours from a 1:50,000 topographic map of the area, and then mock up illuminated contours in CorelDraw. The map seems now to have been lost, although I dimly remember seeing it the last time we moved house, but can’t right now locate it. Never mind, if you want to know why my efforts didn’t score especially well see below.\nThat was a long time ago, and I’m glad to say things have got a lot easier…\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(stars)\nlibrary(ggplot2)\nlibrary(cols4all)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#about-tanakas-illuminated-contours",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#about-tanakas-illuminated-contours",
    "title": "30 Day Map Challenge 2025",
    "section": "About Tanaka’s illuminated contours",
    "text": "About Tanaka’s illuminated contours\nIn short, illuminated contours are an elegant way of combining contours and hillshading to represent surfaces.\nThe method was proposed in a paper by Kitirô Tanaka in 1950.1 Terrain is considered to be lit (as in a hillshaded map) from some direction. The idea is to draw contours on a grey (or other mid-tone) background with black lines on dark slopes facing away from the light source, and white lines on illuminated slopes.\nLine thickness is also varied with the aspect of the slope relative to the light source. Thick white lines are used on slopes facing directly towards the illumination, and thick black lines on slopes facing directly away from the light source, line thickness narrowing as slopes progressively turn to lie parallel to the direction of illumination. An example for a simple circular contour is seen in the next section if this is unclear.\nWhen this idea was proposed, it required a steady hand. Tanaka noted that, “One may reasonably doubt whether such seemingly complicated contour lines can be easily and quickly drawn”, before going on to reassure readers that\n\nA simple and exact method is available […]. If the tip of a drawing pen […] is filed down so that its thickness is the maximum thickness required in a contour line, and if […] the pen-point edge is kept at a fixed orientation […] parallel to the assumed horizontal direction of the incident light, then the thickness of the contour line will vary with the cosine of the angle \\(\\theta\\). (1950, page 449)\n\nI would venture to suggest that this was much harder to do than it was to describe! I spent some time in my Masters study using ink pens to make maps by hand, and it was an exacting business even to reliably render lines of uniform thickness, never mind varying them in a consistent way as proposed for this technique. The technical challenge goes a long way to explaining why the method has remained relatively rarely used until recently."
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#a-crude-diy-implementation",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#a-crude-diy-implementation",
    "title": "30 Day Map Challenge 2025",
    "section": "A crude DIY implementation",
    "text": "A crude DIY implementation\nFortunately for the unsteady of hand we can now easily approximate Tanaka’s method.\nTo illustrate my approach, I’ve made a single circular contour, and two copies, one offset southeast, and the other offset northwest.\n\ncircle &lt;- st_point(c(0, 0)) |&gt; \n  st_sfc() |&gt; \n  data.frame() |&gt;\n  st_as_sf(crs = 2193) |&gt;\n  st_buffer(1000)\n  \ncircle_se &lt;- circle |&gt; \n  mutate(geometry = geometry + c(50, -50)) |&gt;\n  st_set_crs(st_crs(circle))\n  \ncircle_nw &lt;- circle |&gt; \n  mutate(geometry = geometry + c(-50, 50)) |&gt;\n  st_set_crs(st_crs(circle))\n\nNow, layer them on top of one another southeast, centred, and northwest, in that order, coloured black, grey, and white, respectively.\n\nggplot() +\n  geom_sf(data = circle_se, fill = \"black\", colour = NA) +\n  geom_sf(data = circle_nw, fill = \"white\", colour = NA) +\n  geom_sf(data = circle, fill = \"grey\", colour = NA) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"darkgrey\", colour = NA))\n\n\n\n\n\n\n\nFigure 1: A single illuminated contour showing the principle behing the method: this contour is illuminated from the northwest\n\n\n\n\n\nThe effect is striking and in this crude implementation not at all hard to code. I say crude not because this approach is inaccurate but because more recent work has suggested further refinements of the method such that either the colour or thickness of contour lines varies with their facing towards the light source.2 In terms of Tanaka’s original suggestion my simple approach produces lines of the correct thickness, in this fixed case for a light source from the northwest.\n\nAssembling some reusable code\nWith the basis of an approach established we can assemble some functions to produce Tanaka contoured maps.\nFirst, it’s convenient to generate from a DEM a set of evenly spaced contour levels, and given those levels to use stars::st_contour to get contour polygons.\n\n# label: functions-to-get-polygons\nround_to_nearest &lt;- function(x, y) round(x / y) * y\n\ncontour_levels &lt;- function(dem, interval) {\n  range_z &lt;- range(values(dem), na.rm = TRUE) |&gt; round_to_nearest(interval)\n  seq(range_z[1]- interval, range_z[2] + interval, interval)\n}\n\nget_contour_polygons &lt;- function(dem, levels) {\n  dem |&gt; \n    st_as_stars() |&gt; \n    st_contour(breaks = levels) |&gt;\n    st_as_sf()\n}\n\nNext a function to shift supplied contour polygons by some x and y offsets. This uses the constantly surprising functionality in sf where you can simply add a vector to the geometry attribute of an sf dataframe.\n\noffset_shapes &lt;- function(shapes, dx, dy) {\n  shapes |&gt;\n    mutate(geometry = geometry + c(dx, dy)) |&gt;\n    st_set_crs(st_crs(shapes))\n}\n\nNext we need some code to render the various layers in the right order starting at the bottom of the stack of contour polygons, and at each level going through them in a consistent order.\nThe addition of layer and group attributes to the data enables the rendering order to be controlled. ggplot renders data by group, in the order that elements appear in the table, so we use arrange to sort in ascending order of Min (the elevation) and layer. We also assign all the data to a single group group = 0, and include that as an aesthetic in the call to ggplot, so that the data table sort order rendering isn’t overridden by grouping on the layer attribute.\n\nmy_tanaka_contours &lt;- function(dem, interval, thickness, direction = 135,\n                               colour = \"grey\", light = \"white\", dark = \"black\") {\n  levels &lt;- contour_levels(dem, interval)\n  contours &lt;- get_contour_polygons(dem, levels) |&gt; \n    mutate(layer = 3)\n  angle &lt;- direction * pi / 180\n  dx &lt;- cos(angle) * thickness\n  dy &lt;- sin(angle) * thickness\n  contours_light &lt;- contours |&gt; \n    offset_shapes(dx, dy) |&gt; \n    mutate(layer = 2)\n  contours_dark &lt;- contours |&gt; \n    offset_shapes(-dx, -dy) |&gt; \n    mutate(layer = 1)\n  all_contours &lt;- bind_rows(contours, contours_light, contours_dark) |&gt;\n    mutate(layer = ordered(layer, levels = 1:3), group = 0) |&gt;\n    arrange(Min, layer)\n  ggplot(all_contours) +\n    geom_sf(aes(fill = layer, group = group), colour = NA) +\n    scale_fill_manual(values = c(dark, light, colour)) +\n    guides(fill = \"none\") +\n    coord_sf(expand = FALSE) + \n    theme_void()\n}"
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#data-from-mars",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#data-from-mars",
    "title": "30 Day Map Challenge 2025",
    "section": "Data from Mars",
    "text": "Data from Mars\nSince I’m smuggling this in under the guise of Day 18 ‘Out of this world’, we need some out of this world data, so I found some Mars surface topography. Specifically, this dataset which is a high resolution digital elevation model of a crater,3 derived from Mars Reconnaissance Orbiter data by a team at the University of Arizona.\n\ndem_raw &lt;- rast(\"https://astrogeo-ard.s3-us-west-2.amazonaws.com/mars/mro/hirise/controlled/dtm/ESP_050125_1480_ESP_050046_1480/DTEED_050125_1480_050046_1480_G01.tif\")\n\nThe raw data is in an equirectangular cylindrical coordinate reference system, which means it’s stretched east-west because just like on Earth meridians converge toward the poles, so we should reproject to something conformal or at least more sensible local to the area of interest. Mercator gets a lot of stick, but this is local data so it’s fine. The key thing here is to specify a spheroid that is Mars-sized, hence +R=3396190. I am also aggregating the data five-fold because at ~2 meter resolution it’s more detailed than we need (and some smoothing does no harm for this method.\n\nproj &lt;- \"+proj=merc +lat_0=-31.46 +lon_0=-22.9 +k=1 +R=3396190 +units=m\"\ndem &lt;- dem_raw |&gt; \n  project(proj) |&gt;\n  terra::aggregate(5)\nnames(dem) &lt;- \"z\"\n\nAnd so to a couple of maps. I’ve tried one using the default grey-black-white colouring, and a second using a more Martian colour scheme.\n\ng1 &lt;- my_tanaka_contours(dem, 20, 20)\ng2 &lt;- my_tanaka_contours(dem, 20, 20, colour = \"#ff9020\",\n                         light = \"#f8e078\", dark = \"#B04030\")\ng1 | g2\n\n\n\n\n\n\n\nFigure 2: Two Mars crater maps using illuminated contours, one of them (perhaps) in more Mars-appropriate colours"
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#theres-a-package-for-that",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#theres-a-package-for-that",
    "title": "30 Day Map Challenge 2025",
    "section": "There’s a package for that",
    "text": "There’s a package for that\nI didn’t follow my own advice here and went ahead and wrote my own code before checking if there was already an R package out there that does Tanaka contours. Of course there is.4 In fact, there are two, metR and tanaka. Because the former sticks with ggplot2 style plotting I show it here.\n\nlibrary(metR)\n\ndem_xy &lt;- dem |&gt;\n  as.data.frame(xy = TRUE)\n\nggplot(dem_xy) + \n  geom_raster(aes(x = x, y = y), fill = \"grey\") +\n  geom_contour_tanaka(\n    aes(x = x, y = y, z = z),\n    binwidth = 20, sun.angle = 45) + \n  coord_equal() + \n  theme_void() \n\n\n\n\n\n\n\nFigure 3: A basic Tanaka contour map using metR\n\n\n\n\n\nWhat’s nice about this package is that I can also colour the contours using a hypsometric colour ramp. After some experimentation (see the commented out alternatives), I decided the hcl.heat2 ramp was the most ‘Martian-looking’.\n\nggplot(dem_xy) + \n  geom_raster(aes(x = x, y = y, fill = z)) +\n  scale_fill_continuous_c4a_seq(palette = \"hcl.heat2\") +\n  # scale_fill_continuous_c4a_seq(palette = \"hcl.terrain2\") +\n  # scale_fill_continuous_c4a_seq(palette = \"-tableau.orange\") +\n  # scale_fill_continuous_c4a_seq(palette = \"met.greek\") +\n  guides(fill = \"none\") +\n  geom_contour_tanaka(\n    aes(x = x, y = y, z = z), \n    binwidth = 20, sun.angle = 45) + \n  coord_equal() + \n  theme_void()\n\n\n\n\n\n\n\nFigure 4: A Tanaka contour map using metR with the contour intervals hypsometrcally coloured\n\n\n\n\n\nThe tanaka package functions can also do this, although the API in that case is less clean."
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#for-old-times-sake",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#for-old-times-sake",
    "title": "30 Day Map Challenge 2025",
    "section": "For old time’s sake…",
    "text": "For old time’s sake…\n… in about one-hundredth of the time it took way back when, I thought I’d’ revisit the map from my Masters coursework, or at least the terrain representation part of it.\nSo I grabbed a relevant portion of the Continental Europe DTM from OpenTopography5 and ran it through metR’s geom_contour_tanaka.\nThe colours I’ve used are from memory and I know my choices were rather bold. Keep in mind that I also had to overlay symbology, placenames, and so on. It was also supposed to be a promotional map, not a tasteful topographic map. Even so, I think I went a bit overboard.\n\n\nCode\ndonegal &lt;- rast(\"sheephaven.tif\") |&gt;\n  as.data.frame(xy = TRUE)\n\nggplot(donegal) +\n  geom_raster(aes(x = x, y = y), fill = \"#209030\") +\n  geom_contour_tanaka(aes(x = x, y = y, z = donegal), binwidth = 50,\n                      sun.angle = 45) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#3090e0\"))\n\n\n\n\n\n\n\n\nFigure 5: A mock up of the terrain elements of that tourism map of Donegal\n\n\n\n\n\nThe challenge of illuminated contours is finding a base colour that allows both the light and dark contours to show up well, and I recall experimenting endlessly with that in this design, every time having to print it out to figure out if it was working or not.\nI can’t help but wonder if I’d been able to hypsometrically tint my contours back in 1996 if I’d have done a bit better on that coursework assignment.\n\n\nCode\nggplot(donegal) +\n  geom_raster(aes(x = x, y = y, fill = donegal)) +\n  scale_fill_continuous_c4a_seq(palette = \"hcl.terrain2\") +\n  guides(fill = \"none\") +\n  geom_contour_tanaka(aes(x = x, y = y, z = donegal), binwidth = 50,\n                      sun.angle = 45) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#3090e0\"))\n\n\n\n\n\n\n\n\nFigure 6: This time with terrain colours"
  },
  {
    "objectID": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#footnotes",
    "href": "posts/2025-11-18-tanaka-30-day-maps-2025-day-18/index.html#footnotes",
    "title": "30 Day Map Challenge 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTanaka K. 1950. The relief contour method of representing topography on maps. Geographical Review 40(3) 444-456.↩︎\nKenelly P and AJ Kimerling. 2001. Modifications of Tanaka’s illuminated contour method. _Cartography and Geographic Information Science 28(2) 111-123.↩︎\nMars has a lot of craters. According to wikipedia, hundreds of thousands greater than 1 km in diameter.↩︎\nDuh.↩︎\nHengl T, L Leal Parente, J Krizan and C Bonannella. 2022. Continental Europe Digital Terrain Model. Distributed by OpenTopography. https://doi.org/10.5069/G99021ZF. Accessed 2025-11-09↩︎"
  },
  {
    "objectID": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "href": "posts/2021-10-30-locations-of-interest/locations-of-interest.html",
    "title": "Locations of interest in 2021 delta outbreak",
    "section": "",
    "text": "For a time, during the August 2021 COVID outbreak that started in Auckland but eventually led to the end of New Zealand’s attempt to stop COVID completely, I continuously updated the above kepler.gl animated map of ‘locations of interest’. Kepler.gl is a nice tool which I’d recommend for making quick visualizations like this.\n\nThere was something oddly compelling about the mundane details of the reported locations of interest.\nGo to the viewer\nLike most of the country I got pretty fed up doing it in the end, so I stopped… for whatever it’s worth, looking at the timeline on the visualization the delta outbreak did get reigned in eventually, but it it was pretty clear that the mood of the country had changed and lockdowns weren’t going to stick for much longer."
  },
  {
    "objectID": "posts/2025-08-31-gia-chapter-1A/index.html",
    "href": "posts/2025-08-31-gia-chapter-1A/index.html",
    "title": "Raster really is faster",
    "section": "",
    "text": "I’ve been asked relatively often when or if there will be a third edition of Geographic Information Analysis. Notwithstanding how happy it makes me (and Dave Unwin) to know that people find that book valuable, it’s sadly unlikely. Dave has retired, my relationship to academia is not what it was, and there’s also the aggravation associated with Wiley holding the copyright. Dave and I think that their approach to books like this is out of date, not to mention somewhat predatory price-wise, and we’d like to be able to offer a cheaper (perhaps even free) product,1 but Wiley won’t surrender the copyright and we’re stuck.\nAs a limited alternative, I thought I’d write some posts providing code implementing ideas from the book. Robert Hijmans has already done this to a large extent but there’s no harm in repetition, and I expect I’ll wind up presenting different perspectives (or at the very least datasets).\nIn Chapter 1 Geographic Information Analysis and Spatial Data, in a section called Objects Might Have Fractal Dimension2 we presented fractal dimension as one instance of the scale dependence of spatial data. Herewith, some R code exploring that idea3 and incidentally, as the title implies, showing some of the differences between vector and raster data."
  },
  {
    "objectID": "posts/2025-08-31-gia-chapter-1A/index.html#measuring-fractal-dimension",
    "href": "posts/2025-08-31-gia-chapter-1A/index.html#measuring-fractal-dimension",
    "title": "Raster really is faster",
    "section": "Measuring fractal dimension",
    "text": "Measuring fractal dimension\nWe need some packages - the usual suspects, really.\n\n\nCode\nlibrary(sf)\nlibrary(units)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\n\n\nSince fractal dimension is determined from a log-log relationship it is convenient to be able to generate sequences like 1, 2, 5, 10, 20, 50,… and so on, so here are a couple of functions for doing just that.\n\n\nCode\nround_to_1_2_5 &lt;- function(x, offset = 0) {\n  offset &lt;- sign(offset)\n  decade &lt;- c(1, 2, 5) * 10 ^ floor(log10(x))\n  idx &lt;- sum(x &gt;= decade)\n  if (idx == 3 & offset ==  1) return(decade[1] * 10)\n  if (idx == 1 & offset == -1) return(decade[3] / 10)\n  decade[idx + offset]\n}\n\nget_decades &lt;- function(low = 1, high = 1000, \n                        offsets = c(0, 0)) {\n  lo &lt;- round_to_1_2_5(low, offsets[1])\n  hi &lt;- round_to_1_2_5(high, offsets[2])\n  steps &lt;- c(1, 2, 5)\n  decades &lt;- c()\n  for (decade in floor(log10(lo)):ceiling(log10(hi))) {\n    decades &lt;- c(decades, steps * 10 ^ decade)\n  }\n  decades[between(decades, lo, hi)]\n}\n\n\n\nThe measurement process\nNext up, the main event, at least in vector-land.\nFractal dimension reflects how the apparent length of a line (or area of a region) seems to increase the more closely we look. In practical terms in the case of measuring the length of (as here) coastlines, this manifests as follows. Take a measuring stick of some ‘resolution’ (i.e. length), say 1km, repeatedly fit it to the coastline, and count how many times it fits. A long measuring stick will miss lots of nooks and crannies. A shorter one will fit into many more, and so, as the size of your measuring stick shrinks, the measured length of the coastline will grow.\nIt turns out that the relationship between measuring stick length, \\(m\\), and the number of times it will fit the line you are measuring, \\(N\\) scales according to \\(\\log{N}\\propto\\log{m}\\), and the ratio of the logarithms of those two values, is the fractal dimension, \\(D\\), that is\n\\[\n\\log{N}=-D\\log{m}+c\n\\] In other words, the negative slope of a log-log plot showing the relationship between \\(N\\) and \\(m\\) gives us the fractal dimension. The more ‘crinkly’ a line the more rapidly \\(N\\) will increase as \\(m\\) shrinks. In the extreme case of a space-filling curve,4 \\(D=2\\) and the line is in effect an area!\nAnyway, to return to the issue at hand, the function below identifies the ‘measuring points’ at which a yardstick of some specified length will touch a supplied line. Hover over the circled numbers at the side for details.\n\n\nCode\n1get_measuring_pts &lt;- function(shape, yardstick = 1e3) {\n  next_point &lt;- shape |&gt; \n    st_cast(\"POINT\") |&gt; \n2    slice(1)\n3  measuring_pts &lt;- list(next_point)\n  next_position &lt;- 0\n  last_position &lt;- -1\n4  while (next_position &gt; last_position) {\n    last_position &lt;- next_position\n5    buffer_line &lt;- next_point |&gt;\n      st_buffer(yardstick) |&gt; \n      st_boundary()\n6    next_points = buffer_line |&gt;\n      st_intersection(shape) |&gt;\n      st_cast(\"POINT\")\n7    next_positions = st_line_project(\n      shape |&gt; st_as_sfc(),\n      next_points |&gt; st_as_sfc(),\n      normalized = TRUE)\n8    if (any(next_positions &gt; last_position)) {\n      next_position = min(next_positions[\n        which(next_positions &gt; last_position)])\n9      next_point &lt;- next_points |&gt;\n        slice(which(next_positions == next_position))\n      measuring_pts[[length(measuring_pts) + 1]] &lt;- next_point\n    } else { break }\n  }\n  measuring_pts |&gt; \n    lapply(data.frame) |&gt; \n    bind_rows() |&gt; \n10    st_as_sf(crs = st_crs(shape))\n}\n\n\n\n1\n\nshape is an sf dataset containing a single LINESTRING geometry, and yardstick is the length of the measuring stick—its resolution.\n\n2\n\nWe start from the first point in the line.\n\n3\n\nInitialise a list to record the points at which the yardstick touches the line with the first point.\n\n4\n\nnext_position and last_position are how far along the line measurement has progressed. For a closed line, when next_position is less than last_position we’ve looped back around to the start and are done.\n\n5\n\nUse a linear buffer, not a polygon, so that the intersections with the line are a set of points.\n\n6\n\nGet all the potential next points: these are where the line buffer intersects the shape.\n\n7\n\nGet how far along the shape the next points are.\n\n8\n\nIf none of the next positions are further along the shape than the last position, then we are done.\n\n9\n\nPick the closest intersection point as the next point and add it to the list of measuring points.\n\n10\n\nConvert the list of points to an sf dataset, and return.\n\n\n\n\nWe also need a function to convert the points to a polygon.\n\n\nCode\nget_approx_polygon &lt;- function(mpts) { \n  mpts |&gt; \n    st_combine() |&gt;\n    st_cast(\"LINESTRING\") |&gt;\n    st_cast(\"POLYGON\") |&gt;\n    as.data.frame() |&gt; \n    st_as_sf()\n}\n\n\n\n\nA test of the measurement method\nNow, if we load up some data we can see what this looks like.\n\n\nCode\nnz &lt;- st_read(\"nz-large.gpkg\") \n\nnz_no_holes &lt;- nz |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  mutate(geom = st_exterior_ring(geom))\n\nnz_largest_islands &lt;- nz_no_holes |&gt;\n  mutate(area = st_area(geom) |&gt; drop_units()) |&gt;\n  arrange(desc(area)) |&gt;\n  select()\n\nwaiheke &lt;- nz_largest_islands |&gt; \n  slice(8) |&gt;\n  st_cast(\"LINESTRING\")\n\n\nThe eighth largest of the Aotearoan islands is Waiheke, and for it, a resolution of 1km provides a reasonable test of the yardstick measurement procedure.\n\nCode\nyardstick &lt;- 1000\nmeasuring_pts &lt;- get_measuring_pts(waiheke, yardstick)\napproximate_island &lt;- get_approx_polygon(measuring_pts)\n\nggplot() +\n  geom_sf(data = waiheke |&gt; st_cast(\"POLYGON\"), \n          fill = \"#cccccc\", colour = NA) +\n  geom_sf(data = approximate_island,\n          fill = \"NA\", colour = \"blue\", lwd = 0.65) +\n  geom_sf(data = measuring_pts,\n          colour = \"red\") +\n  geom_sf(data = measuring_pts |&gt; st_buffer(yardstick),\n          fill = NA, colour = \"red\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Illustrating the method applied to Waiheke\n\n\n\nHopefully, the figure shows how measuring the coastline at a single resolution works. Starting at some point (one of the red dots). We repeatedly find the closest point further around the coastline that is 1km distant. Counting up how many times we can do this gives an estimate at the current resolution of the length of the coastline, and also gives us the number \\(N\\) of yardsticks at this resolution."
  },
  {
    "objectID": "posts/2025-08-31-gia-chapter-1A/index.html#estimating-fractal-dimension",
    "href": "posts/2025-08-31-gia-chapter-1A/index.html#estimating-fractal-dimension",
    "title": "Raster really is faster",
    "section": "Estimating fractal dimension",
    "text": "Estimating fractal dimension\n\nMethod 1: using several measuring sticks\nHaving established that the method works, we can use it to assemble a series of measurements and estimate the fractal dimension. For this, we’ll use the North Island, Te Ika-a-Māui. We also set up a series of yardsticks to apply.\n\n\nCode\nte_ika_a_maui &lt;- nz_largest_islands |&gt; \n  slice(2) |&gt;\n  st_cast(\"LINESTRING\")\nyardsticks &lt;- get_decades(5000, 200000)\n\n\nNow, we loop over the resolutions, and assemble the resulting approximating polygon, along with the number of contact points in the measurement process.\n\n\nCode\ndf &lt;- yardsticks |&gt; \n1  lapply(get_measuring_pts, shape = te_ika_a_maui) |&gt;\n2  lapply(get_approx_polygon) |&gt;\n  bind_rows() |&gt;\n  mutate(yardstick = yardsticks,\n         yardstick_name = ordered(\n          str_c(yardstick / 1000, \" km\"),\n          levels = str_c(yardstick / 1000, \" km\")),\n3         coastline = geometry |&gt;\n           st_boundary() |&gt; \n           st_length() |&gt; \n           drop_units(),\n4         n = coastline / yardstick)\n\n\n\n1\n\nMake a list by applying the get_measuring_pts function to the set of yardstick lengths.\n\n2\n\nConvert to approximate polygons (for mapping later).\n\n3\n\nSince the measuring stick doesn’t fit an exact number of times, we may as well measure the exact coastline length…\n\n4\n\n… and the exact (non-integer) number of measuring sticks.\n\n\n\n\nThe linear fit between the log of the number of yardsticks and the log of the yardstick length is easily modelled, and the negative coefficient is our estimate of the fractal dimension, which in this case comes to 1.203. A fractal dimension of around 1.2 is about what we would expect for an extensive coastline like this, with some very crenellated sections and other relatively smooth stretches.\n\n\nCode\nlm(log10(n) ~ log10(yardstick), data = df)\n\n\n\nCall:\nlm(formula = log10(n) ~ log10(yardstick), data = df)\n\nCoefficients:\n     (Intercept)  log10(yardstick)  \n           7.336            -1.203  \n\n\nA log-log plot makes the relationship clear.\n\nCode\ny_breaks &lt;- get_decades(min(df$n), max(df$n), c(-1, 1))\nggplot(df, aes(x = log10(yardstick), y = log10(n))) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  scale_x_continuous(breaks = log10(yardsticks),\n                     labels = yardsticks) +\n  xlab(\"Yardstick length, km\") +\n  scale_y_continuous(breaks = log10(y_breaks),\n                     labels = y_breaks) +\n  ylab(\"Number of yardsticks\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The relationship between measuring stick length and the number of measuring sticks for Te Ika-a-Māui.\n\n\n\nWe can see how the measurement process changes, and why the long yardsticks underestimate badly in the sequence of maps below.\n\nCode\nggplot() +\n  geom_sf(data = te_ika_a_maui |&gt; st_cast(\"POLYGON\"), colour = NA) +\n  geom_sf(data = df, lwd = 0.5) +\n  facet_wrap( ~ yardstick_name, ncol = 3) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Map sequence for the six coastline measurements on which the estimated fractal dimension is based.\n\n\n\n\n\nMethod 2: raster is faster\nThe above is all very well, but OMG it is sloooooowwwwwwww… A much quicker approach is possible using a raster method.\nWe replace the measuring stick with raster cells, and simply count how the number of cells that the coastline passes through changes with the resolution of the cells. While the code below converts the raster representation to an xy-table, this is only done to make it easier to produce the small multiple map in ggplot.\n\n\nCode\nlibrary(terra)\n\nsv &lt;- te_ika_a_maui |&gt; as(\"SpatVector\")\nyardsticks2 &lt;- get_decades(200, 200000)\nsums &lt;- c()\nrasts &lt;- list()\nfor (res in yardsticks2) {\n  r &lt;- sv |&gt;\n    rasterize(rast(sv, res = res)) |&gt;\n    terra::as.data.frame(xy = TRUE) |&gt;\n    mutate(resolution = res,\n           resolution_label = ordered(\n             str_c(resolution / 1000, \" km\"),\n             levels = str_c(yardsticks2 / 1000, \" km\")))\n  rasts[[length(rasts) + 1 ]] &lt;- r\n  sums &lt;- c(sums, r |&gt; pull(3) |&gt; sum())\n}\ndf2 &lt;- data.frame(yardstick = yardsticks2, n = sums)\nlm(log(n) ~ log(yardstick), data = df2)\n\n\n\nCall:\nlm(formula = log(n) ~ log(yardstick), data = df2)\n\nCoefficients:\n   (Intercept)  log(yardstick)  \n        17.274          -1.224  \n\n\nNote that because this method is much faster,5 we can extend the range of resolutions used to a lower value of only 200m without difficulty (lower still would be fine too, although you would then struggle to see the resulting rasterized maps of the coastline at all).\n\nCode\ny_breaks &lt;- get_decades(min(df2$n), max(df2$n), c(-1, 1)) \nggplot(df2, aes(x = log10(yardstick), y = log10(n))) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  scale_x_continuous(breaks = log10(yardsticks2),\n                     labels = yardsticks2 / 1000) +\n  xlab(\"Yardstick length, km\") +\n  scale_y_continuous(breaks = log10(y_breaks),\n                     labels = y_breaks) +\n  ylab(\"Number of yardsticks\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Log-log plot of results from the raster-based method.\n\n\n\nThe resulting estimate is similar to that produced by the vector method. It’s not at all obvious which is ‘more correcter’.6 We can also get a map sequence of the progressive degradation of the coastline measurement as the raster resolution coarsens.\n\nCode\nxy_df &lt;- bind_rows(rasts)\nggplot(xy_df) +\n  geom_tile(aes(x = x, y = y,\n                width = resolution, height = resolution)) +\n  coord_sf() +\n  facet_wrap( ~ resolution_label, ncol = 5) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Map sequence for the raster-based estimation method\n\n\n\n\n\nMethod 3: using a vector (hex) grid\nThis section is by way of an addendum really. The success of the raster method got me wondering if an alternative vector grid method might be quicker than the original approach. The answer is… it works, and it is quicker than the first vector method above, but it’s still a lot slower than the raster method.\nIt’s nice that it’s easy to make the grid hexagon-based, which might assuage the concerns of those worried about the different distances diagonal and face-to-face across square grid cells.\n\n\nCode\nyardsticks3 &lt;- get_decades(5000, 200000)\ncounts &lt;- c()\nhex_sets &lt;- list()\nfor (yardstick in yardsticks3) {\n  hex_centres &lt;- te_ika_a_maui |&gt;\n1    st_make_grid(cellsize = yardstick,\n                 square = FALSE,\n                 what = \"centers\") |&gt;\n    st_as_sf() |&gt;\n    st_filter(te_ika_a_maui,\n              .predicate = st_is_within_distance,\n2              dist = yardstick / 2) |&gt;\n    mutate(cellsize = yardstick,\n           cellsize_label = ordered(\n             str_c(cellsize / 1000, \" km\"),\n             levels = str_c(yardsticks3 / 1000, \" km\")))\n  hex_sets[[length(hex_sets) + 1]] &lt;- hex_centres\n  counts &lt;- c(counts, dim(hex_centres)[1])\n}\ndf3 &lt;- data.frame(yardstick = yardsticks3, n = counts)\nlm(log(n) ~ log(yardstick), data = df3)\n\n\n\n1\n\nsquare = FALSE means the grid is hex-based, and what = \"centers\" that we get cell centres not the polygons.\n\n2\n\nNote that the spatial filter is based on half the distance between the centres, cells with centres within that distance of the coastline will intersect it.\n\n\n\n\n\nCall:\nlm(formula = log(n) ~ log(yardstick), data = df3)\n\nCoefficients:\n   (Intercept)  log(yardstick)  \n        16.983          -1.187  \n\n\nAgain, a similar estimate. And here are the corresponding log-log plot and map sequences.\n\nCode\ny_breaks &lt;- get_decades(min(df3$n), max(df3$n), c(-1, 1))\nggplot(df3, aes(x = log10(yardstick), y = log10(n))) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  scale_x_continuous(breaks = log10(yardsticks3),\n                     labels = yardsticks3 / 1000) +\n  xlab(\"Yardstick length, km\") +\n  scale_y_continuous(breaks = log10(y_breaks),\n                     labels = y_breaks) +\n  ylab(\"Number of yardsticks\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Log-log plot of the hex grid centres based method.\n\n\n\n\nCode\ncentres_df &lt;- bind_rows(hex_sets)\nggplot() +\n  geom_sf(data = te_ika_a_maui |&gt; st_cast(\"POLYGON\"), colour = NA) +\n  geom_sf(data = centres_df, size = 0.2) +\n  facet_wrap( ~ cellsize_label) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Map series of the hex grid centres based method."
  },
  {
    "objectID": "posts/2025-08-31-gia-chapter-1A/index.html#a-closing-thought",
    "href": "posts/2025-08-31-gia-chapter-1A/index.html#a-closing-thought",
    "title": "Raster really is faster",
    "section": "A closing thought",
    "text": "A closing thought\nThis method makes me think that perhaps one way to go here might be counting discrete global grid cells along a line at a series of resolutions, although discrete global grid cells are not strictly equal in size and that might offend purists. A very cursory search suggests I am not the first person to think of this. If you are interested, see this paper:\n\nGhosh P. 2025. Measuring fractal dimension using discrete global grid systems. arXiv:2506.18175\n\nIt’s only been on arXiv a couple of months, so I’m not that far off the pace!"
  },
  {
    "objectID": "posts/2025-08-31-gia-chapter-1A/index.html#footnotes",
    "href": "posts/2025-08-31-gia-chapter-1A/index.html#footnotes",
    "title": "Raster really is faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNobody ever got rich writing niche textbooks so it’s no real sacrifice for us.↩︎\nGeographic Information Analysis, pages 13-16.↩︎\nYou’ll find Robert Hijmans’ take on this here.↩︎\nSuch curves are important for geohashes and hierarchical indexing schemes: see this page for more.↩︎\n‘Much faster’ is an understatement, even italicised.↩︎\nAs Dana Tomlin was wont to say, Yes raster is faster / But raster is vaster / And vector. . . / Just seems more correcter. This is one of those GIS things that is difficult to source. The attribution to Tomlin is according to Keith Clarke.↩︎"
  },
  {
    "objectID": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "href": "posts/2024-08-15-netlogo-utils/netlogo-utils.html",
    "title": "Useful utilities for NetLogo",
    "section": "",
    "text": "An intermittent side-project of mine over (github informs me) more than three years has been assembling a collection of useful functions or rather reporters and procedures written in NetLogo to make it just a little bit easier to do a range of fairly standard things in that language.\nDon’t get me wrong, NetLogo is wonderful and I love it dearly. I find it tremendous fun to code in. But any time I start dealing in any serious way with lists and strings or slightly more obscure probability distributions, I find that I am repeating myself rewriting code I’ve written dozens of times before. This probably became most apparent when I was writing the spatial COVID model.\nAnyway… the result has been this extremely intermittent and informal set of netlogo source (.nls) files that you can import into a NetLogo model to get access to more useful list and string handling without having to write an extension.\nNo warranties of fitness are expressed or implied. If you find them useful, let me know!"
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html",
    "title": "Mostly tiles, but also glyphs",
    "section": "",
    "text": "Note: You can take a closer look at any of the maps in this post simply by clicking on them.\nI’ve recently talked a lot (and will again…) about using tiling as an approach to present complex multivariate data in single map views. In this post I look at a relatively simple example of this problem from a project I’ve been working on recently—just three variables—where, it might be that a different approach is warranted, although as I think will become clear, the regularities of tiled patterns are useful regardless of whether the final result is technically (or even remotely) a tiling or not!\nI’m deliberately not going to say what the data represent. Suffice to say we have three variables, shown below for a chunk of New Zealand. From left to right, these are a resource potentially at risk, and two risk factors. The data preparation involved in assembling these risk factors is a whole story in itself, which I might get into some other time. For now, suffice to say, we’ve been careful to stick with colouring these three elements consistently (black, red, blue) as that helps a great deal in keeping some rather complicated data organised in our heads!\nYou’ll see also that we’ve calculated our three facets of the problem in a consistent geographic frame of reference, namely 10km grid squares. That’s 10×10km, i.e. 100km2 and not 10km2 as people so often seem to garble square units.1"
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tilings-by-squares",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tilings-by-squares",
    "title": "Mostly tiles, but also glyphs",
    "section": "Tilings by squares",
    "text": "Tilings by squares\nIt’s fine looking at the three datasets in parallel, but what’s really of interest is where high values of all three variables co-exist, and in given places which, if any of them, are low. Presenting more than one variable at once is exactly what our efforts in map tiling have been all about.\nSince only the red and the blue are risk factors one option using tiles is square tiles cut in half diagonally and choropleth coloured in the same manner as above. This gives us a map like this:\n\nThat seems fine, although… it might cause some unnecessary alarm given how almost the whole mapped area seems at great risk.2 So, can we also show the at-risk variable to offset that effect?\n One option is three variable tilings. You can slice a square into three equal sized elements rather easily. It turns out that if you cut from the centre of a square (or for that matter any regular polygon) to three points spaced at equal distances along its perimeter, then the magic of geometry guarantees the slices will be of equal size.3 Pop over to my handy dandy mapweaver app (I recommend opening that link in a new tab) and you can see what this looks like up close, by selecting the square-slice 3 tiling and setting Offset to 1. Mapping with this tileable unit gives us a map like the one below.\n\nBecause we’ve used the same colours here we can still tell which variable is which, although the tiling’s shapes tend to draw our eye to the cube-like arrangement of one blue tile on the left and one red tile on the right, where these are actually the data values from neighbouring grid cells, and should not strictly be paired with one another!\n We could fix this with ‘insetting’ as shown in the tileable unit to the left, although for me this is a little unsatisfactory as a pattern, and I leave generating such a map as an exercise for the reader.4 Another problem with this tiling is that the different shapes of the three tiles, which should perhaps in theory help us with telling one variable from another are distracting. Nice idea in theory then, but maybe not so great in practice."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#but-squares-are-so-square",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#but-squares-are-so-square",
    "title": "Mostly tiles, but also glyphs",
    "section": "But squares are so square",
    "text": "But squares are so square\nYes, you’re right. Squares are like, totally square. Hexagons are much sexier, even if we’re maybe all a bit over the hexbin moment and they are no longer the basis for the coolest discrete global grid systems.5\nIn the current application where we are interested in three variables, hexagons also have the convenient feature of a number of sides divisible by three so we can slice them into three pieces more nicely than squares. That approach yields a pleasingly cubic map:\n\n We could even make our slices pentagonal and get the best of both worlds. The problem with hexagons in this application is that the repeating hexagon shape doesn’t have a consistent mapping onto the underlying square gridded data, so which underlying data any particular group of tiles represents is unclear, and it’s even possible we are double counting some data and missing others. We could go back and redo all the analysis. It’s worth noting that all these numbers are estimates and subject to all kinds of variation as we move data around from geometry to geometry anyway, so perhaps the hex-square mismatch isn’t as bad as all that. Even so, if we really love the hex output map, then probably we should redo the analysis for hexagonal output areas. For now, I’m going to move on and go back to another way to present these three variables using square tiles cut diagonally."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#back-to-square-two",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#back-to-square-two",
    "title": "Mostly tiles, but also glyphs",
    "section": "Back to square two",
    "text": "Back to square two\nThe map below borrows an idea from the interesting value-by-alpha approach proposed some years ago by Rob Roth and others.6\n\nI’ve never entirely got along with value-by-alpha maps. This is partly because I’m not sure they work especially well on white backgrounds and I’m not cool enough for maps with dark backgrounds, and partly because I struggle to get the alpha (transparency/opacity) channel and all the associated blending modes to play nice. Somehow or other the alpha channel never has quite the effect I hope for and expect it to have in practice.\nThat said, the above map has some merit. I made it by calculating the colours in QGIS. You set the layer symbology to Simple Fill and then edit the expression for the colour to something like the below:\ncolor_rgba(if(\"var2\" &gt; 0, 204, 255),\n           if(\"var2\" &gt; 0, 0, 255),\n           if(\"var2\" &gt; 0, 0, 255),\n           \"var1\" * 255)\nIf you’ve ever used the expression editing in QGIS you’ll know it’s not a lot of fun to work with. Also, if you’re really paying attention, you’ll realise that this isn’t applying a colour ramp to var2 but simply turning it on if var2 is above 0 (which it is across most of the map, if you refer back to the first couple of maps in this post). It’s ‘dampening’ down the red colour by applying an alpha channel based on var1 the at-risk attribute. So this is almost like doing GIS overlay cartographically. The dimmed out reds and blues are of less concern because the resource at-risk is less exposed in those dimmed out areas. It would be possible using a more complex expression than that above to put the reds and blues on colour ramps, although given the transparency effect I think that the approach shown is better."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#just-show-all-the-data",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#just-show-all-the-data",
    "title": "Mostly tiles, but also glyphs",
    "section": "Just show all the data!",
    "text": "Just show all the data!\nI think the last example is interesting, and of the tiling-based approaches shown probably the most successful—although success will ultimately lie in the eyes of the end user of these maps and datasets.\nBut what that last example got me thinking is, “why not just show all the data?” The three variables are on different numerical scales, but if we rescale all three so they range from 0 to 1 then by setting symbology in QGIS to No Symbols and instead using the Diagrams option, we can construct a three variable histogram7 in each of the 10km square cells, after some rather annoying fiddle to get the barcharts to line up properly.8 By sticking with the red-blue-grey colouring it’s easy to tell which bar is which.\nAnyway, after some frustrating experiences with the options in the Diagrams option I wound up with this map:\n\nThis slice of the map doesn’t really do it justice, because what’s interesting about this map (to my tiling-attuned eye) is how the regular arrangement of barchart glyphs9 itself conveys information about the overall pattern. That is clearer when you see the whole map:\n\nWhere there is more ‘ink on the page’ or ‘colour on the screen’ are areas of greater possible concern. At the same time that you can see those overall patterns, it is easy to zoom in on particular areas for a closer look to confirm exactly why a particular area might be of interest. And because this representation is based on barcharts and does not depend on the vagaries of how colour is perceived but on estimation based on the heights of bars, which we are generally better at than reading values based on colours, the underlying data is likely to be conveyed more reliably by this map than some of the others here."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tiles-and-glyphs-the-same-or-different",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#tiles-and-glyphs-the-same-or-different",
    "title": "Mostly tiles, but also glyphs",
    "section": "Tiles and glyphs: the same or different?",
    "text": "Tiles and glyphs: the same or different?\nThere isn’t really one ‘right’ map. Any of the maps shown in this post could I think be useful depending on the exact interest of the end user. We’ll find out more about that in the weeks ahead, and it’s entirely possible they will simply want all the underlying data assembled into some kind of interactive web map (perhaps even a GIS…).\nI still think single map summaries of multivariate data have real value, and their design is an ongoing challenge. I really like the tiling approach I’ve been developing over the last couple of years, but in this case, I think that the barchart glyphs are probably a better overall solution. Saying that, I’ve learned from my adventures in tiling that repeating arrangements are an incredibly powerful tool for revealing patterns in geographical data, and it’s that as much as the individual glyphs that is doing a lot of the ‘information transmission’ work here.\nIt is also worth pondering the question of when a tiling becomes an arrangement of glyphs once we allow for ‘insetting’ as shown in the three slice options above. Insetting the repeating tileable unit when the inset distance is small makes it easier to tell which tile is which in the overall map arrangment. But as shown below, as the inset gets wider, the tileable unit in effect becomes a glyph!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the latest version of the weavingspace module10 the option to scale, rotate, and skew tileable units independently of their spacing within the tiling has been enabled, which can support this perspective. This is certainly a new dimension to the approach that I expect to explore further in other projects."
  },
  {
    "objectID": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#footnotes",
    "href": "posts/2025-05-02-tiles-and-glyphs/tiles-and-glyphs.html#footnotes",
    "title": "Mostly tiles, but also glyphs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was delighted to see this bugbear of mine picked up in a recent XKCD comic. Go. There. Now.↩︎\nAlthough, it’s really not: all those areas where there is no blue have very low risk associated with that factor and if we need both to be present then they are OK.↩︎\nA potentially useful tip for children’s parties. As for the geometry, it’s all about how the area of a triangle is given by \\(A=\\frac{1}{2}bh\\).↩︎\n😉↩︎\nThat honour now belongs to Carto’s A5geo pentagon based system in case you were wondering. I am of course using the word ‘coolest’ in the most nerdy and loosest of senses.↩︎\nSee for example this post.↩︎\nNot technically a histogram, more a bar chart, but now I am being nitpicky.↩︎\nBy default QGIS hovers them over the area centroid, which is fine… and makes complete sense if the areas are typical irregular polygons. For this gridded arrangement I wanted the barchart baselines to line up and that involves some more expressions to be entered to offset barcharts from the centroid depending on the values in the data. The barcharts also have to be drawn in Map Units which conjures up an alarming image of giant 3km wide rectangles the length and breadth of the country.↩︎\nThis paper provides a good overview of glyph-based visualisation: Borgo R, J Kehrer, DHS Chung, E Maguire, RS Laramee, H Hauser, MO Ward and M Chen. 2013. Glyph-based visualization: foundations, design guidelines, techniques and applications. In 34th Eurographics 2013: Girona, Spain - State of the Art Reports, ed. M Sbert and L Szirmay-Kalos, 39-63. doi: 10.2312/conf/EG2013/stars/039-063.↩︎\n0.0.6.79 at time of writing available on pypi.org↩︎"
  },
  {
    "objectID": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html",
    "href": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html",
    "title": "Look ma! (Almost) no javascript!",
    "section": "",
    "text": "Last week I gave talk about how I made a web app for our tiled and woven maps work1 to the Wellington Python meetup held at the Sharesies office on the third Thursday of every month.\nThe talk was recorded, although the original was pretty choppy as a result of some weird interactions between google meet, macos workspaces, and full screen browser windows (or something) so the below is a re-recording I made the day after.\nIf you are mostly interested in how you can make a web app in pure python while writing no javascript, then skip to about 19 minutes in where I talk about marimo and more or less competently demonstrate how easy it is to do. I recently posted about marimo when I first encountered it two or three months ago.\nThe app itself is here. Be patient: it takes a little while to load, which is why I wrote almost no javascript. The little I did write is to show a splash screen while the app loads in the background!"
  },
  {
    "objectID": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html#footnotes",
    "href": "posts/2025-05-22-python-meetup-talk/python-meetup-talk.html#footnotes",
    "title": "Look ma! (Almost) no javascript!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI should note here as I somehow failed to do in the talk that the tiled and woven maps work is in collaboration with Luke Bergmann at UBC in Canada.↩︎"
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "",
    "text": "This post is the first exploring topics raised in Chapter 2 of Geographic Information Analysis1. Specifically it is about one of the ‘pitfalls’ of spatial data, spatial autocorrelation.\nAs we’ll see, and as the book suggests, it’s not really clear that spatial autocorrelation really is a problem, so much as a characteristic feature of spatial data that it’s pretty much the whole purpose of spatial analysis to describe and leverage in various ways to improve our analysis.\nAnway, returning to the text, on page 35, we state that,\nThis post zeros in on that aspect of spatial autocorrelation.\nCode\nlibrary(terra)\nlibrary(spatstat)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nset.seed(1)"
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#a-random-and-an-autocorrelated-surface",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#a-random-and-an-autocorrelated-surface",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "A random and an autocorrelated surface",
    "text": "A random and an autocorrelated surface\nTo get started we make a random and an autocorrelated surface. I do this using terra by first making an empty 100×100 raster, then populating it with random numbers.\n\nextent &lt;- ext(c(xmin = -1, xmax = 1,\n                ymin = -1, ymax = 1) * 5000)\n\nr_random &lt;- rast(res = 100, crs = \"+proj=eqc\", extent)\nnames(r_random) &lt;- \"value\"\nvalues(r_random) &lt;- runif(10000)\n\nNext, smooth the surface. Easiest way to do this is to apply a focal mean a few times.\n\n# label: smooth-the-grid\nr_smoothed &lt;- r_random\nfor (i in 1:20) {\n1  r_smoothed &lt;- r_smoothed |&gt; focal(3, mean, expand = TRUE)\n}\nnames(r_smoothed) &lt;- \"value\"\n\n\n1\n\nexpand = TRUE uses duplicate values around the perimeter of the raster in the calculation of the mean so our raster doesn’t shrink one cell on all sides at each iteration.\n\n\n\n\nFinally, remake the random grid by shuffling the values in the smoothed surface. This is so that our random and smoothed surfaces are (aspatially) statistically identical.\n\nvalues(r_random) &lt;- sample(values(r_smoothed), 10000, replace = FALSE)\n\nAnd here’s a plot comparing the two.\n\n\nCode\nxy_random &lt;- r_random |&gt; as.data.frame(xy = TRUE)\nxy_smoothed &lt;- r_smoothed |&gt; as.data.frame(xy = TRUE)\n\ng1 &lt;- ggplot() +\n  geom_raster(data = xy_random, aes(x = x, y = y, fill = value)) +\n  scale_fill_distiller(palette = \"Reds\") +\n  coord_equal() +\n  guides(fill = \"none\") +\n  ggtitle(\"Random\") +\n  theme_void()\n\ng2 &lt;- ggplot() +\n  geom_raster(data = xy_smoothed, aes(x = x, y = y, fill = value)) +\n  scale_fill_distiller(palette = \"Reds\") +\n  coord_equal() +\n  guides(fill = \"none\") +\n  ggtitle(\"Smoothed\") +\n  theme_void()\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 1: The random and smoothed surfaces"
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#sampling-values-from-the-surfaces",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#sampling-values-from-the-surfaces",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "Sampling values from the surfaces",
    "text": "Sampling values from the surfaces\nWe can sample values at random from a raster using terra::spatSample but here we want to control the configuration of the sampling pattern. For that we’ll use sampling points generated by spatstat’s point pattern functions.\nA couple of helper functions, to do the sampling, and to convert a spatstat point pattern to a plain terra::SpatVector object.\n\nppp_as_spatvector &lt;- function(ppp) {\n  ppp |&gt;\n    st_as_sf() |&gt;\n    select() |&gt;\n    st_set_crs(\"+proj=eqc\") |&gt;\n1    slice(-1) |&gt;\n    as(\"SpatVector\")\n}\n\n2sample_raster_statistic &lt;- function(r, pts, fn = mean) {\n  terra::extract(r, pts |&gt; ppp_as_spatvector(), ID = FALSE) |&gt; \n    pull(1) |&gt; \n    fn()\n}\n\n\n1\n\nslice(-1) removes the first row of the sf dataset which is the polygon representing the window of the point pattern.\n\n2\n\nWe could supply an arbitrary function if we want.\n\n\n\n\nNow make up some point patterns, Poisson random, evenly-spaced, clustered, and a grid-based one.\n\nwindow &lt;- owin(extent[1:2], extent[3:4])\n\npoisson_pps &lt;- list()\nssi_pps &lt;- list()\nclustered_pps &lt;- list()\narea &lt;- diff(extent[1:2]) * diff(extent[3:4])\nfor (i in 1:100) {\n  set.seed(i)\n  poisson_pps[[length(poisson_pps) + 1]] &lt;- rpoispp(100 / area, win = window)\n  set.seed(i)\n  ssi_pps[[length(ssi_pps) + 1]] &lt;- rSSI(600, n = 100, win = window)\n  set.seed(i)\n  clustered_pps[[length(clustered_pps) + 1]] &lt;- rThomas(10 / area, 250, 10, win = window)\n}\n\nget_grid_pp &lt;- function(offset_x = 0, offset_y = 0) {\n  as.ppp(xy_random |&gt; \n           select(x, y) |&gt;\n           filter((x %/% 100) %% 10 == offset_x,\n                  (y %/% 100) %% 10 == offset_y), \n         W = window)\n}\ngrid_pps &lt;- mapply(get_grid_pp, rep(0:9, 10), rep(0:9, each = 10), SIMPLIFY = FALSE)\n\nAnd here’s what those look like on top of our random and smoothed rasters.\n\n\nCode\ng3 &lt;- ggplot() +\n  geom_raster(data = xy_random, aes(x = x, y = y, fill = value), alpha = 0.5) +\n  scale_fill_distiller(palette = \"Greys\") +\n  geom_sf(data = grid_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1),\n          shape = 1) +\n  geom_sf(data = poisson_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1)) +\n  geom_sf(data = ssi_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1), \n          colour = \"dodgerblue\") +\n  geom_sf(data = clustered_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1), \n          colour = \"red\") +\n  coord_sf(xlim = extent[1:2], ylim = extent[3:4]) +\n  guides(fill = \"none\") +\n  theme_void()\n\ng4 &lt;- ggplot() +\n  geom_raster(data = xy_smoothed, aes(x = x, y = y, fill = value), alpha = 0.5) +\n  scale_fill_distiller(palette = \"Greys\") +\n  geom_sf(data = grid_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1),\n          shape = 1) +\n  geom_sf(data = poisson_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1)) +\n  geom_sf(data = ssi_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1), \n          colour = \"dodgerblue\") +\n  geom_sf(data = clustered_pps[[1]] |&gt; st_as_sf() |&gt; slice(-1), \n          colour = \"red\") +\n  coord_sf(xlim = extent[1:2], ylim = extent[3:4]) +\n  guides(fill = \"none\") +\n  theme_void()\n  \ng3 | g4\n\n\n\n\n\n\n\n\nFigure 2: Typical grid-based (open circles), uniform (black), evenly-space (blue), and clustered (red) sampling patterns on the random and smoothed raster surfaces\n\n\n\n\n\nBefore reading further, what difference do you think the surface structure might make to the results for estimating the mean using different sampling schemes?"
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#does-the-surface-autocorrelation-make-a-difference",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#does-the-surface-autocorrelation-make-a-difference",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "Does the surface autocorrelation make a difference?",
    "text": "Does the surface autocorrelation make a difference?\nWe can sample the surfaces 100 times each, using the four different sampling schemes, and see what difference it makes to our estimates of some statistic, here the mean value of our surface.\n\ndf &lt;- data.frame(\n    Random_Grid = grid_pps |&gt; \n      lapply(sample_raster_statistic, r = r_random) |&gt; \n      unlist(),\n    Smoothed_Grid = grid_pps |&gt; \n      lapply(sample_raster_statistic, r = r_smoothed) |&gt; \n      unlist(),\n    Random_Poisson = poisson_pps |&gt; \n      lapply(sample_raster_statistic, r = r_random) |&gt; \n      unlist(),\n    Smoothed_Poisson = poisson_pps |&gt; \n      lapply(sample_raster_statistic, r = r_smoothed) |&gt; \n      unlist(),\n    Random_Even = ssi_pps |&gt; \n      lapply(sample_raster_statistic, r = r_random) |&gt;\n      unlist(),\n    Smoothed_Even = ssi_pps |&gt; \n      lapply(sample_raster_statistic, r = r_smoothed) |&gt; \n      unlist(),\n    Random_Clustered = clustered_pps |&gt; \n      lapply(sample_raster_statistic, r = r_random) |&gt;\n      unlist(),\n    Smoothed_Clustered = clustered_pps |&gt;\n      lapply(sample_raster_statistic, r = r_smoothed) |&gt; \n      unlist()) |&gt;\n1  pivot_longer(cols = 1:8) |&gt;\n  rename(Mean = value) |&gt;\n2  separate_wider_delim(\n    cols = name, delim = \"_\", \n    names = c(\"Raster\", \"Sampling\")) |&gt;\n  mutate(\n    Raster = ordered(\n      Raster, levels = c(\"Random\", \"Smoothed\")),\n    Sampling = ordered(\n      Sampling, levels = c(\"Poisson\", \"Grid\",\n                           \"Even\", \"Clustered\")))\ndf\n\n\n1\n\nMake the variable names into a name column and the values into a value column.\n\n2\n\nSplit the name column into Raster and Sampling columns.\n\n\n\n\n# A tibble: 800 × 3\n   Raster   Sampling   Mean\n   &lt;ord&gt;    &lt;ord&gt;     &lt;dbl&gt;\n 1 Random   Grid      0.495\n 2 Smoothed Grid      0.499\n 3 Random   Poisson   0.501\n 4 Smoothed Poisson   0.501\n 5 Random   Even      0.505\n 6 Smoothed Even      0.500\n 7 Random   Clustered 0.501\n 8 Smoothed Clustered 0.487\n 9 Random   Grid      0.502\n10 Smoothed Grid      0.499\n# ℹ 790 more rows\n\n\nAnd now we can plot the distribution of estimates of the mean for the various combinations of random and smoothed surface and sampling scheme.\n\n\nCode\nggplot(df) +\n  geom_density(aes(x = Mean, fill = Raster), colour = NA, alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap( ~ Sampling, ncol = 4, labeller = label_both) +\n  ylab(\"Density\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = NA),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 3: Density smoothed sampling distributions of the mean value of the surfaces with different combinations of random (red) vs smooth surface (blue) and various sampling schemes.\n\n\n\n\n\nAnd in answer to the question, yes, autocorrelation of the surface values makes the sampling scheme matter a lot! Keep in mind that data values in our two surfaces are exactly the same set of values. It is the difference in their spatial arrangement that causes our different sampling schemes to see different things.\nIn all cases, regardless of sampling scheme the random raster yields up a similar sampling distribution of the mean. It doesn’t matter how we go about picking 100 sample locations, we end up with a similar picture of the data, because regardless of how we pick 100 samples, they are random like the data, and form an unbiased sample.\nThe Poisson random set of observations yields similar distributions of the sampling mean for both surfaces as we might hope. By definition a Poisson point process is unbiased without first- or second-order effects, so in both cases we get an unbiased sample of the data.\nThings go off the rails pretty badly after that on the smoothed surface. Both the grid-based and evenly-spaced sampling schemes yield a very narrow range of estimates of the mean, because these schemes at this low (1%) sampling rate may miss peaks and troughs in the data. If a single sample location happens to hit a peak, then a bunch of others are guaranteed to miss it because the even spacing guarantees that no other samples will be taken nearby.\nMeanwhile the clustered sampling scheme produces a much wider range of estimates of the mean. This is because sometimes a large number of samples close together will land near a peak or trough producing a biased estimate of the mean in that case, because we’re measuring a disporportionate number of points close to an extreme value.\n\nBut nobody would be that dumb, surely?\nOf course this is an artificial setup. In practice people try to sample sensibly, right? Well yes, but…\nBut in practice, our sampling schemes are likely to be biased. This could be for all kinds or reasons. Areas of particular interest may not be typical but we focus data collection in those places (because they are interesting). Samples are likely to be preferentially collected in more accessible locations for practical and economic reasons. Or, perhaps we are being ‘smart’ and plan an evenly spaced sampling strategy. But this approach, unless we have prior knowledge of the data, risks missing peaks and troughs completely. In this case where the statistic of interest is the mean, that might not matter (we get an accurate estimate), but if we really need to know the variance then the data we collect will be misleading.\nAnd very often the data already exist and we don’t get a say in how they were collected.\nAny statistician would of course say that the best sampling scheme is a random, unbiased one.2 In practice, in spatial terms it rarely is, and if the underlying spatial data have spatial structure—i.e. they are autocorrelated—which since we are interested in geography we can presume they do, then how we sample them can matter greatly."
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#pitfall-or-potential",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#pitfall-or-potential",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "Pitfall or potential?",
    "text": "Pitfall or potential?\nWhen it comes to data collection, definitely pitfall. Spatial data should be collected with care and attention to these kinds of concerns. Prior knowledge about the phenomenon of interest should inform decisions about how to go about collecting useful data. It’s important to note here that while regular gridded samples perform abominably in this toy example, if the sampling interval of your grid is fine grained enough relative to variation in the phenomenon then it’s probably OK. Still worth being cognizant of these issues though: it’s easy to be beguiled by ‘detailed’ data and assume away this kind of problem.\nOn the other hand, dealing with these issues and understanding the extent of autocorrelation in spatial data has led to a wide range of methods for characterising spatial variation in spatial data, and those are of great interest to geographers. As we comment on page 36 in Geographic Information Analysis\n\nAlthough autocorrelation presents a considerable challenge to conventional statistical methods and remains problematic, quantitative geographers have made a virtue of it by developing a number of autocorrelation measures into powerful descriptive tools. It would be wrong to claim that the problem of spatial autocorrelation has been solved, but considerable progress has been made in developing techniques that account for its effects and in taking advantage of the opportunity it provides for useful geographic descriptions.\n\nAnd on that cheery note, I’ll end."
  },
  {
    "objectID": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#footnotes",
    "href": "posts/2025-11-14-gia-chapter-2A-spatial-autocorrelation/index.html#footnotes",
    "title": "Spatial autocorrelation: what’s the problem?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://onlinelibrary.wiley.com/doi/book/10.1002/9780470549094↩︎\nStats 101.↩︎"
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "In the previous post I explained the genesis of this short series of ‘Got ____?’ posts. This post comes hard on the heels of the last one because I’d already written some of it before realising that attempting all sixteen transformations in a single post would be too much of a good(?) thing.\nLet’s gone!1\nCode\nlibrary(spatstat)\nlibrary(sfdep)\nlibrary(sfnetworks)\nlibrary(tidygraph)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(terra)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cols4all)\nlibrary(patchwork)\n\ntheme_set(theme_void())\nset.seed(1)"
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#from-lines",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#from-lines",
    "title": "GIS, a transformational approach",
    "section": "From Lines…",
    "text": "From Lines…\nAs before, we need some data. It will make sense to make both some random lines, and also a network.\nRandom lines are simple enough. Again, we’ll have thirty of them, in time-honoured statistical tradition.\n\n\nCode\n1cx &lt;- 1.745e6\ncy &lt;- 5.425e6\n\nget_random_line &lt;- function(offset_e, offset_n) {\n  st_linestring(matrix(\n    rnorm(4, sd = 1000) + rep(c(offset_e, offset_n), 2),\n    ncol = 2, byrow = TRUE))\n}\n\nlines &lt;- mapply(get_random_line,\n                offset_e = rep(cx, 30), \n                offset_n = rep(cy, 30), \n                SIMPLIFY = FALSE) |&gt;\n  st_sfc() |&gt;\n  data.frame() |&gt;\n  st_as_sf(crs = 2193)\n\nggplot() +\n  geom_sf(data = lines)\n\n\n\n1\n\nSo that the lines are centred somewhere near Wellington.\n\n\n\n\n\n\n\n30 random lines\n\n\n\n\nNetworks are a little trickier, because they necessarily also include points. The code below uses spatstat::rMatClust to first make a clustered set of points, then sfdep::st_nb_gabriel and sfdep::st_as_graph to make a Gabriel graph among those points. The result nw is a tidygraph network object, from which we can extract edges (lines) as a sf object.\n\n\nCode\npp &lt;- rMatClust(\n1  kappa = 1e-6,\n2  scale = 400,\n3  mu = 10,\n  win = owin(c(-2500, 2500), c(-2500, 2500))\n4)\n\npp_sf &lt;- pp |&gt;\n  as.data.frame() |&gt; \n  mutate(x = cx + x, y = cy + y) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\"), crs = 2193) |&gt;\n  mutate(ID = row_number())\n\nnw &lt;- pp_sf |&gt; \n  st_as_sfc() |&gt;\n  st_as_graph(\n    pp_sf |&gt;\n      st_as_sfc() |&gt; \n      st_nb_gabriel())\n\nedges &lt;- nw |&gt;\n  activate(\"edges\") |&gt;\n  st_as_sf() \n\nggplot() + \n  geom_sf(data = edges) + \n  geom_sf_label(data = pp_sf, aes(label = ID), size = 3) +\n  theme_void()\n\n\n\n1\n\nIntensity of the Poisson distributed cluster centres.\n\n2\n\nSize of the clusters (in metres).\n\n3\n\nMean number of events per cluster.\n\n4\n\nThis combination of parameters should give us about 250 events on average.\n\n\n\n\n\n\n\nA Gabriel graph based random network\n\n\n\n\nThe network is at least somewhat reminiscent of the connections we might see among a collection of small towns."
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-points",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-points",
    "title": "GIS, a transformational approach",
    "section": "… to points",
    "text": "… to points\nOur oracular table demands that we make points from lines by intersection. We already know that all the lines in our network intersect at nodes, so it makes more sense to generate the points of intersection of our random arrangement of lines.\n\n\nCode\nintersections &lt;- lines |&gt;\n  st_intersection() |&gt;\n  filter(st_geometry_type(geometry) == \"POINT\")\n\nggplot() + \n  geom_sf(data = lines, colour = \"lightgray\") +\n  geom_sf(data = intersections)\n\n\n\n\n\nPoints based on intersection of the random lines\n\n\n\n\nThis is very straightforward. The process slows down rather dramatically if there are a lot more objects, but with only 30 lines is quick. It’s worth noting that I have to filter out all the ‘intersections’ that are themselves lines, in other words not points. That’s because left to its own devices st_intersection treats each line as intersecting with itself. If I omit the filter, you can see what I mean:\n\n\nCode\nlines |&gt; \n  st_intersection()\n\n\nSimple feature collection with 127 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1743195 ymin: 5422785 xmax: 1747402 ymax: 5427173\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n    n.overlaps origins                       geometry\n1            1       1 LINESTRING (1744374 5425184...\n2            1       2 LINESTRING (1745330 5424180...\n3            1       3 LINESTRING (1745576 5424695...\n4            1       4 LINESTRING (1744379 5422785...\n2.1          2    2, 5        POINT (1745487 5425734)\n5            1       5 LINESTRING (1744984 5425944...\n4.1          2    4, 6        POINT (1745388 5424039)\n3.1          2    3, 6        POINT (1745591 5424706)\n6            1       6 LINESTRING (1745919 5425782...\n2.2          2    2, 7        POINT (1745359 5424468)"
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-lines",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-lines",
    "title": "GIS, a transformational approach",
    "section": "… to lines",
    "text": "… to lines\nShortest paths are next. This was interesting to figure out. The tidygraph semantics are quite nice once you get used to them, but at least at first, and for me, a little confusing.\ntidygraph wraps igraph and applies many of the igraph functions that yield subgraphs (such as shortest path) using a morph function, which may yield several subgraphs. For example, a connected subgraphs algorithm might find many potential subgraphs. In such cases, to extract a particular graph of interest another function crystallise is used. In a case such as shortest path where only one subgraph is expected, we can morph and crystallise in one step with convert.\nSo here’s what that looks like for shortest path between two vertices, selected using the vertex ID labelled map above, for the fact that they involved diagonally traversing most of the network. I make two different shortest paths, one topological based on counting only the number of edges traversed, and one distance-based where the weights parameter for shortest path is calculated from the lengths of edges.\n\n\nCode\nsp_edges &lt;- nw |&gt;\n  convert(to_shortest_path, from = 166, to = 160) |&gt;\n  activate(\"edges\") |&gt;\n  st_as_sf()\n\nsp_distances &lt;- nw |&gt;\n  convert(to_shortest_path, from = 166, to = 160, weights = st_length(edges)) |&gt;\n  activate(\"edges\") |&gt;\n  st_as_sf()\n\nggplot() +\n  geom_sf(data = edges, colour = \"lightgrey\") +\n  geom_sf(data = sp_edges, colour = \"black\") +\n  geom_sf(data = sp_distances, colour = \"red\")\n\n\n\n\n\nShortest path across the random networks based on counting edges (black), and accumulated distance along edges (red).\n\n\n\n\nThe two shortest paths produced are quite different, as we would expect, with the distance-based one a much more directly ‘diagonal’ path across the space.\nI’ve used igraph a fair amount (see e.g. this post and this project) but have found its plotting interface confusing. In addition, maintaining spatial coordinates for vertices is difficult at times, and of course igraph knows nothing about cartographic projections. The sfnetworks and tidygraph combination seems a very promising development in this space."
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-areas",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-areas",
    "title": "GIS, a transformational approach",
    "section": "… to areas",
    "text": "… to areas\nAfter the excitement of network analysis, it seems rather pedestrian to go back to boring old buffering, but the table of all geographical knowledge knows what it wants, and buffers it will be.\n\n\nCode\nline_buffers &lt;- edges |&gt;\n  st_buffer(100)\n\nggplot() +\n  geom_sf(data = line_buffers, colour = NA) +\n  geom_sf(data = edges, lwd = 0.35)\n\n\n\n\n\n\n\n\n\nNot a compelling chunk of code, but the visual effect of buffering the lines in a network is very pleasing.\nIt would perhaps be better accomplished in a drawing package, but it’s worth knowing you can create single-sided buffers too.\n\n\nCode\nleft_buffers &lt;- edges |&gt;\n  st_buffer(100, singleSide = TRUE)\n\nright_buffers &lt;- edges |&gt;\n  st_buffer(-100, singleSide = TRUE)\n\nggplot() +\n  geom_sf(data = left_buffers, fill = \"#FF883E\", colour = NA) +\n  geom_sf(data = right_buffers, fill = \"#169B62\", colour = NA) +\n  geom_sf(data = edges, lwd = 2, colour = \"white\")\n\n\n\n\n\n\n\n\n\nI am, of course, channeling the flag of Cóte d’Ivoire here, and not the flag of Ireland.2"
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-fields",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#to-fields",
    "title": "GIS, a transformational approach",
    "section": "… to fields",
    "text": "… to fields\nAs in the previous post concerning transformations from points, the distance to nearest object surface is the output here, and we can adopt a similar approach.\n\n\nCode\ntarget_raster &lt;- edges |&gt;\n  as(\"SpatVector\") |&gt;\n  rast(res = 25)\n\ntarget_points &lt;- target_raster |&gt;\n  as.points() |&gt;\n  st_as_sf()\n\ndistance_surface &lt;- target_points |&gt;\n  mutate(\n    distance = st_distance(geometry, edges) |&gt; \n    apply(MARGIN = 1, min)) |&gt;\n  as(\"SpatVector\") |&gt;\n  rasterize(target_raster, field = \"distance\")\nnames(distance_surface) &lt;- \"distance\"\n\nggplot() +\n  geom_raster(\n    data = distance_surface |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = distance)) +\n  geom_sf(data = edges) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.prgn\")\n\n\n\n\n\n\n\n\n\nAs with the points distance surface output the visual effect here is quite pleasing. Results similar to this may be useful when it comes to the area to line transformation based on polygon skeletonisation.3"
  },
  {
    "objectID": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#footnotes",
    "href": "posts/2025-09-12-gia-chapter-1B-part-2/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWarriors.↩︎\nOr have I? The colours in the two flags are not precisely the same, apparently.↩︎\nSpoilers.↩︎"
  },
  {
    "objectID": "posts/2025-07-10-hulls-tetrahedra/index.html",
    "href": "posts/2025-07-10-hulls-tetrahedra/index.html",
    "title": "A geometric origami classic",
    "section": "",
    "text": "Apropos of nothing in particular, my most recent origami construction is my first proper attempt at Tom Hull’s Five Intersecting Tetrahedra.\n\nAs origami legend Robert Lang notes the geometric object depicted (which goes by the quite fantastic name chiricosahedron) has been known for centuries, but only appeared in origami sometime in the 1990s. Lang says that “Naturally, of course, I wished I’d thought of it”, which in the world or origami is a bit like having Paul McCartney say he wished he wrote your song.\nAnyway, it was an instant classic and according to Tom Hull’s CV voted one of the top 10 origami models of all time by the British Origami Society in 2000.\nAll I can say is that while not difficult to fold, it’s mind-bending to assemble. I only got there with the help of this video, which provides valuable guidance on where exactly to thread the edges of each tetrahedron through to make it all hang together. Even with that help, when I thought I’d finished I had one strut the wrong side of another, which I only realised when I went to photograph it.\nNow I’ve made it once, I have a better appreciation of the logic of its geometry. The key is to realise that each set of five tetrahedra corners form the corners of the faces of a dodecahedron. Or at any rate, I think that will help, next time. There is also a symmetry to how pairs of tetrahedra poke through the middle of one face of two of the others.\nBut writing about origami is, as has been said of writing about music, like dancing about architecture, so I’ll stop there. Suffice to say, I think I could probably make it a second time unassisted by YouTube.\nProbably.\nFull disclosure, I retreated to the relative safety of this model having dismally failed to make this absurd construction, Sixteen Interlocking Triangles by Byriah Loper from a copy of Mind-Blowing Modular Origami that I found in a second hand bookshop a week or two ago. Loper’s models are extraordinary, but really… I think I’m more taken with the balance between complexity and elegance of Five Intersecting Triangles. I certainly won’t be tackling this monstrosity any time soon, although there’s something pleasingly globe-like about Ten Intersecting Nonagons."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html",
    "title": "Imagine setting up a Jupyter hub",
    "section": "",
    "text": "I’ve worked with :Jupyter notebooks for many years. Almost—but not quite!—since their inception.\nI was lucky enough to be a professor at Berkeley in 2014 when the technology was emerging in education. People associated closely with the Berkeley data science program pretty much invented this stuff. I developed a ‘connector’ class in geography as a contribution to the nascent Data Science major. The (I believe correct) thinking was that that it shouldn’t be possible to graduate in Data Science alone, and that all students should have some kind of domain expertise in the form of another major in a traditional discipline. To encourage this line of thought students taking ‘data science 101’ (in fact :Data 8 The foundations of data science) are required to take a parallel half-credit ‘connector’ class in some other discipline.\nI developed Geog 88 Data science applications in geography which was was my first experience with notebooks. It was also my first encounter with a very limited (at the time) geopandas. So limited that I had to hack together some code to make waiting for it to plot polygon layers bearable. As I recall the problem was that geopandas was iterating over the polygons in a dataset using a for loop and plotting them one at a time. The fix was to convert a GeoDataFrame to a PatchCollection and plot that instead. Anyway, they’ve fixed that problem now so this is ancient history. That was my first close-up and personal encounter with matplotlib a python module I continue to hate and love in near-equal measure to this day.1"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#my-life-as-a-jupyter-pioneer",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#my-life-as-a-jupyter-pioneer",
    "title": "Imagine setting up a Jupyter hub",
    "section": "",
    "text": "I’ve worked with :Jupyter notebooks for many years. Almost—but not quite!—since their inception.\nI was lucky enough to be a professor at Berkeley in 2014 when the technology was emerging in education. People associated closely with the Berkeley data science program pretty much invented this stuff. I developed a ‘connector’ class in geography as a contribution to the nascent Data Science major. The (I believe correct) thinking was that that it shouldn’t be possible to graduate in Data Science alone, and that all students should have some kind of domain expertise in the form of another major in a traditional discipline. To encourage this line of thought students taking ‘data science 101’ (in fact :Data 8 The foundations of data science) are required to take a parallel half-credit ‘connector’ class in some other discipline.\nI developed Geog 88 Data science applications in geography which was was my first experience with notebooks. It was also my first encounter with a very limited (at the time) geopandas. So limited that I had to hack together some code to make waiting for it to plot polygon layers bearable. As I recall the problem was that geopandas was iterating over the polygons in a dataset using a for loop and plotting them one at a time. The fix was to convert a GeoDataFrame to a PatchCollection and plot that instead. Anyway, they’ve fixed that problem now so this is ancient history. That was my first close-up and personal encounter with matplotlib a python module I continue to hate and love in near-equal measure to this day.1"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#and-so-to-aotearoa",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#and-so-to-aotearoa",
    "title": "Imagine setting up a Jupyter hub",
    "section": "And so to Aotearoa",
    "text": "And so to Aotearoa\nAnyway, in an ideal world, I’d say ‘and the rest is history’ at this point, but… while the trajectory of Jupyter notebooks has been onwards and upwards from there, my personal journey with them has been rather different.\nWe came back to Aotearoa New Zealand in 2018 for uh… reasons (not unrelated to more recent events in the US) to a position as Professor of Geography and Geospatial Science at Victoria University of Wellington. Taking the second part of that moniker (geospatial science) somewhat seriously, I was excited to advocate for using python notebooks in teaching, and… well… basically I hit a brick wall. Turns out that Berkeley was probably definitely five (maybe even more) years ahead of New Zealand universities in uptake and support for Jupyter notebooks and the associated cloud computing needed to make effective use of the technology in the classroom.\n\nIt’s too hard\nAt first when I asked for IT support to set up notebooks in the lab I encountered blank looks. Then after some research, mostly more blank looks, followed by some version of “it’s too hard” in the context of general computing labs where the GIS classes are taught. We briefly explored the free tier of Azure notebooks then on offer to the university through its everything-MS-all-the-time Microsoft subscription but the CPU cores provided through that deal were underpowered. I also contemplated using the Jupyter capability then emerging in the Esri suite, but that was ArcPy, which was and remains a clunky and unwelcoming introduction to the elegant python programming language. A most ‘unpythonic’ place to begin.2 The more friendly ArcGIS API for Python had yet to emerge (ArcGIS Pro had barely emerged at this point.)\nSo… I shelved things for a while, but returned to notebooks when it came to developing a class in Geographical Computing as part of the eventually-doomed postgraduate program in GIS at VUW. Given lab support limitations, I wound up teaching students about conda environments so we could all setup the same python environment on our computers and took it from there. So we never used Jupyter notebooks in the client-(remote) server mode, but instead in client-(local) server mode. It was fine, if a little deflating, given that Berkeley had been delivering notebook based instructions to classes of several hundred or more students five years earlier! On the upside, students did get to learn about virtual environments and all that good stuff."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#anyway-fast-forward-to-today",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#anyway-fast-forward-to-today",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Anyway… fast forward to today",
    "text": "Anyway… fast forward to today\nI still use notebooks all the time. It’s probably not a great habit, but they’re my go to for quickly exploring coding ideas. I even use them as a high level ‘test harness’ of sorts for the weavingspace module.\nAnd now, I’ve been asked to develop python training materials for a client. In that context, I once again have run into organisational IT. In brief, the course is to be delivered via Jupyter notebooks, but IT want the supplier (that would be me) to provide a solution completely outside their infrastructure. In other words, no in-house Jupyter hub or similar, student login credentials to be provided by me, and so on.\nWhich meant I was faced with the question that VUW IT were posed by me back in 2018: can you support Jupyter notebook based instruction?\nAt first I did the obvious thing and explored options at a number of more or less shrink-wrapped solutions such as Google Colab, CoCalc, and SaturnCloud. What I found there was more confusing than helpful. CoCalc is the only one actually focused on teaching/training and its pricing model is hard to unpick. The interface for setting up a course is also less than intuitive. The other two, like most offerings in this space are so focused on the work of machine-learning teams that it’s not at all obvious that they would work for my use-case: a relatively small group of learners working with a limited (and niche, i.e. geospatial) set of python modules. This pattern was repeated at all the options I looked at.\nAnd so,3 I thought it was time to roll up my sleeves and give setting up my very own Jupyter hub a try. How hard could it be? Really really hard I thought. After all, fully grown university IT departments have shuddered at the very thought."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#reader-its-really-not-that-hard",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#reader-its-really-not-that-hard",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Reader, it’s really not that hard!",
    "text": "Reader, it’s really not that hard!\nSeriously though, it really isn’t.\nIt’s so easy that I’m not going to provide tutorial advice, but instead link you to the excellent instructions I followed at The Littlest Jupyter Hub. It took about 45 minutes.\nBut that included registering payment details at Digital Ocean4 and two false starts, where first, I ignored a clear bolded instruction in the setup guide5\n\nand then I forgot to give the server a more memorable name than the default one assigned by Digital Ocean (this isn’t a problem as such, just annoying).\nWhat can I say? It had been a long day poring over complicated cloud computing pricing structures and negotiating 2FA on new services I had only joined to take a look. If you avoid those false starts and already have a cloud computing subscription, you could be up and running in under half an hour.\nIt took a further half hour or so to configure things on the server itself, setting up the default user environment, and most importantly setting up https rather than http access (for this you need an internet domain name on which you can establish subdomains).\nBut really… I have had more problems getting IMAP access to an email account via gmail to work properly."
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#the-hardest-parts",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#the-hardest-parts",
    "title": "Imagine setting up a Jupyter hub",
    "section": "The hardest parts",
    "text": "The hardest parts\nThe biggest obstacles to doing this are most likely:\n\nFor https access you need an internet domain on which you can set up a subdomain. This is technically straightforward but you do need a domain you own and have some control over.\nCloud compute pricing plans are pretty much meaningless until you actually sign up. You won’t know what you need until you start using it, and you won’t know what that translates to in terms of actual compute you are paying for until you have some people logged in and using a server. It’s reminiscent of the worst days of mobile phone plans.\n\nThe latter is a real issue. I will have to on-charge the compute costs to the client and we won’t know what those are until after we’re done. We do have a no-cost (but some nuisance) plan B involving the somewhat amazing Github Codespaces, and we might even end up using that because of the cost, or more accurately, the uncertainty around the cost of the Jupyter hub solution.\nBut my main reflection on this experience would be to strongly encourage any organisational IT team to take a look at this. Chances are you are already using cloud compute infrastructure, so what’s the holdup?!"
  },
  {
    "objectID": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#footnotes",
    "href": "posts/2025-04-30-setting-up-a-jupyter-hub/jupyter-hub.html#footnotes",
    "title": "Imagine setting up a Jupyter hub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, probably hate &gt; love. If there is a more convoluted API than matplotlib anywhere on the planet, I really don’t want to know about it.↩︎\nYes… I’ve drunk the python koolaid.↩︎\nEgged on, I should acknowledge, by an academic buddy↩︎\nA choice dictated more than anything by them appearing first in the list on the tutorial page: do yourself a favour and give one of the options with free compute hours a try first.↩︎\nRTFM!↩︎"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Computing Geographically argues for the importance of giscience taking geography seriously, and also for geography taking giscience more seriously. Too much GIS work is conducted as if \\((x,y)\\) coordinates were all that is needed to make data geographical; equally, far too many geographers think that those coordinates are all that GISers care about. This book aims to bridge the divide. An accompanying website includes high resolution copies of all the figures, and ‘bonus material’ (mostly code). See: computinggeographically.org.\n Spatial Simulation: Exploring Pattern and Process with George Perry is an advanced introduction to simple spatial simulation models for researchers in fields such as geography, planning, archaeology, ecology and beyond. George and I believe such models have great value across the ‘spatial sciences’ but the literature is scattered, making it a challenge for researchers to get started. We read all those mathematics, physics and statistics papers, so you won’t have to! A library of models in NetLogo accompanies the book, and can be downloaded here or explored at patternandprocess.org.\n Geographic Information Analysis with Dave Unwin, was first published in 2003, with a second edition in 2010, and translations into Mandarin and Korean. It has been a mainstay of my teaching (and that of many others!) since first publication, and offers an excellent, proven introduction to key principles and methods of spatial analysis and modelling. There is a steadily growing series of posts elsewhere on this site, supporting this book, most of them with example code in R."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "More than anything, I enjoy getting to the core of gnarly geospatial problems, and developing good ways to visualise, analyse, and ultimately, resolve them.\n\nConsulting\nI usually work in R and Python developing workflows to tidy, reformat, analyse, and visualize data, especially spatial data. I am not a developer, but can write well organised and documented code and integrate with your in-house coding standards and processes. Most of my work is linked from these pages, so have a look around, or check out my github.\n\n\nCommunicating\nI have presented complicated geospatial ideas to many different audiences thousands of times. My engaging web-enabled visual materials illuminate complex topics. See my talks, or the slides from a class for examples. I’ve also written 3 books and numerous articles, reviews, and book chapters.\n\n\nCartography\nI am a trained cartographer. I routinely create visual analytics and maps of complex data in projects, and in 25 years of research have published hundreds of diagrams and maps of complex data.\n\n\nTraining\nI have taught for 25 years about all things geospatial and can develop training materials tailored to your needs. Topics include geospatial analystics, cartography and visualization, and spatial simulation. Learn these things and more from a world authority on all of them!\n\n\nSimulation\nI love NetLogo. It is unrivalled for rapidly developing dynamic simulations of complex spatial systems. See, for example, this geographical COVID model I built in a matter of weeks). I’ve also written a book about simulation modelling, and can run training in this area as required."
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "",
    "text": "Not long ago I posted my ranking of xckd‘s ’bad map projections’ comics. Not long after that a new one appeared ‘interrupted spheres’ in comic #3122. For the record, I’d place it at number 4 in my list. It’s definitely a bad projection in all kinds of ways, but it is also thought provoking as I hope this post, where I reverse-engineer a version of the projection, will show.\nCode\nlibrary(sf)\nlibrary(units)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(smoothr)\nlibrary(geosphere)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(kableExtra)\nOne thing to note about the data in this post is that I made the file from two Natural Earth datasets—the countries data at 1:110M differenced with the lakes at 1:110M. This is because the countries, without the lakes, especially in North America just look wrong. By the time we are done, they’ll look a whole lot wronger, but it seems advisable to start from a good place.\nCode\nworld &lt;- st_read(\"world.gpkg\") |&gt;\n  st_set_geometry(\"geometry\")\nCode\nggplot() + \n  geom_sf(data = world)\n\n\n\n\n\n\n\n\nFigure 2: The source world data including lakes\nIt’s also worth noting that I had to repair a couple of minor issues in the data which were ultimately the root cause of some strange issues I was having running distance-based filtering. Lesson-learned: don’t ignore st_is_valid() == FALSE problems in your data if you’re planning on any analysis, but perhaps especially if you are doing weird projection transformations."
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#the-plan",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#the-plan",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "The plan",
    "text": "The plan\nReverse-engineering a projection is, as cartographers are fond of saying, an art and a science.1 My plan of attack as far as it went was to make a series of localised spherical views of subsets of the data, filtered based on the CONTINENT attribute in the dataframe. This wasn’t a terrible place to start, but Russia in particular—classified as Europe—presents some challenges to this approach. It also turns out that my chosen local spherical projection near-sided perspective(NSP) frequently introduces irreparable topological errors into data. That led to a diversion via the azimuthal equidistant projection to make it easier to select regions of Earth surface that approximate to continents based on distance from some central location, which reduced the topology problems.2\nSo what follows, traces the process of arriving at my approximation of the interrupted spheres projection, before assembling everything into a couple of functions that allow it to be run in a line or two of code.\nWe start by looking at making a single sphere."
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#a-single-sphere",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#a-single-sphere",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "A single sphere",
    "text": "A single sphere\nWe start with a single continent. For no particular reason,3 let’s go with Africa.\n\n\nCode\nafrica &lt;- world |&gt; filter(CONTINENT == \"Africa\")\nggplot() +\n  geom_sf(data = africa)\n\n\n\n\n\n\n\n\nFigure 3: Africa mapped showing the graticule\n\n\n\n\n\n\nThe near-side perspective projection\nThe near-sided perpsective projection simulates the view from space looking directly down at a specified central point, from some specified height above Earth’s surface. Here are three examples at a series of heights.\n\n\nCode\nplots &lt;- list()\nfor (h in c(1500e3, 3000e3, 6000e3)) {\n  proj &lt;- str_glue(\"+proj=nsper +lon_0=15 +lat_0=0 +h={h}\")\n  plots[[length(plots) + 1]] &lt;- ggplot() +\n    geom_sf(data = africa |&gt; st_transform(proj)) +\n    ggtitle(str_glue(\"From {h/1000}km\")) +\n    theme_void()\n}\nwrap_plots(plots, nrow = 1)\n\n\n\n\n\n\n\n\nFigure 4: The near-sided perspective projection of Africa at three different heights\n\n\n\n\n\nThe shape distortions introduced by more close-up, lower height views appear similar to those we see in the comic map. Eventually, if you go far enough out, the NSP projection becomes the orthographic projection’s view from infinity.\nIt’s useful to know the radius of the circles (of the sphere) in this projection. My initial attempts to do this involved converting polygons in the projected data to points and calculating the distance of the farthest point from the centre. It turns out that the radius of the circle is directly calculable. According to Snyder4 (who can be trusted on such matters), this is given by\n\\[\nr=R\\sqrt{\\frac{(P-1)}{(P+1)}}\n\\] where \\(R\\) is Earth’s radius, \\(P=h/R+1\\), and \\(h\\) is the height above Earth’s surface of the projection. We can make a function to return this result for an instance of the projection, or even more usefully to return a circle of that size.\n\n\nCode\nget_limit_of_nsper_proj &lt;- function(h, projstring, R = 6371008) {\n  P &lt;- h / R + 1\n  r &lt;- R * sqrt((P - 1) / (P + 1))\n  st_point(c(0, 0)) |&gt;\n    st_sfc() |&gt; \n    data.frame() |&gt;\n    st_sf() |&gt;\n    st_buffer(r) |&gt;\n    densify(20) |&gt;\n    st_set_crs(projstring)\n}\n\n\nThis allows us to make a map in the style of the xkcd projection (more or less).\n\n\nCode\nh &lt;- 2000e3\nproj &lt;- str_glue(\"+proj=nsper +lon_0=15 +lat_0=0 +h={h}\")\nglobe &lt;- get_limit_of_nsper_proj(h, proj)\nggplot() +\n  geom_sf(data = globe, fill = \"grey\", colour = NA) +\n  geom_sf(data = africa |&gt; st_transform(proj), fill = \"white\", color = \"black\") +\n  geom_sf(data = africa |&gt; st_transform(proj) |&gt; st_union(), \n          fill = NA, color = \"black\", lwd = 0.65) +\n  geom_sf(data = globe, fill = NA, colour = \"black\", lwd = 1) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 5: A ‘single sphere’ map of Africa with the near-sided perspective height set to 2000km, and a ‘globe’ drawn based on the function above\n\n\n\n\n\n\n\nThe azimuthal equidistant projection (and why we need it)\nSo far so good. We’re halfway there already.5\nAs I mentioned above, selecting by continent name isn’t entirely satisfactory. I could (in fact I did) edit the data to put Russia in Asia, but that felt a bit ad hoc, and instead I decided to base selection of shapes to include in each sub-sphere on specifying a distance on Earth’s surface along with a centre point of the projection. So the included shapes would be any that fall within that distance from the centre point.\nYou would assume6 that with ability we already have to create a circle for any given NSP projection, this would be simple enough. In practice, I found that the shapes resulting from the NSP projection, when I try to union them throw topological errors that seemed not to be repairable with st_make_valid(). The reason I union the shapes is to get the xkcd look where the outlines of landmasses are heavier than national boundaries. The easy option would be to not worry about that detail. I took the harder road of figuring out a workaround.\nAfter quite a bit of experimentation this involved selecting the world data based on a distance criterion, then transforming to a projection where it was safe to do the intersection of the globe with the selected shapes. This turned out to be an equidistant projection centred on the same spot as the NSP projection. The manual filtering by distance seems unnecessary, given the subsequent intersection step, but again, I found cases where this threw topological errors. My guess is that this would be caused by very small differences in the projected circle in the equidistant and NSP projections. In any case, the approach I have now seems not to run into such problems.7\nAnyway, here’s some code working through that process.\n\n\nCode\n1height_for_horizon &lt;- function(d, R = 6371008) {\n  R / cos(d / R) - R\n}\n\nd &lt;- 3500e3\nlon_0 &lt;- 15\nlat_0 &lt;- 0\ncentre &lt;- st_point(c(lon_0, lat_0)) |&gt;\n  st_sfc() |&gt;\n  st_set_crs(4326)\nh &lt;- height_for_horizon(d)\n\n2aeqd_proj &lt;- str_glue(\"+proj=aeqd +lon_0={lon_0} +lat_0={lat_0}\")\nnsper_proj &lt;- str_glue(\"+proj=nsper +lon_0={lon_0} +lat_0={lat_0} +h={h}\")\n\n3globe_nsper &lt;- get_limit_of_nsper_proj(h, nsper_proj)\n4globe_aeqd &lt;- globe_nsper |&gt;\n  st_transform(aeqd_proj)\n\nshapes_to_map &lt;- world |&gt;\n5  filter(drop_units(st_distance(geometry, centre)) &lt; d) |&gt;\n6  densify(20) |&gt;\n  st_transform(aeqd_proj) |&gt;\n7  st_intersection(globe_aeqd) |&gt;\n8  st_transform(nsper_proj)\n\n\n\n1\n\nIt’s useful to be able to determine the NSP height parameter given the desired radius from the central point. The formula \\(h=R/\\cos(d/R)-R\\) is easily derived by drawing the tangent to a circle from a point \\(h\\) away from it, and doing some trigonometry.\n\n2\n\nThis is the azimuthal equidistant projection.\n\n3\n\nOur local sphere…\n\n4\n\n… and it transformed to an equidistant projection at the same centre point.\n\n5\n\nInclude only shapes within the desired distance from the projection centre.\n\n6\n\nIt is good practice with highly generalized data to densify lines when performing projections.\n\n7\n\nDo the necessary intersection in the equidistant projection coordinate system.\n\n8\n\nConvert the resulting shapes into the desired NSP projection.\n\n\n\n\nAnd here is an output map, confirming that everything is still working.\n\n\nCode\nggplot() +\n  geom_sf(data = globe_nsper, fill = \"grey\", colour = NA) +\n  geom_sf(data = shapes_to_map, fill = \"white\") +\n  geom_sf(data = shapes_to_map |&gt; st_union(), fill = NA, lwd = 0.65) +\n  geom_sf(data = globe_nsper, fill = NA, colour = \"black\", lwd = 1) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 6: Single sphere map produced by specifying a radius of 3500km\n\n\n\n\n\nPutting all this together, we can make a function that for a given central point and specified distance returns the ‘shapes’ and ‘globe’ datasets needed to make a multi-sphere map.\n\n\nCode\nmake_local_spherical_projection &lt;- function(lon_0, lat_0, d) {\n  centre &lt;- st_point(c(lon_0, lat_0)) |&gt;\n    st_sfc() |&gt;\n    st_set_crs(4326)\n  h &lt;- height_for_horizon(d)\n  aeqd_proj &lt;- str_glue(\"+proj=aeqd +lon_0={lon_0} +lat_0={lat_0}\")\n  nsper_proj &lt;- str_glue(\"+proj=nsper +lon_0={lon_0} +lat_0={lat_0} +h={h}\")\n  globe_nsper &lt;- get_limit_of_nsper_proj(h, nsper_proj)\n  globe_aeqd &lt;- globe_nsper |&gt; \n    st_transform(aeqd_proj)\n  world_nsper &lt;-\n    world |&gt; \n    filter(drop_units(st_distance(geometry, centre)) &lt; d) |&gt;\n    densify(20) |&gt;\n    st_transform(aeqd_proj) |&gt;\n    st_intersection(globe_aeqd) |&gt;\n    st_transform(nsper_proj)\n  list(shapes = world_nsper, globe = globe_nsper)\n}"
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#many-spheres",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#many-spheres",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "Many spheres",
    "text": "Many spheres\nUsing the helper functions already written, we can now make a further function to iterate over multiple combinations of central location, radius, and a couple of other parameters (explained a little later) to give us a final map.\n\n\nCode\nmake_world_map &lt;- function(df) {\n  shapes &lt;- list()\n  globes &lt;- list()\n  for (i in 1:dim(df)[1]) {\n    shift &lt;- c(df$dx[i], df$dy[i])\n    a &lt;- df$rotn[i] * pi / 180\n    rot_m &lt;- matrix(c(cos(a), sin(a), -sin(a), cos(a)), 2, 2)\n    results &lt;- do.call(make_local_spherical_projection, df[i, 2:4])\n    shapes[[i]] &lt;- \n      results$shapes |&gt; \n      mutate(facet = df$facet[i], \n             geometry = (geometry * rot_m) + shift)\n    globes[[i]] &lt;- \n      results$globe |&gt; \n      mutate(facet = df$facet[i], \n             geometry = (geometry * rot_m) + shift) \n  }\n  shapes &lt;- shapes |&gt; bind_rows()\n  globes &lt;- globes |&gt; bind_rows()\n  outlines &lt;- shapes |&gt; group_by(facet) |&gt; summarise()\n  ggplot() +\n    geom_sf(data = globes, fill = \"grey\", colour = NA) +\n    geom_sf(data = shapes, fill = \"white\", colour = \"black\", lwd = 0.25) +\n    geom_sf(data = outlines, fill = NA, colour = \"black\", lwd = 0.5) +\n    geom_sf(data = globes, fill = NA, colour = \"black\", lwd = 0.75) +\n    theme_void()\n}\n\n\nHere’s a dataframe with settings for the required arguments.\n\n\nCode\nargs &lt;- data.frame(\n  facet = c(\"North America\", \"Europe\", \"Asia\", \"South America\", \n            \"Africa\", \"Antarctica\", \"Oceania\"),\n  lon_0 = c(  -88,    12,    95,   -55,    15,   -30,   148),\n  lat_0 = c(   40,    53,    35,   -18,     0,   -85,   -30),\n  d     = c(4.5e6, 2.7e6, 7.0e6, 4.5e6, 5.0e6, 3.0e6, 3.8e6),\n  dx    = c(-0.79, -0.11,  0.43, -0.67, -0.13,  -0.5,  0.79) * 1e7,\n  dy    = c( 0.38,  0.44,  0.39, -0.09,  0.03, -0.45, -0.09) * 1e7,\n  rotn  = c(    5,     0,   -25,    10,     0,   -15,     5)\n)\nargs |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\", full_width = FALSE) |&gt;\n  scroll_box(height = \"340px\")\n\n\n\n\n\n\nfacet\nlon_0\nlat_0\nd\ndx\ndy\nrotn\n\n\n\n\nNorth America\n-88\n40\n4500000\n-7900000\n3800000\n5\n\n\nEurope\n12\n53\n2700000\n-1100000\n4400000\n0\n\n\nAsia\n95\n35\n7000000\n4300000\n3900000\n-25\n\n\nSouth America\n-55\n-18\n4500000\n-6700000\n-900000\n10\n\n\nAfrica\n15\n0\n5000000\n-1300000\n300000\n0\n\n\nAntarctica\n-30\n-85\n3000000\n-5000000\n-4500000\n-15\n\n\nOceania\n148\n-30\n3800000\n7900000\n-900000\n5\n\n\n\n\n\n\n\nThe lon_0, lat_0, and d columns here correspond to the arguments of the same name in the make_local_spherical_projection function. I’ve chosen centre coordinates and radii for each continent by a process of trial and error. Having realised that I had this kind of flexibility I decided not to replicate the xkcd map, but to make my own new and improved version.\nThe facet attribute is in part to help keep track of which row is which, but more importantly is used in make_world_map to dissolve the shapes in each sphere into a single multi-polygon which can be outlined in a thicker line xkcd-style.\nAdditional arguments for the final map are dx, dy, and rotn. These are offsets in the x and y directions and a rotation to be applied to each sphere. The units of the offsets are uncertain, since we are mixing the output from seven different projections. Nominally, everything is in metres, but given the perspectival aspect and the fact that each of the seven projections is from a different height, they are not the same metres in each sphere. So for present purposes it made more sense to experiment until I got numbers that put the different spheres where I wanted them!\nAnd so… to a final map!\n\n\nCode\nmake_world_map(args)\n\n\n\n\n\n\n\n\nFigure 7: A different interrupted spheres map with added New Zealand, and Antarctica in a more suitable location8\n\n\n\n\n\nI think we can all agree this map is a considerable improvement on the original.9 I decided to have each sphere provide a more complete representation of its assigned continent. That means New Zealand is included in this map, and Iceland just sneaks in. Alaska still misses out. I feel quite strongly10 that Antarctica in the orientation shown in the xkcd original really belongs below South America, so I moved it.\nIt’s a fiddly process, but if you would prefer a replica of the original map, then feel free to reuse the code and modify the arguments dataframe. The worst part is that changing the area of coverage of a globe changes its size and requires changing all the positional offsets. There is certainly a better way. With any luck, some d3 guru (see the next section) will help us out."
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#polyhedral-projections",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#polyhedral-projections",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "Polyhedral projections",
    "text": "Polyhedral projections\nHaving taken this thing as far as I have it seems worth noting that there is clearly a point where these interrupted sphere projections bump into a genre of properly serious world projections, the polyhedral projections.11\nPolyhedral projections are where a polyhedron is wrapped around the sphere and that part of Earth’s surface ‘covered’ by each facet of the polyhedron is projected on to it, usually by gnomonic projection. In other words, a point on Earth surface is projected on to a facet of the enclosing polyhedron by drawing a line from the centre of Earth, through the point and on to the facet.\nThe d3 visualization platform is probably the best tool to use if you are a fan of this projection style, which is unfortunate, because I am a fan, and have bounced off d3 more than once. And of course it works with its own coordinate system and doesn’t play nice with the various standards for exchanging information about projections in geospatial. The results are very cool though. See for example: this, this, this, and this. Peak polyhedrality is to be found in van Wijk’s _myriahedral projections.12\nAnyway, I mention all this because with a bit of effort you can make a kind of ‘polyspherical’ (except they are obvious circles because they are flat) projection using the code I’ve written. Here’s a dodecahedral projection:\n\n\n\nCode\nmap(d3.geoDodecahedral())\n\n\n\n\n\n\n\n\n\nFigure 8: A dodecahedral projection (code to generate this map shamelessly copied from from Philippe Rivière (fil) at observablehq.com)\n\n\n\n\nAnd here’s an interrupted spheres projection arranged the same way.\n\n\nCode\nargs2 &lt;- data.frame(\n  facet = 1:12,\n  lon_0 = c(-12, -156, -84, -12, 60, 132, -120, -48, 24, 96, 168, -48),\n  lat_0 = c(90, 30, 30, 30, 30, 30, -30, -30, -30, -30, -30, -90),\n  d     = rep(5e6, 12),\n  dx    = c(3, 1.24, 2:4, 4.76, 1:4+0.5, 5.26, 2.5) * 6.2e6,\n  dy    = c(1.72, 1.1, rep(0.5, 3), 1.1, rep(-0.5, 4), -1.1, -1.72) * 4.4e6,\n  rotn  = c(0, 36, rep(0, 3), -36, rep(0, 4), 36, 0)\n)\nmake_world_map(args2)\n\n\n\n\n\n\n\n\nFigure 9: Interrupted spheres making like a dodecahedron"
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#final-thoughts",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#final-thoughts",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "Final thoughts",
    "text": "Final thoughts\nMaybe, given that my criterion in ranking the xkcd bad map projections was how thought-provoking they are, this one should be even higher than my provisional #4. Grappling with reverse-engineering it forced me to look into a few interesting rabbit holes.\nMore than anything it reinforces my sense that we really could use much more flexible architecture around map projections in geospatial. I am somewhat in awe of what has been done in the d3-verse in that regard, which is one reason for grabbing some code from that realm to show a dodecahedral map. Maybe I should give the 30 Day Map Challenge a go again, this time with a view to finally cracking d3/plot.\nWith that… I look forward with some trepidation to the next time an xkcd bad map projection appears…\n\n\nCode\nfunction map(projection) {\n  const context = DOM.context2d(width, width / 2);\n\n  projection.fitExtent([[2,2],[width-2, width/2-2]], {type:\"Sphere\"})\n  \n  var path = d3.geoPath(projection, context);\n  context.beginPath(), path({type:\"Sphere\"}), context.fillStyle = \"#bbb\", context.fill();\n  context.beginPath(), path(land), context.fillStyle = \"white\", context.fill();\n  context.beginPath(), path({type: \"Sphere\"}), context.strokeStyle = \"#000\", context.stroke();\n  \n  // Polyhedral projections expose their structure as projection.root()\n  // To draw them we need to cancel the rotate\n  var rotate = projection.rotate();\n  projection.rotate([0,0,0]);\n\n  // run the tree of faces to get all sites and folds\n  var sites = [], folds = [];\n  function recurse(face) {\n    var site = d3.geoCentroid({type:\"MultiPoint\", coordinates:face.face});\n    // site.id = face.id;\n    sites.push(site);\n    if (face.children) face.children.forEach(function(child){\n      folds.push({type:\"LineString\", coordinates: child.shared.map(\n        e =&gt; d3.geoInterpolate(e, site)(1e-5)\n      )});\n      recurse(child);\n    });\n  }\n  recurse((projection.tree || projection.root)());\n  \n  // folding lines\n  folds.forEach(fold =&gt; {\n      context.beginPath(),\n        context.lineWidth = 0.5, context.setLineDash([3,4]), context.strokeStyle = \"#000\",\n        path(fold), context.stroke();\n  });\n\n  // restore the projection’s rotate\n  projection.rotate(rotate);\n \n  return context.canvas;\n}\n\nd3 = require(\"d3-geo\", \"d3-fetch\", \"d3-geo-polygon\")\n\nland = d3.json(\n  \"https://unpkg.com/visionscarto-world-atlas@0.0.4/world/110m_land.geojson\"\n)"
  },
  {
    "objectID": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#footnotes",
    "href": "posts/2025-10-27-xkcd-3122-interrupted-spheres/index.html#footnotes",
    "title": "XKCD 3122 ‘Interrupted spheres’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this case, I think both words probably over-dignify the activity.↩︎\nPerhaps even solves them, but my code has not been exhaustively tested…↩︎\nAlphabetical is as good an order as any.↩︎\nPage 173 in Snyder PJ. 1987. Map Projections a Working Manual United States Government Printing Office↩︎\nTechnically, since there are seven continents (opinions vary), 1/7th of the way there.↩︎\nI certainly did.↩︎\nThis is all no doubt associated with floating point calculations, the perennial bugbear of all geometry in geospatial. Don’t get me started…↩︎\nIMHO.↩︎\nNo correspondence will be entered into on this matter.↩︎\nSometimes I surprise myself.↩︎\nTo be fair, polyhedral world projections weren’t really taken seriously until relatively recently.↩︎\nSee van Wijk JJ. 2008. Unfolding the Earth: Myriahedral projections Cartographic Journal 45(1) 32-42, and also here.↩︎"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#load-libraries",
    "title": "Experiments with R interpolators",
    "section": "",
    "text": "library(akima)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nThis notebook shows how we can use a set of paired ‘control points’ of a projection to interpolate unknown locations to that projection. The basic setup is a table of pairs of coordinate pairs \\((x_1,y_1)\\) and \\((x_2,y_2)\\) representing the same location in two different coordinate systems. Given this setup assuming that the projection is well-behaved with no serious ‘breaks’ we can form an empirical projection to estimate locations in one coordinate system for ‘unknown’ locations in the other. See, for example\n\nGaspar J A, 2011, “Using Empirical Map Projections for Modeling Early Nautical Charts”, in Advances in Cartography and GIScience Ed A Ruas (Springer Berlin Heidelberg), pp 227–247, http://link.springer.com/10.1007/978-3-642-19214-2_15"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#get-input-datasets",
    "title": "Experiments with R interpolators",
    "section": "Get input datasets",
    "text": "Get input datasets\n\nThe empirical projection\nThis file contains points on a global grid system, generated using the dggridR package. We can see the points in ‘lat-lon’ space below. Note how because this is a global grid system the points appear to ‘thin out’ towards the poles. This is an artifact of plotting the points in lat-lon, which is also explored in this post.\n\nemp_proj &lt;- read.csv(\"dgg-2432-no-offsets-p4-briesemeister.csv\")\nggplot(emp_proj) +\n  geom_point(aes(x = lon, y = lat), size = 0.05) +\n  coord_equal()\n\n\n\n\n\n\n\n\nInspection of the data shows we have two sets of coordinates lon, lat and x, y.\n\nhead(emp_proj)\n\n  ID dir     lon      lat          x       y\n1  0   .   11.25 58.28253  -428675.9 1520344\n2  1   . -168.75 58.28253 -1197290.8 7794188\n3  2   . -168.75 65.09003 -1120150.7 7234575\n4  3   . -168.75 72.07407 -1048583.9 6613223\n5  4   . -168.75 79.18998  -973499.6 5945572\n6  5   . -168.75 86.38746  -892969.4 5242144\n\n\nThis projection is Briesemeister, which is an oblique form of the Hammer-Aitoff projection. See\n\nBriesemeister W, 1953, “A New Oblique Equal-Area Projection” Geographical Review 43(2) 260\n\nIt’s possible to form this projection with a proj string, but it is not commonly supported in GIS, and who knows proj strings that well?! For the record, this is the string you are looking for:\n+proj=ob_tran +o_proj=hammer +o_lat_p=45 +o_lon_p=-10 +lon_0=0 +R=6371007\n\n\nA sample dataset\nWe also want a set of points to project, and what better than a world map. Note that we can only project points, so this is points along world coastlines, not polygons.\n\npts &lt;- read.csv(\"world_better.csv\") |&gt;\n  dplyr::select(lon, lat)\n\n# sanity check with a map\nggplot(pts) + \n  geom_point(aes(x = lon, y = lat), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#triangles-interpolator",
    "title": "Experiments with R interpolators",
    "section": "Triangles interpolator",
    "text": "Triangles interpolator\nThere are many different ways we can do this kind of interpolation. The simplest is based on triangulation. This method is available in the package interp but also in akima which is much quicker. The output x and y coordinates are formed by interpolating as shown below. x and y are the known locations of the input coordinate, which here are the longitude and latitude in out empirical projection dataset emp_proj. The desired outputs are at the longitude and latitude coordinates in the world maps dataset pts. And we do the interpolation twice, once for the x coordinate and once for the y coordinate in our target projection.\n\nx_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$x,\n                xo = pts$lon, yo = pts$lat)\ny_out &lt;- akima::interpp(x = emp_proj$lon, y = emp_proj$lat, z = emp_proj$y,\n                xo = pts$lon, yo = pts$lat)\n\nNow make up a results data table and map it. akima puts the result in a column z in its output.\n\nresult &lt;- data.frame(x = x_out$z, y = y_out$z)\nggplot(result) + \n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()"
  },
  {
    "objectID": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "href": "posts/2021-10-21-experiments-with-r-interpolators/experiments-with-R-interpolators.html#apply-the-empirical-projections-cut-region",
    "title": "Experiments with R interpolators",
    "section": "Apply the empirical projection’s cut region",
    "text": "Apply the empirical projection’s cut region\nWhat are those dots across the southern area of the map? These are points that happen to fall in triangles in the first coordinate system (i.e. lon-lat) where one corner of the triangle lies on a different side of a discontinuity in the projection than the other corners. We should avoid projecting points inside these triangles because they project (as we can see!) unreliably.\nFor the Briesemeister projection we know the precise location of this discontinuity, and have prepared a file delineating the ‘cut’ position. We can use this to remove points from the sample dataset that lie inside triangles that intersect the cut region.\nFirst, here is the discontinuity. Points close to or on this line could end up in very different parts of the projected output and so are ‘unsafe’ to project using our interpolation-based approximation.\n\ncut_sf &lt;- st_read(\"briesemeister-cut.geojson\")\n\nReading layer `briesemeister-cut' from data source \n  `/Users/david/Documents/code/dosull.github.io/posts/2021-10-21-experiments-with-r-interpolators/briesemeister-cut.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -179.8892 ymin: -82.94613 xmax: -0.3442386 ymax: 44.55223\nGeodetic CRS:  WGS 84\n\nggplot(cut_sf) + \n  geom_sf()\n\n\n\n\n\n\n\n\nNow triangulate the empirical projection data points, and assemble a polygon from all those triangles that are intersected by the discontinuity.\n\n# make the cut region into a sf dataset\nemp_proj_sf &lt;- emp_proj |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326)\n\ntriangles &lt;- emp_proj_sf |&gt;\n  st_union() |&gt;\n  st_triangulate() |&gt;   # triangulation of empirical projection points\n  st_cast() |&gt;\n  st_as_sf() \n\ncut_triangles &lt;- triangles |&gt;\n  st_filter(cut_sf)\n\ncut_region_sf &lt;- cut_triangles |&gt; \n  st_filter(cut_sf) |&gt;\n  st_union() |&gt;       \n  st_as_sf() \n\nWe quite reasonably get a warning that triangulation doesn’t really apply to geographical coordinates, but… akima did the interpolation by triangulating these points and it doesn’t know it’s unsafe (because it’s not a geospatial package). It’s not actually ‘unsafe’ as such in this case, because we aren’t using the triangulation for its metric properties anyway. So… we ignore this warning and plot this to see what we are dealing with\n\nggplot(triangles) +\n  geom_sf(colour = \"grey\") + \n  geom_sf(data = cut_triangles, fill = \"grey\", colour = \"white\") +\n  geom_sf(data = cut_region_sf, fill = \"#00000000\", colour = \"black\") +\n  geom_sf(data = cut_sf, color = \"red\")\n\n\n\n\n\n\n\n\nNow we use st_disjoint to remove points in the data to project that are inside the cut region.\n\npts_to_project_sp &lt;- pts |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n  st_set_crs(4326) |&gt;\n  st_filter(cut_region_sf, .predicate = st_disjoint) |&gt;\n  as(\"Spatial\")\n\nThe last step converts the points to the SpatialPointsDataFrame format of the sp package, which akima can also work with:\n\n# we also need the empirical projection data in the sp format\nemp_proj_sp &lt;- emp_proj_sf |&gt;\n  as(\"Spatial\")\n\nx &lt;- akima::interpp(emp_proj_sp, z = c(\"x\"), xo = pts_to_project_sp, linear = TRUE)\ny &lt;- akima::interpp(emp_proj_sp, z = c(\"y\"), xo = pts_to_project_sp, linear = TRUE)\n\nA bit unexpectedly, akima outputs the data to a two column dataframe with the interpolated values in a column with the same name as the input data, so getting the results into a final output table is as below.\n\nresult &lt;- data.frame(x = x$x, y = y$y)\nggplot(result) +\n  geom_point(aes(x = x, y = y), size = 0.05) + \n  coord_equal()\n\n\n\n\n\n\n\n\nAnd those rogue dots are all gone!"
  },
  {
    "objectID": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "href": "posts/2024-08-02-model-zoo-latest/model-zoo.html",
    "title": "The model zoo",
    "section": "",
    "text": "I’ve been diligently keeping the so-called ‘model zoo’ associated with our book Spatial Simulation updated for compatibility with new releases of NetLogo. Each time I generally manage to clean up a few parts of the code and slowly complete the ‘Info’ tabs (otherwise known as user documentation).\nThe models are available from this repo and you can get an overview of what they’re all about at this website. Really though, you should buy the book:"
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "",
    "text": "If you’ve seen me or my co-conspirator Luke Bergmann present at a cartographic meeting recently, there’s a fair chance you’ve heard us talk about using tiling and/or weaving to make maps of complex multivariate data.1 The idea is to break up map areas into multiple units by tiling so that we have available more than one shape to colour choropleth-style so that we can represent more than one variable in each area.\nPeople have been enthusiastic about the concept and the look of the maps, but (I’m fairly sure) reticent about taking up the idea because doing so demanded a willingness to dive into writing python code to make such maps, or even just to explore the idea. For example, this talk which gives a useful overview of the concepts is liberally sprinkled with snippets of python code. That was intended to make it easier for anyone interested to jump in and have a go at making their own maps. But realistically, it probably discouraged the vast majority!\nAnyway, I recently came across a tool called marimo that enables creation of reactive python notebooks, which are shareable as apps. That got me thinking about providing a way to make tiled maps without writing code, which might get a few more people interested in the idea."
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#a-prototype-web-app-mapweaver",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#a-prototype-web-app-mapweaver",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "A prototype web app: MapWeaver2",
    "text": "A prototype web app: MapWeaver2\nIt’s almost there.\nLoad the app in another tab by clicking the button below. It takes a while to load, so in the meantime keep reading…\nLaunch MapWeaver\nUsing the app you can experiment with different tiling designs on a multivariate dataset: Dan Exeter’s Index of Multiple Deprivation3 for the central Auckland region in 2018.\nYou can choose how many variables you want to include (between 2 and 10), and what kind of tiling or weave pattern you want to use. You can also choose colour ramps to associated with each of the chosen variables. So with a few clicks, you can make this map\n\nor this one\n\nYou can also modify the spacing or rotation of a chosen tiling, or (for a weave) the width of the strands\n\nor experiment with different colour ramps and the resulting map appearance"
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#feedback-welcome",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#feedback-welcome",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "Feedback welcome!",
    "text": "Feedback welcome!\nIt’s proving trickier than I hoped to upload user data to the app (see this issue) so this isn’t the finished product, which will allow you to upload and tile your own data and download the results for further refinement in a GIS or graphics package. Even so, it would still be useful to get any thoughts or reactions from prospective users. Keep in mind:\n\nI know it is slow to load. I don’t think there is much I can do about this in a pyodide Web Assembly app that depends on several python packages beyond the core language. Just be patient… or if you have experience with this problem, and know how to fix it, get in touch and tell me how!\nIt’s worth exploring the available options with spacing set to a high value so that the refresh of the map is faster. Once you have a pattern you are happy with, then adjust the spacing to the desired value.\n\nThat aside, any thoughts just get in touch on LinkedIn or email me! We’re also interested in thoughts about this mapping method, not just this implementation.\nFor more on how MapWeaver does what it does, check out the weavingspace code repository."
  },
  {
    "objectID": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#footnotes",
    "href": "posts/2025-03-09-mapweaver-beta/mapweaver-beta.html#footnotes",
    "title": "MapWeaver: tiled and woven multivariate maps without code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNothing at all to do with web map tiles.↩︎\nNothing at all to do with MapTiler↩︎\nSee also Exeter DJ, Zhao J, Crengle S, Lee A, Browne M, 2017, The New Zealand Indices of Multiple Deprivation (IMD): A new suite of indicators for social and health research in Aotearoa, New Zealand PLOS ONE 12(8) e0181260↩︎"
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html",
    "title": "Giscience 2025",
    "section": "",
    "text": "WARNING Quite a long post. Get yourself a cup of tea.\nWhile I’ve been a giscientist for a quarter of a century,1 I’ve only actually made it to four instances of its eponymous conference: Boulder 2002, Montréal 2016, Melbourne 2018, and now Christchurch 2025. For the Montréal and now Christchurch meetings I was on the program committee and therefore among the co-editors of the proceedings volume. And I have the beanie to prove it.2"
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#before-the-meeting",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#before-the-meeting",
    "title": "Giscience 2025",
    "section": "Before the meeting",
    "text": "Before the meeting\nAs a ‘remote-local’ organiser, I was involved in quite a few meetings over the last year or so as we assembled the conference program and made plans to welcome the world of giscience to Aotearoa. While I was closely involved in the paper and poster review process I can take very little credit for the success of the conference logistics, location, venue, A/V, refreshments, conference dinner, and so on. Credit for that must go solely to Vanessa da Silva Brum Bastos, Carolynne Hultquist, and most especially Ben Adams, the ‘local-local’ organisers, who did a spectacular job.\nIt’s never not a risk to bring an international meeting to New Zealand given the arduous travel involved in attending,3 and it is to the credit of the GIScience conference head honchos that they were willing to take a chance on us, and of the organising team that it was a success. It doesn’t do any harm that in population terms New Zealand is actually not the world’s most remote location. The fact of Aotearoa’s existence as a Asia-Pacific nation was abundantly clear with a preponderance of conference attendees from East Asia. A few brave souls made it all the way from Europe, a fair number from US universities (many of them PhD students supported by an NSF award secured by Song Gao), and there was a good local contingent from around New Zealand, and of course Australia.\nOne minor concern from a community perspective was that there seemed to be a distinctly bimodal distribution of seniorities: quite a few old-timers (among whom I must now consider myself), and a large number of graduate students (boosted by that NSF award) and early-career folks, but by casual observation, not so many mid-career researchers. Hopefully my anecdata is a poor reflection of the realities of the field, but I am a little concerned that a more robustly healthy community would have better representation across career stages."
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#keynotes",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#keynotes",
    "title": "Giscience 2025",
    "section": "Keynotes",
    "text": "Keynotes\nUnlike GeoCart, which tends to keynote overload, GIScience 2025 featured the regulation one keynote per day, with interesting presentations from Michelle LaRue (penguins!), Matt Duckham (research with impact), and Krzysztof Janowicz (‘Jano’) (can we know what AI knows?).\nMichelle made a compelling case for the impact of giscience—via remote sensed imagery—on her field, tracking penguin populations in Antarctica. In brief, before the advent of regular high resolution imagery we had no idea that there are so many emperor penguin colonies. We also had little idea of if or how colonies interact, or whether individuals migrate between colonies, and so on. Interesting stuff. The multiple scales at which penguins interact—competitively between colonies at up to continental scales, and collaboratively as colonies through winter, but competitively within colonies during nesting—provides great teachable material for thinking about spatial processes.4\n\n\n\nMatt beamed in from Melbourne, continued in the impact vein, encouraging us to think differently about how our research is valued. We tend as academics to focus on citations, which is… well… I was going to say fine, but really it’s very not fine. Citations are gameable, very much ‘inside baseball’, and not evidence of real engagement with the work. On that last point in particular, it’s not a point that Matt made,5 but I can’t help but notice how often citations to my work miss the point completely.6 Almost as if the citing author hadn’t read beyond the abstract. So… I’m very much here for different ways to value our own work and that of others. Having said that, I didn’t come away from Matt’s talk with a clear sense of what alternative methods might be. Not all research is well-placed to be taken up by end-user communities. Part of Matt’s argument was that a lot of giscience is well-placed in that sense, which is good for us. But perhaps a more radical take would be that we should be dropping league tables and metrics and so on entirely.7 It’s not at all clear that we’d lose anything in the process, except perhaps excessive numbers of not very informative papers.\n\n\n\nJano spoke very entertainingly about his ongoing efforts to figure out WTAF8 LLMs ‘think’ about geography. Along the way he mentioned more or less in passing a fascinating map of the fictional land of Allestone made by a 5-year old Thomas Williams Malkin which I had not heard of before. Allestone previously appeared in the giscience literature in a paper by Mike Goodchild and Linna Li,9 where they measured the fractal dimension of the coastline of Allestone as one way to check how reliable geographic data might be.10 In the different context of evaluating the geographical musings of AI, Jano suggested that this might be one way (Zipf’s rank size law is another) that we as giscientists could improve the geographical knowledge of our AI overlords. Reporting on an entertaining early exchange with an LLM Jano noted the machine’s confusion about the notion of measuring the distance between neighbouring countries. Sure enough, as I write, DuckDuckGo’s ‘Search Assist’ informs me, in response to the question ‘how far apart are russia and ukraine?’ that,\n\nThe distance between Russia and Ukraine varies depending on the specific locations being measured, but the driving distance from Kyiv, Ukraine, to Moscow, Russia, is approximately 1,044 miles (1,680 kilometers). The shortest flight distance is about 471 miles (758 kilometers)\n\nNot actually wrong but, yeah, nah, not really right either. The machines are coming for us, but they have a lot to learn from giscience, and giscientists may have a lot to gain from engaging with the development of more specifically geographical intelligence."
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#my-big-picture-take-away",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#my-big-picture-take-away",
    "title": "Giscience 2025",
    "section": "My big picture take away",
    "text": "My big picture take away\nA notable feature shared by all three keynotes (which might be a first for a GIScience conference) is that all three speakers were asked explicity political11 questions, concerning the politics of funding and the duty (or not) of scientists to take political and/or ethical positions.12\nTowards the end of Computing Geographically13 I suggest that,\n\ngiscience […] should really “never again be quite the comfortable retreat for the technically minded” (Goodchild, 200614, p. 687), which it remains, in spite of the best efforts of critical GIS scholars. (p 254)\n\nTo disentangle that quote, Mike Goodchild reflecting on the impact of Ground Truth suggested that that book meant GISers had to properly engage with the social and political implications of their work, and that this was an irreversible change. As the quote suggests, I remain unconvinced. Exhibit A would have to be the dominance over time of GIScience meetings by the ‘technically minded’.\nNow, to be clear, there’s nothing wrong with technical research. Technical research is absolutely necessary. Everything is political, but not everything we say about everything has to emphasise its political aspects. But to ignore its political aspects is itself political, and so, there is also nothing wrong with work that takes an avowedly political position in relation to the technical, or the purposes to which technical developments are put.\nThe sense that giscientists ‘get this’ seems to me to have been expressed in the questions to the conference keynotes, and also in the special sessions at the meeting, two of which directly focused on clearly political themes: privacy in the context of movement data, and geospatial work for sustainable cities and communities. But here’s the thing, some of the discussions on these topics are exactly the topics that our colleagues in geography departments have been researching for years, in subfields like critical GIS and digital geographies. We don’t have to approach themes like surveillance capitalism and privacy starting from scratch. There are literally hundreds of papers and books exploring these themes, and giscientists should be fully engaged with this more-than-technical geospatial stuff.\nOverall, it seems to me like there’s been some progress towards giscientists recognising that what we do matters in the world and not only on a technical level. Certainly, the program committee in 2025 was much more comfortable with lightly policing the traditionally fraught boundary between ‘theoretical’ and ‘applied’ work15 than the program committee in 2016. This lighter touch would not have passed muster in 2016, and certainly not in 2000 when this whole thing got started.16\nOf course, the applied vs. theoretical binary is not really the topic at hand. In Computing Geographically I argue at length that one way to take seriously the wider implications of our work would be for giscientists to engage with broader currents in geographical thought. I do this in a way that directs attention to some of those currents that seem to me the most promising for doing interesting technical work while also enhancing how diverse geographies are represented. It’s intended as an invitation for giscientists to really step away from the purely technical and fully engage with the messy (and yes, political) realities of what giscience / geospatial / GIS does in the world.17\nAnyway, rant over."
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#highlights",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#highlights",
    "title": "Giscience 2025",
    "section": "Highlights",
    "text": "Highlights\nWith three tracks, you can’t catch ’em all, so this is inevitably an incomplete perspective… In particular, I actively avoided sessions dominated by AI/ML methods, and for whatever reason, my personal highlights were improbably all packed in to the first day.\n\nRené Westerholt’s two presentations about spatial weights and how we should take them more seriously. I couldn’t agree more! We use spatial weights all the time in geographic analysis, and too often we gloss over where they came from or why we picked the weights we did. René and his team’s serious engagement with questions of how we choose spatial weights, and just as importantly how we understand and explain why we choose the weights we do, seems to me inarguable. I was particularly impressed with the detailed close reading of papers that they are working on in their literature review.\nIn the same vein I very much enjoyed the presentation by Simon Scheider and Judith Verstegen essentially making an argument about the limitations of spatial ‘prediction’ otherwise known as static spatial models when we want to understand cause and effect. Cause and effect are hopelessly entangled with one another in any spatial pattern. Our only hope of disentangling them is to explicitly represent the interacting processes and experiment with turning things on and off to see what happens. Inevitably, equifinality will not make this process foolproof, but it’s much more likely to help us understand how things got the way they are18 than statistical modelling, no matter how sophisticated.\nGus Ellerm’s paper (with Ben Adams and Mark Gahegan) on a framework for writing reproducible papers that update themselves as new data come in was a fascinating glimpse into how scientific results might get reported in years to come. Of course… speaking to Matt Duckham’s keynote on ‘impact’ it gave yet another reason to be wary of citations and counting papers as a measure of academic success. If I’m not mistaken, during the talk, Gus generated two new papers from the data!\nAmazingly, for me, at least on day 1, the good stuff kept coming. In a session on ‘Historical and Societal Perspectives’, Rere-No-A-Rangi Pope presented on georeferencing historical maps at scale. This is work I have been involved with so I’m not really an unbiased observer and won’t say any more. The presentation was well placed, sharing a session with work on the Biowhere gazetteer and a fun presentation by Sidney Wong self-confessed ‘computational sociolinguist’ exploring linguistic differences in reddit posts for different countries and regions in New Zealand. A little hard to shake off the question of how reflective of language use in everyday life in the world in the places concerned (versus at the keyboard, and not in those places at all), but even so an interesting talk, well given.\nThe posters were great. The standard was uniformly high, and the engagement of attendees with the poster presenters was enthusiastic. It was all a bit crowded for me, but I was able to spend some ‘quiet time’ later in the week with some of the posters and was very impressed. Furthermore, overall, as a program committee, I think we made the right call on most of this work which was well suited to poster presentation, while the papers I saw presented were well suited to that mode too.\nThe session I chaired was a bit of a wreck, with one presenter stranded by visa issues, and another presenting by video due to illness. But this did include the second of René Westerholt’s presentations (discussed above). And it was an unexpected, if mildly embarrassing, highlight to have Lex Comber (the video presenter) open with, “Thank you David for that kind introduction”,19 and then to have him both asking and answering his own questions at the end of the presentation. But damnit! He overran the time, even on video.\nChristchurch Town Hall where the conference dinner was held was magnificent (I had no idea), and Hagley Park, which I was able to walk through to and from the conference each day was a joy. It made me a little sad that for whatever reason conversations in 2023 about a possible position in the School of Earth and Environment at Canterbury eventually came to naught.\nAnd finally: (i) someone at the conference dinner was excited to meet me because they enjoy the website; (ii) I was reminded that Qian Sun of RMIT took a class in Python programming with me way back in uh… 2005; and (iii) May Yuan did eventually make it to the meeting, although sadly not for very long."
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#final-thoughts",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#final-thoughts",
    "title": "Giscience 2025",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe conference chairs Ben Adams and Mark Gahegan made an impassioned plea20 at the start of the conference for people to engage fully with the presentations, to be present for the presentations as it were, and set phones, and laptops, and email aside. I guess it’s a while since I’ve been at a conference, but I think people paid attention and did engage. There were always good questions after every presentation. Renée Sieber deservedly won the newly instituted ‘best question’ of the conference (really, a lifetime achievement award), but the general level of engagement was high.21\nWell done all!"
  },
  {
    "objectID": "posts/2025-09-10-reflections-on-giscience-2025/index.html#footnotes",
    "href": "posts/2025-09-10-reflections-on-giscience-2025/index.html#footnotes",
    "title": "Giscience 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLonger if you grant me membership when I gained my Masters in 1997↩︎\nI also have a reusable coffee mug/glass, which like the beanie has the conference logo which I accidentally designed. The higher resolution, more boldly coloured version shows the tiled map aspect of the design, which unfortunately is less apparent in the merch.↩︎\nEspecially now in the wake of COVID-19 with so many universities cutting international travel often under the guise of carbon consciousness, but given the parlous state of university funding globally, I’m not convinced that’s the whole story…↩︎\nFunny enough a picture of Adelie penguins is something I’ve used to talk about second order effects in spatial process for years.↩︎\nHe’s probably too nice a person for that.↩︎\nYes, yes, death of the author and all that, but alternative readings are one thing, and complete failures of reading are something else again.↩︎\nSee e.g., The University of Zurich’s recent parting ways with the Times Higher Education World University Ranking.↩︎\nMy term not Jano’s.↩︎\nGoodchild MF and L Li. 2012. Assuring the quality of volunteered geographic information. Spatial Statistics 1 110–120. doi: 10.1016/j.spasta.2012.03.002.↩︎\nThis might have indirectly been the prompt to my recent post on fractal dimension.↩︎\nThroughout this section by ‘political’ I mean ‘related to the exercise of power’ not party political.↩︎\nAnd importantly, not all those questions were asked by Renée Sieber.↩︎\nO’Sullivan D. 2024. Computing Geographically: Bridging Giscience and Geography. Guilford Press, New York.↩︎\nGoodchild MF. 2006. GIScience ten years after Ground Truth. Transactions in GIS, 10(5), 687-692↩︎\nThis was an undercurrent in Matt’s keynote too: applied work is often more ‘impactful’ even if it doesn’t tick the ‘theoretical’ box for some.↩︎\nDare I say it, GIScience is becoming a bit more like GISRUK…and that’s OK. A bit more applied and a bit less theoretical, if not any more political.↩︎\nSo go read my book!↩︎\nSee O’Sullivan D. 2021. Things are how they are because of how they got that way: Thoughts from the beach, on 50 years of Geographical Analysis. Geographical Analysis 53(1) 157–163.↩︎\nPro-tip if you find yourself having to pre-record a talk, find out who is chairing and thank them for their introduction. It’s guaranteed to catch them off-guard.↩︎\nWell… what passes for impassioned in so-laid-back-its-horizontal New Zealand.↩︎\nMaybe that missing mid-career demographic are the problem!↩︎"
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "Like a few of the earlier posts on a transformational approach to GIS this one turned into a rabbit hole, and so it will only cover field→point and field→line transformations. This takes us into the realm of surface networks and the journey has given me a better appreciation of some of the reasons that this seemingly promising approach to surface analysis has never quite taken off.1\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tmap)\nlibrary(whitebox)\nlibrary(cols4all)\nBefore we get started, a convenience function for making binary rasters into point data sets.\nCode\nraster_to_points &lt;- function(r) {\n  pts &lt;- r |&gt; \n    as.points() |&gt;\n    st_as_sf() |&gt;\n    rename(value = 1) |&gt;\n    dplyr::filter(as.logical(value))\n}"
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#from-fields",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#from-fields",
    "title": "GIS, a transformational approach",
    "section": "From fields…",
    "text": "From fields…\nThe data this time, perhaps a little ambitiously are a digital elevation model of the area around the Zealandia wildlife sanctuary in Wellington. This is an area of about 225 hectares surrounded by a predator proof fence that keeps out the various mammalian critters that have been so devastating to Aotearoa’s birdlife providing those birds with an opportunity to get established and thrive. In combination with local predator trapping efforts Zealandia has seen very encouraging signs of recovery in local native bird populations. Even in the few years we have been in Wellington the uptick in the numbers of kākā in particular has been obvious as a result of ‘spillover’ from the sanctuary into nearby suburbs. So much so that from time to time kākā are on the verge of becoming a pest.2\n\n\nCode\ndem &lt;- rast(\"zealandia-5m.tif\")\ntracks &lt;- st_read(\"tracks.gpkg\") |&gt; select(highway)\nfence &lt;- st_read(\"fences.gpkg\") |&gt; select(NAME, TYPE) |&gt; filter(TYPE == \"Main\")\nstreams &lt;- st_read(\"streams.gpkg\") |&gt; select()\nlake &lt;- st_read(\"lake.gpkg\") |&gt; select(name)\nroads &lt;- st_read(\"roads.gpkg\") |&gt; select(name, highway)\nhillshade &lt;- shade(terrain(dem, unit = \"radians\"), \n                   terrain(dem, v = \"aspect\", unit = \"radians\"), \n                   angle = 30, direction = 135)\n\n\nAnyway, here’s a map. The focus of this post is on the underlying DEM which characteristically for Wellington is rugged!\n\n\nCode\ntm_shape(dem) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"hcl.terrain2\", \n                                    limits = c(50, 400)),\n    col.legend = tm_legend_hide()) +\n  tm_shape(hillshade) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col_alpha = 0.25, col.legend = tm_legend_hide()) +\n  tm_shape(fence) +\n  tm_lines(col = \"purple\", lwd = 4) +\n  tm_shape(tracks) +\n  tm_lines(lwd = 1, lty = \"dashed\") +\n  tm_shape(streams) +\n  tm_lines(col = \"dodgerblue\") +\n  tm_shape(roads) +\n  tm_lines(col = \"black\", lwd = 2.5) +\n  tm_lines(col = \"white\", lwd =1.5) +\n  tm_shape(lake) +\n  tm_fill(fill = \"dodgerblue\") +\n  tm_scalebar(position = c(0.01, 0.04), text.size = .75,\n              bg = TRUE, bg.color = \"white\", bg.alpha = 0.5)\n\n\n\n\n\n\n\n\nFigure 1: General map of the Zealandia sanctuary showing predator proof fence (purple), tracks (dashed lines), and surrounding urban streets.\n\n\n\n\n\nOK, on to the code (such as it is)."
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#to-points",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#to-points",
    "title": "GIS, a transformational approach",
    "section": "… To points",
    "text": "… To points\nSurface specific points to be more precise. According to an early computer cartography text\n\nSurface-specific points have a higher information content than surface random-points. Surface-specific points, however, exhibit a hierarchy among themselves. These points are, in decreasing order, peaks and pits, passes, and ridge and course lines. These features not only tell their values, but also give some idea about their surroundings. In the environs of a peak, we can say that everything else will be lower (Peucker 19723, page 29)\n\nWe can find pits using a built in function in terra that operates on a flow direction surface,4 but we can also find pits and peaks directly with simple focal operations on the DEM.\n\n\nCode\npits &lt;- dem |&gt;\n1  focal(w = 3, \\(x) all(x[5] &lt; x[-5])) |&gt;\n  raster_to_points()\n\npeaks &lt;- dem |&gt;\n  focal(w = 3, \\(x) all(x[5] &gt; x[-5])) |&gt;\n  raster_to_points()\n\n\n\n1\n\n\\(x) defines an anonymous function of the 3x3 focal matrix x.\n\n\n\n\nAs we will see shortly, these functions are overly simplistic. They don’t handle situations where the value at a focal location (x[5]) and its neighbours (x[-5]) are very close, or where a peak or pit might extend over more than one pixel. This limitation is non-trivial to address in any raster representation of terrain, and doing so is really beyond the scope of this post.\nNevertheless, carrying on, if we combine these into a single point data set and tag them as pit or peak\n\n\nCode\npts &lt;- bind_rows(\n  pits |&gt; mutate(type = \"pit\"),\n  peaks |&gt; mutate(type = \"peak\")\n)\n\n\nthen we can see them on our original surface:\n\n\nCode\ntm_shape(dem) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"brewer.greys\"),\n    col.legend = tm_legend_hide()) +\n  tm_shape(streams) +\n  tm_lines(col = \"dodgerblue\") +\n  tm_shape(pts) +\n  tm_dots(fill = \"type\",\n    fill.scale = tm_scale_categorical(values = \"brewer.set1\"),\n    fill.legend = tm_legend_hide())\n\n\n\n\n\n\n\n\nFigure 2: Pits (blue) and peaks (red) on the original DEM surface\n\n\n\n\n\nIt’s easy to see that pits are generally found in valleys and peaks on ridge lines. We can get further reassurance on this by mapping the peaks and pits relative to the surface’s geomorphons5 using the Whitebox Tools whitebox::wbt_geomorphons() function.\n\n\nCode\ngeomorphon_labels &lt;- read.csv(\"geomorphons.csv\") |&gt;\n  pull(Landform.Type) |&gt;\n  as.vector()\nwbt_geomorphons(dem, \"geomorphons.tif\", search = 3, threshold = 0)\ngeomorphons &lt;- rast(\"geomorphons.tif\") |&gt;\n  subst(1:10, geomorphon_labels) \ngeomorphons_cropped &lt;- geomorphons |&gt;\n  crop(ext(ext(geomorphons) + c(-15, -15)))\n\n\nOverlaying our pits and peaks on the geomorphon map confirms that the locations of pits and peaks are where we would expect to find them. Here and elsewhere click on the map for a closer look.\n\n\nCode\ntm_shape(geomorphons_cropped) +\n  tm_raster(\n    col.scale = tm_scale_categorical(\n      levels = geomorphon_labels, \n      values = c(c4a_na(\"brewer.rd_bu\"), c4a(\"brewer.rd_bu\", n = 9))),\n    col.legend = tm_legend(\n      title = \"\", orientation = \"landscape\", \n      item.height = 1.5, item.width = 4)) +\n  tm_shape(pts) +\n  tm_dots(shape = \"type\", size = 0.5,\n    shape.scale = tm_scale_categorical(values = c(2, 6)),\n    shape.legend = tm_legend_hide()) +\n  tm_layout(\n    legend.frame.lwd = 0,\n    legend.width = 55,\n    legend.position = tm_pos_out(\n      cell.h = \"center\", cell.v = \"bottom\", pos.h = \"center\"))\n\n\n\n\n\n\n\n\nFigure 3: Geomorphons detected by whitebox tools with peaks (point-up triangles) and pits (point-down triangles)\n\n\n\n\n\n\nPasses (saddle points)\nThe other surface-specific points that are of interest are passes or saddle points. These have zero slope, but opposite signed curvature in (at least) two directions. By contrast, pits and peaks have the same signed curvature in all directions. Passes are trickier to identify than pits and peaks.\nI’ve written some code to do this in a crude fashion below, although as we’ll see shortly there is much more thorough prior work on this, which we will use in the next section.\n\n\nCode\ncount_sign_flips &lt;- function(m) {\n  bdy_indices &lt;- c(1, 2, 3, 6, 9, 8, 7, 4, 1)\n  sum(c(m)[bdy_indices[1:8]] * c(m)[bdy_indices[2:9]] &lt; 0)\n}\n\nis_pass &lt;- function(m) {\n  count_sign_flips(m) %in% c(4, 6, 8)\n}\n\npasses &lt;- dem |&gt; \n  focal(w = 3, \\(x) is_pass(x[5] - x)) |&gt;\n  raster_to_points() |&gt;\n  mutate(type = \"pass\")\n\n\nThe is_pass() function first determines how many times the sign flips as we go around the outer elements in a 3-by-3 matrix of height differences between those outer elements and the focal element in the matrix. Then, if the number of sign flips is 4, 6, or 8, the location is tagged as a pass.\nAnd here is the output:\n\n\nCode\ntm_shape(geomorphons_cropped) +\n  tm_raster(\n    col.scale = tm_scale_categorical(\n      levels = geomorphon_labels, \n      values = c(c4a_na(\"brewer.rd_bu\"), c4a(\"brewer.rd_bu\", n = 9))),\n    col.legend = tm_legend(\n      title = \"\", orientation = \"landscape\", \n      item.height = 1.5, item.width = 4)) +\n  tm_shape(passes) +\n  tm_dots(shape = 1, size = 0.35) +\n  tm_layout(\n    legend.frame.lwd = 0,\n    legend.width = 55,\n    legend.position = tm_pos_out(\n      cell.h = \"center\", cell.v = \"bottom\", pos.h = \"center\"))\n\n\n\n\n\n\n\n\nFigure 4: Passes mapped on top of geomorphons\n\n\n\n\n\nI should emphasise that none of the above is production code, not even close. For example, I know for a fact that the count of sign flips is sometimes an odd number, which should never happen (think about it). It also doesn’t handle situations where the height difference between a cell and any of its eight neighbours is zero. And, surprisingly, my code detects a lot more passes than we need to create a surface network (see next section). Smarter code would allow (as the geomorphon method does) for a ternary classification of the relationship between a cell and its neighbours (higher, lower, and almost level with), but as we’ll see in just a paragraph or three, these challenges have been addressed by others, so… moving on."
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#to-lines",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#to-lines",
    "title": "GIS, a transformational approach",
    "section": "… To lines",
    "text": "… To lines\nOK… so why did we go to all the trouble of finding passes (however inadequately)? Thereby hangs a tale. We can use these three types of surface-specific point to construct a topological representation of a surface known as a surface network, and that is the designated output for field to line transformations in that pesky table on page 26 of Geographic Information Analysis.\nTopological considerations tell us that a well-formed surface should be such that peaks, pits, and passes form a tripartite network with peaks connected to passes via ridges, and passes to pits via courses (or much better via thalwegs), and with no direct links between peaks and pits.6\nWork on surface networks in giscience in spite of its very early first appearances is spotty. Gerd Wolf, a leading researcher on the topic, writes—I think a little regretfully—about how the approach has been adopted in other fields.7 Wolf himself proposed a method for cartographic generalisation of surfaces using the information contained in surface networks (essentially by preserving their ridge and thalweg structure) as far back as 1988.8\nJo Wood’s still rather impressive Landserf software, which still runs in spite of the most recent published manual being dated 2009, can extract surface networks. Landserf is fun to explore, although its interface takes some getting used to.9\nThe most recent work I’ve found on this topic is by Eric Guilbert and colleagues (Guilbert 202110, Guilbert et al. 202311) and hallelujah! there is runnable code that produces well-formed surface networks.\nAn important part of this code is finding the right number of passes. This is because the topological constraints imply that\n\\[\n\\#_{\\mathrm{peak}} + \\#_{\\mathrm{pit}} - \\#_{\\mathrm{pass}} = 2\n\\]\nand it’s tricky to detect peaks, pits, and passes reliably due to the approximate nature of raster surface representations. In fact, comparing my results with Guilbert et al.’s my code’s naïve approach identifies too few pits and peaks, and (surprisingly) too many passes. Some of my problems are my old friend floating point numbers and the question of when is a really small number actually The Guilbert et al. papers linked in the footnotes go into some detail on how these issues can be resolved, but in essence it revolves around introducing the right number of diagonal connections between cells in the grid to maximise the number of saddle points detected, and satisfy the topological constraints.\nAnyhow, all that is by the by. As noted, there is runnable python code available. I ran it on the Zealandia DEM data12 and here’s the result.\n\n\n\n\n\n\nFigure 5: The surface network generated by Eric Guilbert’s python code for the Zealandia study area, showing ridges (red) and thalwegs (blue)\n\n\n\nThe surface network representation allows the terrain to be partitioned into hills (regions surrounded by thalwegs) and dales (regions surrounded by ridges). Terrain faces with two thalweg and two ridge sides, and a sequence of corners pit-pass-peak-pass can also be identified. The latest work by Eric Guilbert and colleagues looks to me like it goes a long way to putting surface networks into useful relation with more established methods of terrain analysis that tend to emphasise (understandably so) the identification of hydrological networks."
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#a-pause-for-breath",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#a-pause-for-breath",
    "title": "GIS, a transformational approach",
    "section": "A pause for breath",
    "text": "A pause for breath\nHave I mentioned before how outrageously cheeky is the casual suggestion that you see how many of these transformations you can accomplish,\n\n[i]f you already know enough to navigate your way around a GIS (Geographic Information Analysis, page 26)\n\nMy belated apologies to anyone traumatised by taking this suggestion literally. Indeed these posts on a transformational approach to GIS have been an interesting exercise in discovering how preposterous that throwaway remark really is.\nOn a more positive note, surface networks deserve more attention than they’ve gotten from the community over the decades, but their reliable construction has proved elusive. Perhaps now that there is at least one tool out there that is up to the task they’ll make their way into the mainstream and beginner would-be geospatial analysts won’t be left wondering why they can’t construct a surface network in spite of already knowing enough to navigate their way around a GIS.\nNext up, and closing out this series will be field→area and field→field transformations."
  },
  {
    "objectID": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#footnotes",
    "href": "posts/2025-10-23-gia-chapter-1B-part-4/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTL;DR; data problems make constructing surface networks challenging.↩︎\nWe worry for our gutters from time to time—those beaks are powerful.↩︎\nPeucker T. Computer Cartography. Resource Paper No. 17, College Geography Commission, Association of American Geographers, 1972↩︎\nAnd by inverting the surface we can also find peaks.↩︎\nSee Jasiewicz J and TF Stepinski. 2013. Geomorphons — a pattern recognition approach to classification and mapping of landforms. Geomorphology 182 147–156.↩︎\nPfaltz JL. 1976. Surface networks. Geographical Analysis 8(1) 77-93.↩︎\nWolf GW. 2014. Knowledge diffusion from GIScience to other fields: the example of the usage of weighted surface networks in nanotechnology. International Journal of Geographical Information Science 28(7) 1401-1424.↩︎\nWolf GW. 1988. Weighted surface networks and their application to cartographic generalization. In W Barth (ed) Visualisierungstechniken und Algorithmen. Informatik-Fachberichte, 182. Springer, Berlin, Heidelberg. If you’re really keen complete FORTRAN code is available in this paper: Wolf GW. 1991. A FORTRAN subroutine for cartographic generalization. Computers & Geosciences 17(10) 1359-1381.↩︎\nWhile Jo’s morphometric work remains impressive (I might use Landserf for one of the remaining posts in this series…) he has long since switched focus to visualization methods, and that work is well worth checking out.↩︎\nGuilbert E. 2021. Surface network extraction from high resolution digital terrain models. Journal of Spatial Information Science 22 33-59↩︎\nGuilbert E, F Lessard, N Perreault, and S Jutras. 2023. Surface network and drainagenetwork: towards a common datastructure Journal of Spatial Information Science 26 53-77↩︎\nI’m not crazy: I tried it first on something much simpler.↩︎"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html",
    "title": "GeoCart’2024",
    "section": "",
    "text": "Another (even-number year) August coming to a close and another GeoCart’ is over.\nThe passing last year of Igor Drecki inevitably cast a shadow. But importantly, and even under that shadow, it was a fun meeting, as Igor would have wanted. Given the current woes of Wellington’s public sector, attendance was as expected down a little, but there were still enough faces old and new for the meeting to retain its usual friendly buzz, without it being too overwhelming.\nThe organisers of GeoCart’2024 can be proud of what they achieved in putting on a meeting that Igor would have enjoyed. It bore all the hallmarks of Igor’s vision for the meeting. Many keynotes (perhaps too many, but more on that below…) and many opportunities for informal discussion and exchange in the breaks between presentations, over lunch, and at the Icebreaker and Conference Dinner events. The latter in particular was really excellent—I don’t recall ever before having FOUR choices of starter, main, and dessert at a catered sit-down meal like this one: well done Dockside.1"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#keynotes",
    "title": "GeoCart’2024",
    "section": "Keynotes",
    "text": "Keynotes\nSo many keynotes…\nA two-and-a-half day meeting with six (yes SIX!) ‘all-hands’ presentations seems perhaps a little too much of a good thing. Having said that, all the keynotes were excellent. Like, really excellent. I didn’t necessarily agree with everything everyone said, but I was certainly engaged throughout. In chronological order:\nOcean Mercier kicked things off with ‘Te Taunaha Whenua: Mapping Connections to Place’ emphasising the role that maps and mapping play in defining and making place, and the more than coincidental relationship between those processes and colonisation and (yay!) decolonisation. I loved that Ocean also shared her longtime passion for orienteering— and its slightly goofy (but highly functional) maps. She was even heading off :rogaining one evening of the conference.2\n\n\n\nDaniel Huffman in their presentation ‘Sharing the Emotional Work of Mapping’ spoke to a topic that has been front of mind for me lately: the rollercoaster ride of the uncertainties of taking on projects as they appear, and balancing ambitions for your creative work against the expectations of paying clients. I have to confess to some disappointment that we didn’t get some Huffman eye candy along the way, but instead had to content ourselves with :Madison, Wisconsin’s—admittedly very nice—flag.\n\n\n\nDaniel O’Donohue. After Daniel H’s quietly reflective take on the emotional journey of making maps, next morning podcaster and self-described geospatial evangelist Daniel O gave everyone a good kick in the pants encouraged us all to get busy making millions on the interwebs. Some of this talk leaned a bit too much for cycnical old gen-X-er me into the social media influencer vibe, but more than a couple of people were taking notes, and I expect to see mass-market cartography rocket to the top of Aoteroa’s export earnings charts in the years to come. Seriously though, I did have a couple of conversations with folks prompted by this talk to explore the possibilities for getting paid more for their skills. Being underpaid is a problem in cartography/geospatial,3 so I wish them all the best, and will join them, just as soon as I come up with an idea. Also: we all got socks.4\n\n\n\nDavid Garcia somehow managed to keep the energy going in a talk entitled ‘Revisiting the Social Nature of Mapping’. As in Daniel O’s talk, the memes were flowing, but so were the serious messages echoing both Ocean’s and Daniel H’s talks from the previous day in taking seriously the emotional heft of cartography, and its roles in colonisation and decolonisation. The recently minted Dr. Garcia wears his learning lightly and with great good humour, but his work is deeply serious. For a sense of just how serious, here’s the thesis in full. I have made my own (marginal) contributions to critical GIS and critical cartography, and even managed to inject a little of it into the program at VUW but I still learned a great deal from this talk (and I also laughed a lot…).\n\n\n\nSarah Bell introduced the final day with an emotionally5 nuanced presentation pushing us to carefully consider the many ways that maps can be biased. So far so obvious,6 but this talk added a few more dimensions for the manipulation of an audience, most poignantly sound, in an animated revisiting of Snow’s cholera data. As co-author of a book whose first edition cover featured :the John Snow map, I was fully prepared to yawn my way through that part of the presentation, but this was very compelling.\n\n\n\nAnd finally, Wendy Shaw Toitū Te Whenua - Land Information New Zealand and Secretary of Ngā Pou Taunaha o Aotearoa - New Zealand Geographic Board, on the 100th anniversary of the board, updated us on progress in the ongoing consideration and restoration of Māori placenames across Aotearoa. The quick summary is that much has been done, and that much remains to be done. I scored a couple of the associated maps to add to Daniel O’s socks.\n\n\n\n\nToo much of a good thing?\nSo… I’ll say it again: all the keynotes were excellent. I can’t help wondering even so, if this balance in the meeting foregrounds the accomplishments of the already well-recognised, to the disadvantage of the great work being done by others.\nI am only really say this comparing GeoCart’s complement of keynotes relative to other conferences of similar size, where there is more likely to be one keynote a day, not two. In part the meeting follows this schedule to justify its two-and-a-half day duration so that there is space for two evening social events. There is likely not enough submitted work to make for two parallel tracks of presentations over that length of time. But I do wonder if in a small meeting like this two tracks are really needed. Cutting things back to three keynotes and a single track would make this a non-issue—and nobody would have to miss any of the talks!\nBut really, it’s not actually a problem, especially not if the keynotes can maintain this level of quality. And I know that for Igor a key mission for GeoCart was to connect cartography in Aotearoa New Zealand with the wider world, and keynotes from afar are one important way to do this. And having our visiting keynotes see the quality of the work in Aotearoa as exemplified in the ‘home team’ keynotes doesn’t hurt either.\nOne thing I do know is that finding that many keynote-worthy speakers every two years is a challenge: don’t hesitate to offer suggestions to the NZGS Committee!"
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#other-highlights",
    "title": "GeoCart’2024",
    "section": "Other highlights",
    "text": "Other highlights\nAs noted, with two tracks, you can’t catch ’em all, so this is inevitably a partial perspective… but among the highlights for me were:\n\nKarl Baker spoke with great good humour and impressive clarity about the often rather sorry tale of the mishandling of New Zealand’s longest placename in maps. Say it with me: Te Taumata­whaka­tangi­hanga­kōauauo­tamatea­turi­pūkākā­piki­maunga­horo­nuku­pōkai­whenua­kita­na­tahu.7 You can get some insight into the sorry tale of the use and abuse of the name from the wikipedia talk page for the place name, and there’s a concise correct pronunciation :here.\nAndy Tyrell presented a really cool workflow for automating the production of linework style oblique terrain maps like these.\nCraig Devereux’s map of the Roman Empire which won the map contest was a standout and hard to beat.\nJessie Colbert’s presentation of work with Katarzyna Sila-Nowicka and Dan Exeter visualising maps of multiple deprivation was a model of completeness and clarity. I hope to maybe get involved in this work with my ‘dots within dots’ and weaving contributions.\nSam van der Weerden gave a great great talk about Maynard Design’s work on the mapping for Auckland’s cycle network. Really interesting: particularly so was the insight that perhaps as cartographers we should sometimes resist our instinct to differentiate things by colour. After all, no matter what kind of cycle path it is, it’s still a cycle path and should be coloured green, and perhaps the less important differences among types of cycle path should be differentiated by line style not colour!\nDid I mention the dinner at Boatshed?\n\n\nSociety business\nAlso worth mentioning, the New Zealand Cartographic Society after somewhat inexplicably being refused permission to dual list its name in English and te reo Māori way back in 1989 (when doing so would have been well ahead of the curve) is finally en route, remaining bureaucratic hurdles notwithstanding, to adopting a te reo Māori name: Te Rōpū Tuhi Whenua o Aotearoa."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#finally-permit-me-a-moment-of-self-promotion",
    "title": "GeoCart’2024",
    "section": "Finally, permit me a moment of self-promotion",
    "text": "Finally, permit me a moment of self-promotion\nI enjoyed giving a talk on time-space maps of mountainous terrain. Here, as a bonus is an animated time-space map of a loop walk around Taranaki Maunga, which probably should have made it into the talk, but for one reason or another did not."
  },
  {
    "objectID": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "href": "posts/2024-08-30-geocart-24/geocart-24.html#footnotes",
    "title": "GeoCart’2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDisclaimer: I have not been recompensed in any form by Dockside.↩︎\nThat word rogaining is very weird derived from the given names of its ‘inventors’ ROd, GAil, and NEil. The names of new sports aren’t what they used to be: see also pickleball.↩︎\nThis recent episode of the MapScaping podcast is thought-provoking on the subject.↩︎\nI’m wearing mine now.↩︎\nThere’s that word again.↩︎\nMonmonier’s How To Lie With Maps anyone?↩︎\nI’ve gone with the longer not yet official form.↩︎"
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "Channeling Nick Chrisman, on page 26 of Geographic Information Analysis we presented ‘a transformational approach to GIS operations’.1 in a table that looks something like this:2\nThis table is an artifact from early in giscience’s development, when imposing some kind of order on the multiplicity of ways that spatial data can be processed was one promising approach to formalising what it is people are doing when they ‘do GIS’. It shares some DNA with the flowcharts that used to show up in statistics textbooks as an aid to choosing the right statistical test given the nature of your data, although it’s perhaps a little less evolved than that, and more focused on simply getting oriented to the wide array of things you can do with spatial data.\nBut, if you have ever attempted to navigate the geoprocessing toolboxes in either Esri products or QGIS and find a specific operation you need as you piece together a workflow, the value of imposing this kind of order on things is probably clear. If nothing else, it might help you home in on a subset of tools that might work for you, narrowing the search space. Schemes like this are particularly important as efforts are made to automate geospatial workflows: it wouldn’t do your not-so-friendly neighbourhood LLM any harm to absorb the information in this table.3\nIn Geographic Information Analysis there is a text box immediately following the table that brightly suggests\nIn the spirit of that innocuous little ‘exercise for the reader’, I thought I’d take some of my own medicine and give this exercise a go. At this precise moment (as I write these words) I don’t quite know what to expect of this exercise, but I can already see that some of these are going to be tricky.4\nI can also see that 16 (count ’em!) geospatial transformations is a lot so rather than tackle them all in a single post, I’m going to do one row of the table in a series of four posts.5"
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#from-points",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#from-points",
    "title": "GIS, a transformational approach",
    "section": "From points…",
    "text": "From points…\nMy current everyday ‘GIS’ is the spatial stack in R, so here goes…\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cols4all)\nlibrary(patchwork)\n\ntheme_set(theme_void())\nset.seed(1)\n\n\nSince we are in the ‘From point’ row of the table, we need a point dataset. We don’t need it to be real, so let’s just make one. In case we need to put the points on a map or relate them to some other dataset, and to prevent sf or terra moaning about projections, we’ll put the points in a known projection (New Zealand Transverse Mercator, EPSG 2193), somewhere near Wellington, but really, these could be anywhere. In time-honoured statistical fashion I’ve set \\(n\\) to 30.\n\n\nCode\npoints &lt;- data.frame(x = rnorm(30, mean = 1.745e6, sd = 1000), \n                     y = rnorm(30, mean = 5.425e6, sd = 1000),\n                     z = rexp(30) * 500)\npoints_sf &lt;- points |&gt;\n  st_as_sf(coords = c(\"x\", \"y\"), crs = 2193)"
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-points",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-points",
    "title": "GIS, a transformational approach",
    "section": "… to points",
    "text": "… to points\n\nMean centre\nThe table of all geospatial knowledge™ proposes ‘mean centre’ as a transformation between points. More basic ones might be various kinds of translation, rotation, and so on. But as instructed, here’s one way to find the mean centre of a bunch of points.\n\n\nCode\npoints_centre &lt;- points_sf |&gt;\n1  st_union() |&gt;\n  st_centroid()\n\nggplot() +\n  geom_sf(data = points_sf, colour = \"black\") +\n  geom_sf(data = points_centre,\n          colour = \"red\", shape = 4, size = 5)\n\n\n\n1\n\nIf we don’t union the points, we just end up with the same set of points all over again…\n\n\n\n\n\n\n\nMean centre of a set of points\n\n\n\n\nAnd we are off and running."
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-lines",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-lines",
    "title": "GIS, a transformational approach",
    "section": "… to lines",
    "text": "… to lines\n\nNetwork paths\nUnfortunately, the first hurdle shows up pretty quickly. ‘Network paths’ is open to many interpretations. I am going to go with making a network of connections among the points based on some notion of neighbourhood, although that doesn’t narrow things down a whole lot.\nUsing the now fairly mature sfdep package along with sfnetworks many options are available, so here are just two, the 3-nearest neighbours and Gabriel graph of our set of points. Once made, the networks contain both edge and vertex data, so to get strictly the lines as a standard sf object we have to ‘activate’ the edges and convert to an sf dataset.\n\n\nCode\nlibrary(sfdep)\nlibrary(sfnetworks)\n\nG_k3 &lt;- points_sf |&gt;\n  st_as_sfc() |&gt;\n  st_as_graph(\n    points_sf |&gt; st_knn(3)) |&gt;\n  activate(\"edges\") |&gt;\n  st_as_sf()\n \nG_gabriel &lt;- points_sf |&gt;\n  st_as_sfc() |&gt;\n  st_as_graph(\n    points_sf |&gt; st_as_sfc() |&gt; st_nb_gabriel()) |&gt;\n  activate(\"edges\") |&gt;\n  st_as_sf()\n\n\nAnd here they both are:\n\n\nCode\ng1 &lt;- ggplot() + \n  geom_sf(data = G_k3) +\n  geom_sf(data = points_sf) +\n  ggtitle(\"3 nearest neighbours\")\n\ng2 &lt;- ggplot() +\n  geom_sf(data = G_gabriel) + \n  geom_sf(data = points_sf) +\n  ggtitle(\"Gabriel graph\")\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 1: 3 nearest-neighbour and Gabriel graphs of our point dataset\n\n\n\n\n\nThe Gabriel graph is a thinned out close relative of the Delaunay triangulation, which we’re about to see a couple of sections later in this post.\nFollowing recent presentations by René Westerholt at GIScience 2025 I’ll have more to say about these kinds neighbourhood structures soon.6"
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-areas",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-areas",
    "title": "GIS, a transformational approach",
    "section": "… to areas",
    "text": "… to areas\n\nProximity polygons\nThis is straightforward enough. Sensible proximity polygons should probably be clipped to a region close to the input points, and you also have to join the data from the points back to the polygons. The minimal solution that yields the polygon geometries is shown in the first operation below, while reattaching the data and clipping are applied in the following operation.\n\n\nCode\nprox_polygons_min &lt;- points_sf |&gt;\n1  st_union() |&gt;\n  st_voronoi() |&gt;\n  st_cast()\n\nprox_polygons &lt;- prox_polygons_min |&gt;\n  st_intersection(\n    points_sf |&gt; \n      st_buffer(1500) |&gt;\n      st_bbox() |&gt; \n      st_as_sfc()) |&gt;\n  data.frame() |&gt;\n  bind_cols(\n    points_sf |&gt; st_drop_geometry()) |&gt;\n  st_sf()\n\nggplot() +\n  geom_sf(data = prox_polygons) +\n  geom_sf(data = points_sf)\n\n\n\n1\n\nA union operation is necessary otherwise the proximity polygons will be generated for each point in isolation.\n\n\n\n\n\n\n\nProximity polygons for our point data\n\n\n\n\n\n\nTriangulated irregular network (TIN)\nOne way to make a TIN is the Delaunay triangulation, which is similar to operations in either of the previous two sections. There is an st_nb_delaunay function in sfdep which is applied in the same way as st_nb_gabriel. But that function yields a graph of edges, not areas, and since it’s areas we are trying to make, here is the vanilla sf method, which yields polygons.\nAs with the st_voronoi function, we have to apply a slightly bewildering series of operations to get the output as a bona fide sf.\n\n\nCode\ntin &lt;- points_sf |&gt;\n1  st_union() |&gt;\n  st_triangulate() |&gt;\n  st_cast() |&gt;\n  st_sfc() |&gt;\n  data.frame() |&gt;\n  st_sf()\n\nggplot() +\n  geom_sf(data = tin) +\n  geom_sf(data = points_sf)\n\n\n\n1\n\nAgain, a union operation is necessary otherwise the triangulation will be generated for each point in isolation, which would be meaningless.\n\n\n\n\n\n\n\nDelaunay triangulation of our point data\n\n\n\n\n\n\nPoint buffers\nBuffers are the ur-method in GIS and easily generated. Since our points have a z attribute we’ll vary the buffer size using that variable.\n\n\nCode\npt_buffer &lt;- points_sf |&gt;\n  st_buffer(dist = points_sf$z) \n\nggplot() +\n  geom_sf(data = pt_buffer, fill = NA) +\n  geom_sf(data = points_sf)\n\n\n\n\n\nBuffers of our point data\n\n\n\n\nThe buffer regions are actually polygons, but I’ve rendered them showing their boundaries only, so that we can see the overlaps."
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-fields",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#to-fields",
    "title": "GIS, a transformational approach",
    "section": "… to fields",
    "text": "… to fields\nThis is where things start to get properly gnarly. The vector-raster divide remains surprisingly tricky to cross in almost all geospatial platforms. In R that’s exacerbated by the reliance on different packages with different APIs, which can make it less than obvious how to proceed.\n\nInterpolation\nI could be sneaky here, and argue as we do in Chapter 9 of the book that proximity polygons are already a form of interpolation. But you probably want to see some ‘real’ interpolation. That requires us to make a statistical model, and then apply it to predict values at some set of point locations.\nHere’s the statistical model for inverse-distance weighting, made in gstat.\n\n\nCode\nlibrary(gstat)\n\n1fit_IDW &lt;- gstat(\n2  formula = z ~ 1,\n3  data = as(points_sf, \"Spatial\"),\n4  set = list(idp = 2)\n)\n\n\n\n1\n\nMakes a statistical model.\n\n2\n\nThe column z is what we are interested in.\n\n3\n\nUsing sf but converting to sp, which is required here.\n\n4\n\nidp = 2 specifies an inverse-distance square.\n\n\n\n\nNext we set up some target locations in both raster and point formats where the model will be used to ‘predict’ values. The easiest way to do this is making an empty raster with some required resolution. I use the prox_polygons as a convenient extent extending beyond the range of the point data. Arguably, when interpolating we shouldn’t go much beyond the extent of the point data, but there’s no real harm in this situation.\n\n\nCode\ntarget_raster &lt;- prox_polygons |&gt;\n  as(\"SpatVector\") |&gt;\n  rast(res = 25)\n\ntarget_points &lt;- target_raster |&gt;\n  as.points() |&gt;\n  st_as_sf()\n\n\nNow we can interpolate into the target locations. First into the set of points, and then (since the purpose here is to make a raster), rasterizing the points into an actual raster data set. After doing that we rename the variable to something a bit more user-friendly than var1.pred.\n\n\nCode\ninterp_pts &lt;- predict(fit_IDW, target_points)\n\n\n[inverse distance weighted interpolation]\n\n\nCode\ninterp_rast &lt;- rasterize(\n  as(interp_pts, \"SpatVector\"),\n     target_raster, field = \"var1.pred\")\nnames(interp_rast) &lt;- \"z\"\n\n\nAnd finally, we can make a map of what we got. This is all a bit circular, because I am using geom_raster for plotting, which requires that I reconvert the raster back to xyz data, but the essential point is that we’ve completed the transformation from point data to an interpolated raster surface!\n\n\nCode\nggplot() +\n  geom_raster(\n    data = interp_rast |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = z)) +\n  geom_sf(data = points_sf) +\n  scale_fill_continuous_c4a_seq(palette = \"hcl.terrain2\")\n\n\n\n\n\n\n\n\nFigure 2: Inverse-distance weighted interpolation of the point data\n\n\n\n\n\n\n\nKernel density estimation\nIt’s rather mysterious that kernel density is not straightforward to apply in R ‘out of the box’.\nOne easy approach is that ggplot will do a form of density estimation purely visually, using geom_density2d and often a visual output is all we need. Purists have issues with the smoothing applied by geom_density2d—so I gather from the documentation of the eks package, which we look at in passing below—but this is quick and easy.\n\n\nCode\nggplot() +\n  geom_density2d(data = points, aes(x = x, y = y)) +\n  geom_sf(data = points_sf)\n\n\n\n\n\n\n\n\nFigure 3: The result of geom_density2d applied to point data\n\n\n\n\n\nThat’s all very well. But doesn’t get us the desired raster output data. One option would be to round trip the data via the density functionality in spatstat. I’ve shown how to do that in a post from four years ago here.\nOther options are spatialEco::sf.kde() and eks::st_kde(). The latter does everything properly but insists on choosing its own kernel bandwidth, and how that bandwidth is determined is somewhat obscure, or at the very least highly technical. In day to day use I think of KDE as primarily a visualization method, and in that context, I want to be able to specify kernel bandwidths in units of the map projection and see how the map changes as the bandwith varies.\nspatialEco::sf.kde gives me that control.\nOne oddity here is that the raw output from sf.kde has rectangular, not square cells, even though I supply the function call with a ‘reference’ raster that has square cells! I think this is because spatialEco is using eks::st_kde behind the scenes. Anyway, this behaviour accounts for my reprojecting the results into the target_raster.7\n\n\nCode\nlibrary(spatialEco)\n\nkde &lt;- points_sf |&gt;\n  sf.kde(bw = 1500, ref = target_raster, res = 25,\n         standardize = FALSE, scale.factor = 1) |&gt;\n  terra::project(target_raster)\n\nkde_weighted &lt;- points_sf |&gt;\n  sf.kde(y = points_sf$z, \n         bw = 1500, ref = target_raster, res = 25,\n         standardize = FALSE, scale.factor = 1) |&gt;\n  terra::project(target_raster)\n\ng1 &lt;- ggplot() +\n  geom_raster(\n    data = kde |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = z)) +\n  geom_sf(data = points_sf) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  ggtitle(\"Unweighted KDE\")\n\ng2 &lt;- ggplot() +\n  geom_raster(\n    data = kde_weighted |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = z)) +\n  geom_sf(data = points_sf, aes(size = z), alpha = 0.35) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  ggtitle(\"Weighted KDE\")\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 4: The result of spatialEco’s sf.kde function applied to our point data\n\n\n\n\n\nsf.kde’s scaling behaviour is not especially well documented, but by setting standardize = FALSE and scale.factor = 1 I get estimates of the density that appear to be in the units of the projection, i.e. counts per square metre in the present example. This is nice to know if you do happen to want to use an estimated density surface to, for example, aggregate cell densities up to estimaged counts for a set of areas.\nYou can also weight the points using the y parameter in the sf.kde function call, but it’s not clear to me that the resulting density estimate respects the total weight associated with the data. If it did, I would expect the value of the weighted surface to be 100 or more times greater than in the unweighted case.\nAnother possibility here, if you’d rather not use Gaussian kernels, but instead stick with the kernels that are typically available in a GIS, then the SpatialKDE package has you covered.\n\n\nCode\nlibrary(SpatialKDE)\n\nkde_2 &lt;- points_sf |&gt;\n  kde(band_width = 1500, scaled = TRUE,\n      grid = target_raster |&gt; as(\"Raster\"), quiet = TRUE) |&gt;\n  as(\"SpatRaster\")\nnames(kde_2) &lt;- \"kde_value\"\n\nkde_2_weighted &lt;- points_sf |&gt;\n  kde(band_width = 1500, scaled = TRUE, weights = points_sf$z,\n      grid = target_raster |&gt; as(\"Raster\"), quiet = TRUE) |&gt;\n  as(\"SpatRaster\")\nnames(kde_2_weighted) &lt;- \"kde_value\"\n\ng1 &lt;- ggplot() +\n  geom_raster(\n    data = kde_2 |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = kde_value)) +\n  geom_sf(data = points_sf) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  ggtitle(\"Unweighted KDE\")\n\ng2 &lt;- ggplot() +\n  geom_raster(\n    data = kde_2_weighted |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = kde_value)) +\n  geom_sf(data = points_sf) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.reds\") +\n  ggtitle(\"Weighted KDE\")\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 5: The result of SpatialKDE’s kde function applied to our point data\n\n\n\n\n\nHere, we get the difference in densities we expect between the unweighted and weighted case. The SpatialKDE::kde() function complains about the size of the target raster being large, so it likely doesn’t scale especially well. As is apparent, the interpretation of the specified bandwidth is not directly comparable to the bandwidth associated with the Gaussian kernel produced by the spatialEco::sf.kde function.\nAll in all, there are no shortage of options for kernel density estimation in R. SpatialKDE::kde() is probably the closest to running the operation in a GIS, other than using a package like qgisprocess to access this functionality in QGIS directly.\n\n\nDistance surface\nAfter the complexities of KDE, it’s nice that calculating the minimum distance to any one of a set of points from every cell in a raster is straightforward, although we do the calculation using the target_points data and then convert to a raster.\n\n\nCode\ndistance_surface &lt;- target_points |&gt;\n  mutate(\n    distance = st_distance(geometry, points_sf) |&gt; \n    apply(MARGIN = 1, min)) |&gt;\n  as(\"SpatVector\") |&gt;\n  rasterize(target_raster, field = \"distance\")\nnames(distance_surface) &lt;- \"distance\"\n\nggplot() +\n  geom_raster(\n    data = distance_surface |&gt; as.data.frame(xy = TRUE),\n    aes(x = x, y = y, fill = distance)) +\n  geom_sf(data = points_sf, colour = \"white\", shape = 1) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.prgn\")\n\n\n\n\n\n\n\n\nFigure 6: The minimum distance to points in our data as a distance surface\n\n\n\n\n\nIt’s fun to notice here that the approximate boundaries of many of the proximity polygons (more or less) emerge in the distance surface, at least when coloured on a diverging scheme as here.\nOK. That’s quite enough of that for one day. Next time I post one of these, I’ll assume you have line data."
  },
  {
    "objectID": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#footnotes",
    "href": "posts/2025-09-09-gia-chapter-1B-part-1/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChrisman N. 1999. A transformational approach to GIS operations. International Journal of Geographical Information Science. 13(7) 617-637. doi: 10.1080/136588199241030.↩︎\nI can’t begin to tell you how long it took to make this table in HTML. At some point it was obvious that a screenshot of the actual table from the book would be a better idea, but my obstinate streak kicked in, and here we are. Saying that, I expect it took almost as long to make the original in Word, back in the day. Plus ça change.↩︎\nCome to think of it, I expect it long since has.↩︎\nEnjoy the schadenfreude.↩︎\nNote from future self: that’s not quite how it turned out when things got messy in transformations from areas.↩︎\nAnd also thoughts on GIScience 2025 itself…↩︎\nCall me old fashioned, but I like my raster cells square.↩︎"
  },
  {
    "objectID": "posts/2025-08-13-temperature-dials/index.html",
    "href": "posts/2025-08-13-temperature-dials/index.html",
    "title": "Mean temperature dials",
    "section": "",
    "text": "In this post I show one way that a time series can be symbolised on a map using a tiling-based approach.\nBecause the example data in this post are monthly mean temperatures I’ve called this post ‘mean temperature dials’ but the idea is applicable to any sequential data.1 In this case, given monthly data, 12 slices make an analogy with clockfaces an obvious one, but there is no reason that this method couldn’t be used with other numbers of time steps.\nI’m sure there are other ways to make maps like these,2 but here I apply the tiling tools in my weavingspace Python module. The tiling pattern used in this example particularly makes sense for cyclical data.\nAnyway, the data are as shown below.3\nCode\nimport geopandas as gpd\n\nnz_temps = gpd.read_file(\"data/nz-avg-temps.gpkg\")\nprint(nz_temps.head())\n\n\n         Jan        Feb        Mar        Apr        May        Jun  \\\n0  19.505730  19.889172  19.054600  17.195982  15.189572  13.520309   \n1  19.348455  19.754276  18.935886  17.079866  15.101235  13.446260   \n2  19.381758  19.818325  19.018015  17.199263  15.224555  13.585258   \n3  19.498827  19.888285  19.062172  17.150700  15.166919  13.491609   \n4  19.519226  19.936918  19.119717  17.222164  15.210229  13.567471   \n\n         Jul        Aug        Sep        Oct        Nov        Dec  \\\n0  12.600797  12.972336  13.583259  14.676813  16.179123  17.992401   \n1  12.530506  12.867455  13.486817  14.565450  16.039982  17.834658   \n2  12.671771  12.993229  13.603150  14.670104  16.126814  17.862686   \n3  12.591331  12.931929  13.605309  14.708787  16.189005  18.011904   \n4  12.650140  12.988663  13.651422  14.751802  16.205250  18.013773   \n\n                                            geometry  \n0  POLYGON ((19222240.002 -4101542.362, 19222240....  \n1  POLYGON ((19231516.626 -4101542.362, 19231516....  \n2  POLYGON ((19240793.25 -4101542.362, 19240793.2...  \n3  POLYGON ((19231516.626 -4112809.727, 19231516....  \n4  POLYGON ((19240793.25 -4112809.727, 19240793.2...\nWe can retrieve a months list from the column names in the data.\nThe average temperatures have somewhere along the way been granted fake super high precision, so we round to one decimal place, and also get the overall minimum and maximum temperatures. This is so we can symbolise all the temperatures on the same colour scale in the final maps.\nCode\nmonths = [m for m in nz_temps.columns if m != \"geometry\"]\nnz_temps.loc[:, months] = nz_temps.loc[:, months].round(1)\nt_min = min(nz_temps.loc[:, months].min())\nt_max = max(nz_temps.loc[:, months].max())"
  },
  {
    "objectID": "posts/2025-08-13-temperature-dials/index.html#make-a-tiled-map-dataset",
    "href": "posts/2025-08-13-temperature-dials/index.html#make-a-tiled-map-dataset",
    "title": "Mean temperature dials",
    "section": "Make a tiled map dataset",
    "text": "Make a tiled map dataset\nNow we make a TiledMap using weavingspace. This is essentially an overlay operation underneath, but internally weavingspace handles ‘copying and pasting’ the tileable unit across the map area to give a tiled pattern. All the particulars are stored in a TiledMap object, which has a .map attribute that contains all the needed data.\n\n\nCode\nfrom weavingspace import TiledMap\nfrom weavingspace import Tiling\n\ntiled_map = Tiling(tile_unit, nz_temps).get_tiled_map(\n  join_on_prototiles = True, retain_tileables = True)\n\n\nAnd here’s what that tiled_map.map data looks like.\n\n\nCode\nprint(f\"{tiled_map.map.shape = }\\n{tiled_map.map.head()}\")\n\n\ntiled_map.map.shape = (5724, 15)\n  tile_id  prototile_id                                           geometry  \\\n0       a           153  POLYGON ((19864662.651 -4511236.578, 19883478....   \n1       b           153  POLYGON ((19864662.651 -4511236.578, 19878451....   \n2       c           153  POLYGON ((19864662.651 -4511236.578, 19869704....   \n3       d           153  POLYGON ((19864662.651 -4511236.578, 19859621....   \n4       e           153  POLYGON ((19864662.651 -4511236.578, 19850874....   \n\n    Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec  \n0  18.3  18.4  17.3  15.1  12.8  10.8  10.1  10.4  11.8  13.3  14.9  16.8  \n1  18.3  18.4  17.3  15.1  12.8  10.8  10.1  10.4  11.8  13.3  14.9  16.8  \n2  18.3  18.4  17.3  15.1  12.8  10.8  10.1  10.4  11.8  13.3  14.9  16.8  \n3  18.3  18.4  17.3  15.1  12.8  10.8  10.1  10.4  11.8  13.3  14.9  16.8  \n4  18.3  18.4  17.3  15.1  12.8  10.8  10.1  10.4  11.8  13.3  14.9  16.8  \n\n\nWe’re only showing a few lines of the data table here, but it contains 5724 tiles (i.e., 477×12).\nThe key attribute here in addition to the data sourced from the monthly temperatures is the tile_id, which we can use in mapping to split out the tiles into sets each of which we associate with a different variable. Here, we have 12 sets of tiles, and 12 variables, and we can make some maps."
  },
  {
    "objectID": "posts/2025-08-13-temperature-dials/index.html#making-tiled-multivariate-maps",
    "href": "posts/2025-08-13-temperature-dials/index.html#making-tiled-multivariate-maps",
    "title": "Mean temperature dials",
    "section": "Making tiled multivariate maps",
    "text": "Making tiled multivariate maps\nThe weavingspace.TiledMap class has a render() method which makes a map with a single call, but since we want to apply exactly the same colour palette to every set of tiles, here it makes sense to split the tiles out manually by looping over them.5\n\nA static map\nTo make a static map, we loop over the tile_id variable and month variable names, do a selection on the map dataset to pick out only those tiles with a given tile_id, and add those to a geopandas.plot() coloured by the value of the associated month’s temperatures. Note the sequence of tile_id values required, as we saw in Figure 1 and #fig-clip-by-circle that is the order of the tile_id value starting at 12 o’clock and going clockwise round the tile unit. Data limits on the colour ramp are specified with vmin and vmax and are the same for every set of tiles.\n\nCode\nfig = plt.figure(figsize = (10, 14))\nax = fig.add_subplot(111)\nax.set_axis_off()\n\nfor tile_id, month in zip(list(\"dcbalkjihgfe\"), months):\n  tiled_map.map.query(\"tile_id == @tile_id\") \\\n  .plot(ax = ax, column = month, cmap = \"RdYlBu_r\",\n        vmin = t_min, vmax = t_max, linewidth = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: A static map showing monthly average temperatures\n\n\n\n\n\nA web map\nGiven the scale of the data, it’s useful to put them on a web map. The core of the code is essentially identical to the static map, but with slightly different parameter names needed to get a similar effect.\n\nCode\nimport folium\nfrom folium.plugins import Fullscreen\nfrom folium import LayerControl\n\nm = folium.Map(location = [-41, 173.5], tiles = \"CartoDB Positron\",\n               zoom_start = 5, min_zoom = 4, max_zoom = 14)\n\nfor tile_id, month in zip(list(\"dcbalkjihgfe\"), months):\n  m = tiled_map.map.query(\"tile_id == @tile_id\") \\\n  .explore(m = m, column = month, cmap = \"RdYlBu_r\",\n            legend = False, vmin = t_min, vmax = t_max,\n            tooltip = month, name = month,\n            style_kwds = {\"weight\": 0, \"fillOpacity\": 0.8})\nLayerControl(position = \"topleft\").add_to(m)\nFullscreen(position = \"topright\", force_separate_button = True).add_to(m)\n\nm\n\n\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFigure 4:   A web map of the monthly average temperatures\n\n\n\nIf you zoom in on the map you can see which month corresponds to which pie slice and get a better feel for how the visualization works.\nIf this post has piqued your interest in this approach to mapping multivariate data, then you can find out more at the weavingspace github repo or even experiment with making such maps using the MapWeaver web app."
  },
  {
    "objectID": "posts/2025-08-13-temperature-dials/index.html#footnotes",
    "href": "posts/2025-08-13-temperature-dials/index.html#footnotes",
    "title": "Mean temperature dials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis approach originated with a map of world anthromes designed by Luke Bergmann and myself, which was displayed in the 2022 NACIS meeting Map Gallery. However, that map had only three ‘snapshots’ in time so didn’t really convey a sequence of data points in the same way that the example developed in this post does.↩︎\nMuch derided pie charts come to mind.↩︎\nI prepared the data in R using the geodata package to download WorldClim data followed by raster-to-vector shenanigans using terra and sf.↩︎\nAlso known as Laves tiling 4.6.12.↩︎\nI’ll add this functionality to the to-do list for weavingspace. For now, doing it by hand is instructive.↩︎"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html",
    "title": "Low level handling of sf objects",
    "section": "",
    "text": "You can handle sf objects at a low level but it can take a bit of getting used to, and you have to watch out for floating point gotchas.\nknitr::opts_chunk$set(error = TRUE, message = TRUE)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#packages",
    "title": "Low level handling of sf objects",
    "section": "Packages",
    "text": "Packages\nEverything here needs just sf and dplyr.\n\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#making-polygons",
    "title": "Low level handling of sf objects",
    "section": "Making polygons",
    "text": "Making polygons\nMy main confusion dealing with polygons in sf sounds dumb, but was easily fixed. Matrices in R get populated by column, by default, where the points in a polygon are in the rows of the matrix (as they would be in a dataframe with x and y attributes). You just have to make sure to populate the matrices in the right order.\nThere’s also the slightly strange fact that you have to wrap a matrix of points in a list to make a polygon.\nSo because of the row-column thing, there’s a tendency to do\n\nmat &lt;- matrix(c(0, 0,\n                1, 0,\n                1, 1,\n                0, 1,\n                0, 0), nrow = 5, ncol = 2)\nsquare &lt;- st_polygon(list(mat))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBut that fails, because the matrix we made was\n\nmat\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    0    0\n[3,]    1    1\n[4,]    0    0\n[5,]    1    0\n\n\nand the first and last rows don’t match (even if they did, it’s not actually a polygon!).\nBut specify that the matrix should be populated byrow and all is well\n\nsquare &lt;- st_polygon(list(matrix(c(0, 0,\n                                   1, 0,\n                                   1, 1,\n                                   0, 1,\n                                   0, 0), nrow = 5, ncol = 2, byrow = TRUE)))\nplot(square)\n\n\n\n\n\n\n\n\nIf you happen to have vectors of the x and y coordinates, then it’s easier.\n\nx &lt;- c(0, 1, 1, 0, 0)\ny &lt;- c(0, 0, 1, 1, 0)\nsquare &lt;- st_polygon(list(matrix(c(x, y), nrow = 5, ncol = 2)))\nplot(square)"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#floating-point-coordinates-and-their-discontents",
    "title": "Low level handling of sf objects",
    "section": "Floating point coordinates and their discontents",
    "text": "Floating point coordinates and their discontents\nsf defaults to using floating point calculations which has some annoying side-effects. For example, the code below results in an error\n\nangles &lt;- 0:3 * 2 * pi / 3\nx &lt;- cos(angles)\ny &lt;- sin(angles)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 7, ncol = 2)))\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\n\nBecause sf defaults to floating point it doesn’t consider the polygon closed due to precision issues that mean R considers sin(0) != sin(2 * pi):\n\nsin(0) == sin(2 * pi)\n\n[1] FALSE\n\n\nThere is no easy way to fix this except to round the coordinates!\n\nx &lt;- round(x, 6)\ny &lt;- round(y, 6)\ntriangle &lt;- st_polygon(list(matrix(c(x, y), nrow = 4, ncol = 2)))\nplot(triangle)\n\n\n\n\n\n\n\n\nThere’s not a lot you can do about this when you are constructing sf objects. Polygons must be closed, and equality is strictly applied to the opening and closing points. You can’t ask st_polygon to automatically close polygons for you.\nOnce you have polygons to work with, the problem can come back to bite you, but there is a way around it. For example, this works OK:\n\nsquare |&gt; \n  st_difference(triangle) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nBut let’s make two squares that are theoretically adjacent to one another, but happen to have non-integer coordinates (which… is pretty commonplace!)\n\nangles &lt;- seq(1, 7, 2) * 2 * pi / 8\nangles &lt;- c(angles, angles[1])\nx1 &lt;- cos(angles)\ny1 &lt;- sin(angles)\n\ns1 &lt;- st_polygon(list(matrix(c(x1, y1), nrow = 5, ncol = 2)))\nbb &lt;- st_bbox(s1)\ns2 &lt;- s1 + c(bb$xmax - bb$xmin, 0)\n\nplot(s1, xlim = c(-1, 2.1))\nplot(s2, add = TRUE)\n\n\n\n\n\n\n\n\nTwo squares, next to one another as we might hope, but if, for example, we st_union them we get a MULTIPOLYGON.\n\ns3 &lt;- st_union(s1, s2)\ns3\n\nMULTIPOLYGON (((0.7071068 0.7071068, -0.7071068 0.7071068, -0.7071068 -0.7071068, 0.7071068 -0.7071068, 0.7071068 0.7071068)), ((2.12132 0.7071068, 0.7071068 0.7071068, 0.7071068 -0.7071068, 2.12132 -0.7071068, 2.12132 0.7071068)))\n\n\nIf we plot them, they still appear separate\n\nplot(s3)\n\n\n\n\n\n\n\n\nand if we measure the distance between them, turns out they don’t touch at all, but are in fact a miniscule distance apart…\n\ns1 |&gt; st_distance(s2)\n\n             [,1]\n[1,] 3.330669e-16\n\n\nIt’s probably not necessary to point out how silly this is, even if it is strictly correct.\n\nRXKCD::getXKCD(2170)$img\n\n\n\n\n\n\n\n\n[1] \"https://imgs.xkcd.com/comics/coordinate_precision.png\""
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#specifying-precision-for-spatial-operations",
    "title": "Low level handling of sf objects",
    "section": "Specifying precision for spatial operations",
    "text": "Specifying precision for spatial operations\nBy contrast if we use rgeos functions the equivalent union operation works as we might expect (although we do have to feed rgeos the old sp types of polygon, which we can do via a call to as(\"Spatial\")…)\n\nrgeos::gUnion(as(s1, \"Spatial\"), as(s2, \"Spatial\")) |&gt;\n  st_as_sfc() |&gt;\n  plot()\n\nError in loadNamespace(x): there is no package called 'rgeos'\n\n\nsf does allow us to effectively emulate the rgeos behaviour, albeit not for simple geometries. When we instead bundle geometries up into feature collections, we can assign them a precision, and this will take care of the kinds of problems we see above:\n\ns1_sfc &lt;- s1 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\ns2_sfc &lt;- s2 |&gt; \n  st_sfc() |&gt;\n  st_set_precision(1e8)\n\ns1_sfc |&gt;\n  st_union(s2_sfc) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThe first time I looked this up in help, I got it wrong due to careless reading, and, I think, assuming that the number you provide to st_set_precision() was a ‘tolerance’, or, in effect a ‘snap distance’. The help is also a bit roundabout, and directs you to this page, for an explanation of how it works.\nIn effect all coordinates are adjusted by applying a function like this one:\n\nadjust_precision &lt;- function(x, precision) {\n  round(x * precision) / precision\n}\nsqrt(2) |&gt; adjust_precision(1000)\n\n[1] 1.414\n\n\n\nst_snap\nAnother possible fix for the floating point issue is snapping points to the coordinates of another object before applying operations. So this works, although it is not as clean as the st_precision option. On the other hand, it does work on plain geometry objects, not only on those that have been bundled up into collections.\n\ns1 |&gt; st_snap(s2, 1e-8) |&gt;\n  st_union(s2) |&gt;\n  plot()"
  },
  {
    "objectID": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "href": "posts/2021-12-08-low-level-handling-sf-objects/low-level-sf-objects.html#in-conclusion",
    "title": "Low level handling of sf objects",
    "section": "In conclusion",
    "text": "In conclusion\nThe tools for making and manipulating geometries at a low level are available in sf but they are not always as simple as you’d like. Of course, most often you are dealing with datasets and that’s where sf comes into its own. Just remember st_set_precision() and you should be able to avoid quite a few headaches…"
  },
  {
    "objectID": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "href": "posts/2021-09-23-dulux-colours-map/dulux-colours-map.html",
    "title": "Mapping the Dulux colours",
    "section": "",
    "text": "Dulux have had a range of colours named for places in New Zealand for several years now. Clearly this is an opportunity for mapping too good to be missed. You can thank me later.\n\nGo to the map\nI gave a presentation about how this map was made using R to a Maptime! Wellington audience, and it takes you through the code in enough detail to require no further explanation from this post!"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html",
    "href": "posts/2025-06-01-nsew-island/index.html",
    "title": "North-south or east-west islands?",
    "section": "",
    "text": "Rather obviously, the proper names for Aotearoa New Zealand’s two largest islands are Te Ika-a-Maui and Te Waipounamu. It’s not just that those are the names given by the indigenous peoples of the whenua (and also official names), it’s that the extremely boring ‘North Island’ and ‘South Island’ are wrong. Or at any rate, not entirely right.\nThat’s a bold claim. Let me explain with the help of some uh… in-depth spatial analyis.\nCode\nlibrary(sf)\nlibrary(units)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nnz &lt;- st_read(\"nz-islands.gpkg\")\nSo here’s a usefully labelled map of the three big islands along with quite a few smaller offshore islands.1\nCode\nggplot(nz) + \n  geom_sf(aes(fill = north), lwd = 0) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  theme_void()\nI considered colouring Te Ika-a-Māui blue and Te Waipounamu red, for the respective colours of their allegedly strongest sports teams. But I’m not much invested in rugby, and Wellington are clearly the best cricket team,2 so I’ve inverted what many might consider the ‘natural’ colours for the two islands. This is also a nice echo of one of my favourite spreads from Chris McDowall and Tim Denee’s wonderful We Are Here:\nAnyway, if you look at that north-south map, it’s not entirely clear that east-west isn’t just as accurate a binary. If we randomly sample the islands, we get a sense of how strongly aligned north/east and south/west designations are.\nCode\npts &lt;- nz |&gt; \n  st_sample(1000) |&gt; \n  st_coordinates() |&gt;\n  as.data.frame()\n\nggplot(nz) + \n  geom_sf(lwd = 0) +\n  geom_point(data = pts, aes(x = X, y = Y), size = 0.25) +\n  geom_smooth(data = pts, aes(x = X, y = Y), method = \"lm\") +\n  coord_sf(datum = 2193) +\n  theme_minimal()\nWe can even put a number on this observation.\ncor(pts)\n\n          X         Y\nX 1.0000000 0.8515348\nY 0.8515348 1.0000000\nWith pretty high accuracy, if a place is in the ‘north’, it is in the ‘east’! There are probably better ways to ‘put a number’ on this observation (contingency tables and whatnot), but a correlation coefficient will do for now.\nTo be serious—for just a paragraph—this correlation between latitudes and longitudes is something to be aware of when doing spatial modelling. In the context, for example, of species distribution models, it’s an open question if latitude or ‘island’ is a better variate to include for this reason, and it’s certainly questionable due to collinearity to include both latitude and longitude in models.3"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#slicing-and-dicing-the-islands",
    "href": "posts/2025-06-01-nsew-island/index.html#slicing-and-dicing-the-islands",
    "title": "North-south or east-west islands?",
    "section": "Slicing and dicing the islands",
    "text": "Slicing and dicing the islands\nTo put areas on this perspective, we can slice the islands at their mutual centroid. First get a bounding box, and a centroid.\n\nbb &lt;- nz |&gt; st_bbox()\ncentroid &lt;- nz |&gt; \n  st_union() |&gt; \n  st_centroid() |&gt; \n  st_coordinates() |&gt;\n  c()\n\nNext make a function to move one bound of a bounding box and turn it into a simple features data set.\n\n1bbox_split &lt;- function(bb, ctr, half = \"N\") {\n  key   &lt;- c(N = \"ymin\", S = \"ymax\", \n             E = \"xmin\", W = \"xmax\")\n  coord &lt;- c(N = 2, S = 2, E = 1, W = 1)\n  bb |&gt; replace(key[half], ctr[coord[half]]) |&gt;\n    st_as_sfc() |&gt;\n    as.data.frame() |&gt;\n    st_sf()\n}\n\n\n1\n\nYes… this function is pretty ugly, but it gets the job done.\n\n\n\n\nAnd now we can split the country in each of these two directions.\n\n1ns_split &lt;- bind_rows(\n  bbox_split(bb, centroid, \"N\") |&gt; mutate(north_c = TRUE),\n  bbox_split(bb, centroid, \"S\") |&gt; mutate(north_c = FALSE))\n\new_split &lt;- bind_rows(\n  bbox_split(bb, centroid, \"E\") |&gt; mutate(east_c = TRUE),\n  bbox_split(bb, centroid, \"W\") |&gt; mutate(east_c = FALSE))\n\nnz_split &lt;- nz |&gt; \n  st_intersection(ns_split) |&gt;\n  st_intersection(ew_split) |&gt;\n2  mutate(ns_correct = north_c == north,\n         ew_correct = east_c == north) |&gt;\n3  group_by(ns_correct, ew_correct) |&gt;\n  summarise() |&gt;\n  mutate(area = st_area(geom) |&gt; set_units(\"km^2\"))\n\n\n1\n\nWe add boolean variables indicating the centroid-based classification of each half.\n\n2\n\nIf the centroid-based and toponym based classification are the same we consider the toponym ‘correct’.\n\n3\n\nDissolving the areas together with group_by |&gt; summarise makes for nicer maps.\n\n\n\n\nAnd we get the calculated areas below.\n\nnz_split |&gt; st_drop_geometry()\n\n# A tibble: 4 × 3\n# Groups:   ns_correct [2]\n  ns_correct ew_correct    area\n* &lt;lgl&gt;      &lt;lgl&gt;       [km^2]\n1 FALSE      FALSE        7169.\n2 FALSE      TRUE         7334.\n3 TRUE       FALSE       11494.\n4 TRUE       TRUE       238720.\n\n\nTo my (slight) disappointment north-south is less wrong than east-west would be, although not by much. Oh well, so much for that idea.\nHere are a couple of maps in case you’re missing them in that blizzard of code. Notably the very tip of Te Ika-a-Māui winds up in a notional ‘west’ island, and to the distress of many, a chunk of Canterbury winds up lumped with the ‘east’ island.4\n\n\nCode\nmap1 &lt;- ggplot(nz_split) +\n  geom_sf(aes(fill = ns_correct), lwd = 0) +\n  scale_fill_manual(breaks = as.logical(0:1),\n                    values = c(\"red\", \"lightgrey\")) +\n  geom_hline(aes(yintercept = centroid[2]), linetype = \"dotted\") +\n  guides(fill = \"none\") +\n  ggtitle('North-South wrong') +\n  theme_void() +\n1  theme(plot.title = element_text(hjust = 0.5))\n\nmap2 &lt;- ggplot(nz_split) +\n  geom_sf(aes(fill = ew_correct), lwd = 0) +\n  scale_fill_manual(breaks = as.logical(0:1),\n                    values = c(\"red\", \"lightgrey\")) +\n  geom_vline(aes(xintercept = centroid[1]), linetype = \"dotted\") +\n  guides(fill = \"none\") +\n  ggtitle('East-West wrong') +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n2map1 + map2\n\n\n\n1\n\nCentre-aligned titles seem better here.\n\n2\n\nUsing the excellent patchwork package here rather than facetted plots."
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#a-better-binary",
    "href": "posts/2025-06-01-nsew-island/index.html#a-better-binary",
    "title": "North-south or east-west islands?",
    "section": "A ‘better’ binary",
    "text": "A ‘better’ binary\nAt this point, I contemplated finding a decision boundary for points sampled from the islands, but I haven’t (yet) lost it completely.5\nA more rough and ready approach involved jiggling geom_abline around a bit until it threaded through the Cook Strait / Te Moana-o-Raukawa.\n\n\nCode\nhulls &lt;- nz |&gt;\n  group_by(north) |&gt;\n  summarise() |&gt;\n  st_convex_hull()\n\ng1 &lt;- ggplot(nz) +\n  geom_sf(lwd = 0) +\n  geom_sf(data = hulls, fill = NA, colour = \"red\") +\n  geom_abline(aes(intercept = 9.6125e6, slope = -2.414214), \n              linetype = \"dashed\") +\n  annotate(\"polygon\", x = c(1.65e6, 1.65e6, 1.75e6, 1.75e6),\n                      y = c(5.4e6,  5.5e6,  5.5e6,  5.4e6), \n                      fill = \"#00000030\", colour = \"black\") +\n  theme_void()\n\ng2 &lt;- ggplot(nz) +\n  geom_sf() +\n  geom_sf(data = hulls, fill = NA, colour = \"red\") +\n  coord_sf(xlim = c(1.65e6, 1.75e6), ylim = c(5.4e6, 5.5e6), \n           expand = FALSE, datum = 2193) +\n  geom_abline(aes(intercept = 9.6125e6, slope = -2.414214), \n              linetype = \"dashed\") +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA))\n\ng1 + g2\n\n\n\n\n\n\n\n\n\nAs it happens, that line is on a bearing close to north by northwest.6\nSo, in conclusion… East Northeast and West Southwest Islands, anyone?\nWell no: Te Ika-a-Māui and Te Waipounamu will do just fine, thanks!"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#addendum",
    "href": "posts/2025-06-01-nsew-island/index.html#addendum",
    "title": "North-south or east-west islands?",
    "section": "Addendum",
    "text": "Addendum\nWere the islands ever to split (politically,7 not seismically, where the splits run in different directions) the equidistance principle would require a line be drawn more like the one I’ve worked out below using Voronoi polygons generated from points along the coastlines of the islands.\n\n1bb &lt;- (nz |&gt; st_bbox() + 5e4 * c(-1, -1, 1, 1)) |&gt;\n  st_as_sfc()\n\nvoronoi_islands &lt;- nz |&gt;\n2  st_cast(\"POINT\") |&gt;\n  st_union() |&gt; \n  st_voronoi() |&gt; \n3  st_cast() |&gt;\n  st_as_sf() |&gt; \n  st_intersection(bb) |&gt;\n  st_join(nz) |&gt;\n4  group_by(north) |&gt;\n  summarise()\n\n\n1\n\nThe raw Voronoi polygons produced by st_voronoi extend well beyond the area of interest, so make a bounding box to clip them. The strange shenanigans with adding to the bounding box is to get properly squared off corners on extended bounding box (st_buffer’s settings don’t seem to allow for this).\n\n2\n\nIt’s necessary to merge points into a single multipoint for the Voronoi function to work properly.\n\n3\n\nIt’s further necessary to go through a number of steps to massage the polygons into a useable simple features dataset.\n\n4\n\nFinally dissolve them into single polygons.\n\n\n\n\nAnd here’s a map:\n\n\nCode\ng1 &lt;- ggplot(voronoi_islands) + \n  geom_sf(aes(fill = north), lwd = 0) + \n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  geom_sf(data = nz, lwd = 0) +\n  guides(fill = \"none\") +\n  theme_void()\n\ng2 &lt;- ggplot(voronoi_islands) + \n  geom_sf(aes(fill = north), lwd = 0) + \n  scale_fill_brewer(palette = \"Set1\", direction = -1) +\n  geom_sf(data = nz, lwd = 0) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(1.65e6, 1.75e6), ylim = c(5.4e6, 5.5e6), \n           expand = FALSE, datum = 2193) +\n  theme_void()\n\ng1 + g2"
  },
  {
    "objectID": "posts/2025-06-01-nsew-island/index.html#footnotes",
    "href": "posts/2025-06-01-nsew-island/index.html#footnotes",
    "title": "North-south or east-west islands?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t mention the Chathams. Also, I’m lumping Stewart Island / Rakiura with Te Waipounamu.↩︎\nDon’t mention the football.↩︎\nFor what it’s worth, that observation is what I have to ‘thank’ for all this.↩︎\nIt may be worth noting here that a relatively common pub or Stuff quiz question concerns identifying which of a number of cities in Aotearoa New Zealand is the most easterly/westerly.↩︎\nPerhaps another time… or an exercise for an enthusiastic reader.↩︎\nGreat movie.↩︎\nNot entirely impossible to imagine, see this page.↩︎"
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "",
    "text": "I’ll never not want to see a map of the Dulux colours of New Zealand and am frankly a bit confused why they’ve never put one on their website.1 I especially enjoy a categorical choropleth map where the categories are the actual colours in the map, but that’s probably just me.\nIn the face of their gross dereliction of cartographic duty, I’m here to save the day. I already did this (using R, see here) but this time thought I’d give it a go in Python."
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html#downloading-the-colours",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html#downloading-the-colours",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "Downloading the colours",
    "text": "Downloading the colours\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport json\nimport pandas as pd\n\n\nThe Python modules requests, BeautifulSoup, and json make grabbing the place names and RGB information about the colours relatively straightforward. The colour collections are on nine different pages, so we make up a list of the URLs we need to visit, and set up a couple of empty lists to put the colour names and hex codes in.\n\n\nCode\nbase_url = \"https://www.dulux.co.nz/colour\"\ncollections = [\n  \"whites-and-neutrals\", \"greys\", \n  \"browns\", \"purples-and-pinks\", \n  \"blues\", \"greens\", \"yellows\", \n  \"oranges\", \"reds\",]\nurls = [f\"{base_url}/{collection}/\" for collection in collections]\nnames, hexes = [], []\n\n\nSome sleuthing on one of the colour collection pages led to a HTML div with id __NEXT_DATA__ which is Javascript wrapped JSON containing all the information needed. Admittedly the information is deeply buried in what seems like an unnecessarily complicated nested data structure.2 The complexity I think relates to the JSON doing double—even triple—duty structuring the web pages, providing ordering and stocking information, and also information about the actual colours themselves.\nIn any case, that complexity accounts for having to reach four levels down into the JSON to get to the list of colours in each collection, and then another two levels further into each colour definition to get the title and hex elements to add to our names and hexes lists. These can then be used to make up a data table.\nOf note in the code is using time.sleep(1) to avoid overloading the server by introducing a 1 second delay between requesting the pages.\n\n\nCode\nfor url in urls:\n  page = BeautifulSoup(requests.get(url).text, \"html.parser\")\n  data = json.loads(page.find(id = \"__NEXT_DATA__\").text)\n  cols = data[\"props\"][\"pageProps\"][\"colourCollection\"][\"colourEntries\"]\n  names.extend([col[\"fields\"][\"title\"] for col in cols])\n  hexes.extend([col[\"fields\"][\"hex\"] for col in cols])\n  time.sleep(1)\n\n\nThen we can make a data table of the colour names and their RGB hex definitions.\n\n\nCode\ncolours = pd.DataFrame(dict(paintname = names, hex = hexes))\n\n\nHere’s what all that gets us.\n\ncolours\n\n                paintname      hex\n0                 Ōpononi  #d4cdc0\n1          Tōrere Quarter  #e2ddd3\n2               Mason Bay  #d5ccbd\n3            Glinks Gully  #d6cec1\n4     Glinks Gully Double  #c9bdac\n...                   ...      ...\n1108            Red Jacks  #95352e\n1109       Oxford Terrace  #af3f42\n1110              Kelburn  #a85c60\n1111      Gibbston Valley  #68393d\n1112        Cashel Street  #e6a7ae\n\n[1113 rows x 2 columns]\n\n\nSome cleanup is required. Specifically, some paint names include a ‘variant’ suffix, ‘Half’, ‘Double’, or `Quarter’, which we need to split out into their own column in the data and correct the place names accordingly.\nThere are a number of ways this might be done, but consistent with using a pandas.Series.apply, the best and clearest approach seems to be writing a helper functions as below, then applying it and expanding its output to a list so that it can be assigned to two new columns in the data table.\n\n\nCode\nmodifiers = [\"Half\", \"Quarter\", \"Double\"]\n\ndef handle_suffixes(s):\n  words = s.split(\" \")\n  if words[-1] in modifiers:\n    return \" \".join(words[:-1]), words[-1]\n  else:\n    return s, \"\"\n\ncolours[[\"place\", \"modifier\"]] = pd.DataFrame(\n  colours.paintname.apply(handle_suffixes).to_list())\ncolours\n\n\n                paintname      hex            place modifier\n0                 Ōpononi  #d4cdc0          Ōpononi         \n1          Tōrere Quarter  #e2ddd3           Tōrere  Quarter\n2               Mason Bay  #d5ccbd        Mason Bay         \n3            Glinks Gully  #d6cec1     Glinks Gully         \n4     Glinks Gully Double  #c9bdac     Glinks Gully   Double\n...                   ...      ...              ...      ...\n1108            Red Jacks  #95352e        Red Jacks         \n1109       Oxford Terrace  #af3f42   Oxford Terrace         \n1110              Kelburn  #a85c60          Kelburn         \n1111      Gibbston Valley  #68393d  Gibbston Valley         \n1112        Cashel Street  #e6a7ae    Cashel Street         \n\n[1113 rows x 4 columns]"
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html#geocoding-the-colours",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html#geocoding-the-colours",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "Geocoding the colours",
    "text": "Geocoding the colours\nNext up is geocoding. For this I used Nominatim. It’s free and unencumbered by usage restrictions. It’s far from perfect. In this particular application it makes some unfortunate choices. ‘Cuba Street’, famously in Wellington, also exists not so far away in Petone. ‘Rangitoto’ famously Auckland’s dormant/extinct, friendly neighbourhood volcano is also the name of a large high school on the North Shore. More perplexingly ‘Te Kopua Beach’, which I believe the Dulux people probably intended to refer to a camp ground near Raglan, winds up geocoded as Beach Pizza in Glendene, West Auckland for no very obvious reason.\nI did consider using a better geocoder. Google’s is probably the pick of the bunch for quality results, but… well, this is very much a demonstration project, and their terms of use are very restrictive,3 including not storing results or using them to make maps outside the Google maps ecosystem. Life’s too short, and this project isn’t serious enough for that kind of stupidity.\nAnyway, the code below makes up a dictionary locations where each place name is associated with a list of longitude-latitude pairs as returned by the geocoder.\n\n\nCode\nfrom geopy.geocoders import Nominatim\nfrom collections import defaultdict\n\ngeolocator = Nominatim(user_agent = \"Firefox\")\nlocations = defaultdict(list)\nfor place in list(pd.Series.unique(colours.place)):\n  geocode = geolocator.geocode(place, country_codes = [\"NZ\"], \n                               exactly_one = False, timeout = None)\n  if geocode is not None:\n    locations[place].extend([(loc.longitude, loc.latitude) for loc in geocode])\n  time.sleep(1)\n\n\nRetaining a list of locations for each place allows us to associate different locations with the various modified paint colours where these exist. When we assign coordinates to each row in the data table we pop the first one off this list, and if the list is now empty remove it from the available locations, so that any later appearances of that place in the table are skipped.\n\n\nCode\nfrom collections import namedtuple\nColour = namedtuple(\"Colour\", tuple(colours.columns))\nGeocode = namedtuple(\"Geocode\", Colour._fields + (\"longitude\", \"latitude\"))\n\ngeocodes = []\n1for col in colours.itertuples(index = False, name = \"Colour\"):\n  if col.place in locations:\n2    geocodes.append(Geocode(*(col + locations[col.place].pop(0))))\n    if len(locations[col.place]) == 0:\n      del locations[col.place]\n\n\n\n1\n\nIterate over rows in the table as Colour named tuples.\n\n2\n\nAdd coordinates from the longitude-latitude tuple in the locations dictionary entry to the colour record to form the geocoded record.\n\n\n\n\nDefining named tuples for colour and geocoded records allows convenient iteration over each row in the colours table, and also adding coordinates from the geocoding to the colour records. Compiling a list of the geocoded records then allows convenient conversion to a data table at the end of the process.\n\n\nCode\ndf = pd.DataFrame(geocodes)\n\n\nAnd now we have something we can make maps with:\n\n\nCode\ndf\n\n\n            paintname      hex            place modifier   longitude   latitude\n0             Ōpononi  #d4cdc0          Ōpononi           173.391729 -35.511666\n1      Tōrere Quarter  #e2ddd3           Tōrere  Quarter  177.491325 -37.949759\n2           Mason Bay  #d5ccbd        Mason Bay           167.709020 -46.913846\n3        Glinks Gully  #d6cec1     Glinks Gully           173.857638 -36.081105\n4    St Clair Quarter  #edefee         St Clair  Quarter  170.489146 -45.909391\n..                ...      ...              ...      ...         ...        ...\n987         Red Jacks  #95352e        Red Jacks           171.437996 -42.404411\n988    Oxford Terrace  #af3f42   Oxford Terrace           174.807967 -36.823977\n989           Kelburn  #a85c60          Kelburn           174.762393 -41.289205\n990   Gibbston Valley  #68393d  Gibbston Valley           168.915154 -45.012283\n991     Cashel Street  #e6a7ae    Cashel Street           172.653702 -43.533105\n\n[992 rows x 6 columns]"
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html#making-a-map",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html#making-a-map",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "Making a map",
    "text": "Making a map\nThe map making part is pretty simple. Voronoi polygons are the obvious way to go.\n\n\nCode\nimport geopandas as gpd\n\nnz = gpd.read_file(\"data/nz.gpkg\")\npts = gpd.GeoDataFrame(\n  data = df, \n  geometry = gpd.GeoSeries.from_xy(x = df.longitude,\n                                   y = df.latitude, \n                                   crs = 4326)) \\\n  .query(\"longitude &gt; 0 & latitude &gt; -47.5\") \\\n  .to_crs(2193)\n\ndulux_map = gpd.GeoDataFrame(\n    geometry = gpd.GeoSeries(\n      [pts.geometry.union_all()]).voronoi_polygons(), crs = 2193) \\\n  .sjoin(pts) \\\n  .clip(nz)\n\n\nThe only real wrinkle here is that voronoi_polygons() must necessarily be applied to a collection of points, so we must union_all() the points before applying it. Also, this method doesn’t guarantee returning the polygons in the same order as the points supplied, so we must spatially join the resulting data set back to its originating points dataset. Finally, we clip with a New Zealand data layer to get a sensible output map.\n\n\nCode\ndulux_map.explore(\n  color = dulux_map.hex, \n  tiles = \"CartoDB Positron\", tooltip = \"place\", \n  popup = [\"place\", \"hex\", \"paintname\"],\n  style_kwds = dict(weight = 0, fillOpacity = 1))\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n \nThere might be a hint in the map in the form of a band of off-white colours down the spine of Te Waipounamu of the Southern Alps. But there’s not a lot else to suggest anything systematic about the colours, apart from the slightly obvious fact that the lake places are nearly all blues of one kind or another. We can see these two features in the subsetted maps below.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nlake_recs = dulux_map.place.str.startswith(\"Lake\")\nlakes = gpd.GeoDataFrame(\n  data = dulux_map[lake_recs][[\"hex\"]],\n  geometry = gpd.GeoSeries(dulux_map.geometry[lake_recs]))\n\nmt_recs = dulux_map.place.str.startswith(\"Mt\")\nmts = gpd.GeoDataFrame(\n  data = dulux_map[mt_recs][[\"hex\"]],\n  geometry = gpd.GeoSeries(dulux_map.geometry[mt_recs]))\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2,\n                         figsize = (8, 6),\n                         layout = \"constrained\")\n\nfor ax, subset, title in zip(axes, [lakes, mts], [\"Lakes\", \"Mountains\"]):\n  nz.plot(ax = ax, fc = \"none\", ec = \"grey\", lw = 0.25)\n  subset.plot(ax = ax, fc = subset.hex, lw = 0.1, ec = \"k\" )\n  ax.set_title(title)\n  ax.set_axis_off()"
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html#reflection",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html#reflection",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "Reflection",
    "text": "Reflection\nThis sort of things makes one contemplate ones life choices. Or at any rate reflect on the never-ending comparison between R and Python. It wasn’t especially difficult to make these maps in either platform.\nPython’s requests, BeautifulSoup, json combo is pretty amazing for pulling the data. Python’s general purpose lists, dictionaries, named tuples are much easier to deal with than always trying to make things work as dataframe pipelines in R. I know you can write functions in R too, it’s just that somehow the pipeline mindset takes over and you find yourself puzzling out how to do things using tables when a dictionary makes it trivial.\nBy the same token, I don’t know if I will ever get completely comfortable with pandas rather oblique subsetting and data transformation methods compared to tidyverse R’s dplyr pipelines, but saying that, splitting strings (the paint names) to remove the paint modifier suffixes was easier in Python that using tidyverse’s separate_wider.\nI enjoyed using namedtuples here in the geocoding step to make the code a little clearer, although it may be overkill where a small table like this is involved. It’s never a bad thing to extend your use of language features like this.\nAs always, when it comes to visualization I miss ggplot, but geopandas explore method makes it trivial to create a web map, which was the end goal here.\nNow, I should go and paint that room."
  },
  {
    "objectID": "posts/2025-08-03-dulux-colours-in-python/index.html#footnotes",
    "href": "posts/2025-08-03-dulux-colours-in-python/index.html#footnotes",
    "title": "Dulux colours of Aotearoa New Zealand mapped",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPerhaps it’s not unrelated to the cost of running the Google Maps API…↩︎\nI don’t run a paint company, so I’m probably wrong about this.↩︎\nMapscaping have a very useful overview of the options on this page↩︎"
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html",
    "href": "posts/2025-08-20-street-level-isochrones/index.html",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "",
    "text": "In this post, I revisit my very earliest academic contribution,1 which demonstrated how to generate isochrone maps in a GIS based on public transport timetable data. An isochrone is a line delineating a set of points reachable in some set time from some starting location, given the transport options available. This is a notion that has by now become a lot more familiar than it was back then, perhaps most especially via the (ahem) magnificent mapnificent.net website.\nIn my masters thesis on which that paper was based, I did a lot of this by hand. I started by grabbing a huge number of paper bus timetables from the central station in Glasgow.2 In the absence of reliable data on where bus stops were, and with insufficient time to travel every bus route and geocode them, I used the timetables to generate ‘fake’ bus stops by running shortest path analysis between the end points of each route via some known intervening locations, determined from the timetables and a handy-dandy copy of the AZ Street Atlas of Glasgow Hamilton Motherwell Paisley.3 A further simplification was to focus on off-peak times only, and to use only bus route frequencies not timetabled arrival and departure times.\nAnyway, I used all this to generate some truly horrible maps. The colour scheme I used boggles my mind. Let’s just say I am retrospectively grateful that the reference electronic versions of the maps in the literature are in greyscale.4\nAll of which brings me back to this post, where using the recently released city2graph5 module and the by now venerable osmnx6 I make some street based, combined bus and walking isochrones for my local Wellington bus route, the number 7."
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#back-in-the-day",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#back-in-the-day",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "",
    "text": "In this post, I revisit my very earliest academic contribution,1 which demonstrated how to generate isochrone maps in a GIS based on public transport timetable data. An isochrone is a line delineating a set of points reachable in some set time from some starting location, given the transport options available. This is a notion that has by now become a lot more familiar than it was back then, perhaps most especially via the (ahem) magnificent mapnificent.net website.\nIn my masters thesis on which that paper was based, I did a lot of this by hand. I started by grabbing a huge number of paper bus timetables from the central station in Glasgow.2 In the absence of reliable data on where bus stops were, and with insufficient time to travel every bus route and geocode them, I used the timetables to generate ‘fake’ bus stops by running shortest path analysis between the end points of each route via some known intervening locations, determined from the timetables and a handy-dandy copy of the AZ Street Atlas of Glasgow Hamilton Motherwell Paisley.3 A further simplification was to focus on off-peak times only, and to use only bus route frequencies not timetabled arrival and departure times.\nAnyway, I used all this to generate some truly horrible maps. The colour scheme I used boggles my mind. Let’s just say I am retrospectively grateful that the reference electronic versions of the maps in the literature are in greyscale.4\nAll of which brings me back to this post, where using the recently released city2graph5 module and the by now venerable osmnx6 I make some street based, combined bus and walking isochrones for my local Wellington bus route, the number 7."
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#preliminaries",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#preliminaries",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst up, we need a bunch of python modules. requests and io are needed to pull the Wellington GTFS feed from its online home; pandas and geopandas for processing the tabular data; city2graph and osmnx for the core processing; and folium for web map outputs.\n\n\nCode\nimport requests\nimport io\nimport pandas as pd\nimport geopandas as gpd\nimport city2graph as c2g\nimport osmnx as ox\nimport folium"
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#unpacking-gtfs",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#unpacking-gtfs",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "Unpacking GTFS",
    "text": "Unpacking GTFS\nYou can do many things with city2graph. Here, the main task is to process General Transit Feed Specification (GTFS) data into a format suitable for our analysis. First, here’s a URL for the Wellington GTFS feed.\n\n\nCode\ngtfs_url = \"https://static.opendata.metlink.org.nz/v1/gtfs/full.zip\"\n\n\nAs I noted in my masters thesis public transport systems are complicated. Very complicated. At the time the Glasgow region’s system apparently had as many as 12,000 transit routes, at least when viewed from the perspective of the people running the system, who consider every minor variation of a route to be different, where variation might include running on different days, at different times, or following a slightly different route. As a transport user we tend to think in terms of the number 7 bus, but for the transport system operator it’s not as simple as that: the number 7 might refer to several hundred possible trips, in several subtly different varieties.\nGTFS reflects this complexity with the full specification requiring minimally 7 files setting out agencies, routes, trips, stops, stop-times, a calendar, and calendar dates, and with an additional optional 15 files that provide further details of fares, fare transfer rules, and a great deal more besides.\n\nParsing GTFS with city2graph\ncity2graph happily parses all this complexity into a dictionary of tables which provide us with all the information we need for the present analysis at least. It’s so easy, that the trickiest part of the line of code below was working out how to get it to read from an online source.\n\n\nCode\ngtfs = c2g.load_gtfs(io.BytesIO(requests.get(gtfs_url).content))\n\n\nI’ll focus on the high frequency route 7 that passes through Brooklyn, the suburb where I live, into central Wellington (direction 1).\n\n\nCode\nroute = \"7\"\ndirection = \"1\"\n\n\nI use this information to get a route_id.\n\n\nCode\nroute_id = (\n  gtfs[\"routes\"]\n1  .query(\"route_short_name == @route\")\n  .route_id\n  .iloc[0]\n)\nroute_id\n\n\n\n1\n\nWe could also use an f-string here, but the @ operator let’s you include variables in a pandas table query.\n\n\n\n\n'70'\n\n\nAnd then use this to retrieve trip IDs that relate to this route.\n\n\nCode\ntrip_ids = (\n  gtfs[\"trips\"]\n  .query(\"route_id == @route_id & direction_id == @direction\")\n  .sort_values([\"service_id\", \"trip_id\"])\n  .loc[:, [\"trip_id\"]]\n)\ntrip_ids.shape[0], trip_ids.head()\n\n\n(152,\n                                       trip_id\n 7019  7__1__700__TZM__102__2__102__2_20250817\n 5703  7__1__100__TZM__102__7__102__7_20250817\n 4691  7__1__116__TZM__102__7__102__7_20250817\n 47    7__1__400__TZM__103__3__103__3_20250817\n 7397  7__1__450__TZM__103__3__103__3_20250817)\n\n\nThere are actually 152 of these! For lack of anything better, I’m just going to focus on the first, and retrieve the associated stop times. These are the timetabled arrival times at each stop along the route for a particular trip.\n\n\nCode\ntrip_id = trip_ids.trip_id.iloc[0]\nstop_times = (\n  gtfs[\"stop_times\"]\n  .query(\"trip_id == @trip_id\")\n  .loc[:, [\"trip_id\", \"stop_id\", \"arrival_time\"]]\n  .sort_values(\"trip_id\")\n)\nstop_times.head(),\n\n\n(                                        trip_id stop_id arrival_time\n 255719  7__1__700__TZM__102__2__102__2_20250817    7730     07:35:00\n 255741  7__1__700__TZM__102__2__102__2_20250817    5012     07:54:24\n 255740  7__1__700__TZM__102__2__102__2_20250817    5010     07:53:51\n 255739  7__1__700__TZM__102__2__102__2_20250817    5008     07:52:27\n 255738  7__1__700__TZM__102__2__102__2_20250817    7708     07:50:55,)\n\n\nThis table gives us a stop_id and associated arrival time on this particular trip on the number 7, and we can use that information to build an associated set of isochrones."
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#building-isochrones",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#building-isochrones",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "Building isochrones",
    "text": "Building isochrones\nBelow, I use pandas’ facility with date and time formats, to convert the arrival times into a series of ‘time remaining’ values. For some allowed period of time, which I here choose to be 30 minutes, I set the end time of the isochrone to be 08:05 (since the bus sets off at 07:35). The arrival time at each stop on the route admits some further travel time before 08:05, which is calculated as a ‘time remaining’.\n\n\nCode\n1end_time = pd.to_datetime(\"08:05:00\", format = \"%X\")\ntimes_remaining = (\n  stop_times\n  .assign(\n    time_remaining = (\n      end_time - pd.to_datetime(stop_times.arrival_time, format = \"%X\"))\n      .dt.seconds)\n  .loc[:, [\"stop_id\", \"time_remaining\"]]\n)\ntimes_remaining.head(),\n\n\n\n1\n\nThis converts the time into a number of seconds which is used in the calculation of time remaining that follows.\n\n\n\n\n(       stop_id  time_remaining\n 255719    7730            1800\n 255741    5012             636\n 255740    5010             669\n 255739    5008             753\n 255738    7708             845,)\n\n\nThis code is where, if we wanted to make a set of isochrones with different total available times, we could do so by iterating over a series of end_time values, calculating the time remaining, and if applicable dropping stops where there was no time or only negative time remaining (i.e. it takes longer to get to them than the time budget allows).\n\nCrow’s flight ‘bubble’ isochrones\nGiven the time remaining at each stop, we can use a simple buffering method to produce a circular area centred on each stop which is the potentially reachable area around that stop, assuming a walking speed of 1m/s and an ability to pass through walls, trees, and anything else in between (i.e., this ignores things like footpaths, roads, and so on).\n\nCode\nroute_stops = (\n  gtfs[\"stops\"]\n  .merge(times_remaining)\n  .sort_values(\"time_remaining\", ascending = False)\n)\nbubbles = [s.buffer(d)\n           for s, d in zip(route_stops.geometry.to_crs(2193),\n                           route_stops.time_remaining)]\n\nisochrone_bubbles = gpd.GeoDataFrame(\n  geometry = gpd.GeoSeries(bubbles), crs = 2193)\n\nisochrone_bubbles.plot(alpha = 0.1).set_axis_off()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Isochrone bubbles for the number 7 bus\n\n\n\n\n\nIsochrones like streets matter\nWe can think of the ‘bubble’ isochrones as a best-case scenario. But in the real world we have to pound the pavement and walk along approved paths and this will generally mean that progress is slower than the straight line distances of simple buffers imply. This is where osmnx comes in.\nFirst, it’s useful to make a bounding box from the bubble isochrone, since it defines the farthest possible progress at every location, and use this to extract a street graph using osmnx’s graph.graph_from_bbox() method.\n\n\nCode\nbb = [float(x) \n      for x in isochrone_bubbles.to_crs(4326).total_bounds]\nG = ox.graph.graph_from_bbox(bb, network_type = \"walk\", \n                             simplify = False)\n\n\nNow, based on this graph, we can determine for each stop, the nearest node on the street network.\n\n\nCode\nnearest_nodes = ox.distance.nearest_nodes(G,\n  [p.x for p in route_stops.geometry],\n  [p.y for p in route_stops.geometry])\n\n\nAnd based on these nodes, we can pull subgraphs of the street graph that are within a distance along the street network limited by the time remaining at each stop, again assuming progress at 1m/s.7\n\nCode\nisochrone_streets = (\n  gpd.GeoDataFrame(\n    geometry = gpd.GeoSeries(pd.concat([\n      ox.convert.graph_to_gdfs(\n        ox.truncate.truncate_graph_dist(G, node, dist), \n        nodes = False).geometry\n      for node, dist in zip(\n        nearest_nodes,\n        route_stops.time_remaining)]).union_all(),\n    crs = 4326)\n  ).to_crs(2193)\n)\nisochrone_streets.plot().set_axis_off()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The streets reachable within 30 minutes\n\n\n\nWe can also derive an approximate polygon outlining the street network using the relatively recent concave_hull addition to geopandas.\n\n\nCode\nisochrone_approx = (\n  isochrone_streets\n  .concave_hull(ratio = 0.025)\n)\n\n\n\n\nPutting it all together\nNow we’re ready to assemble all these layers into a composite map of the number 7 bus route.\n\nCode\nm = folium.Map(\n  tiles = None,\n  zoom_start = 13, max_bounds = True,\n  location = [(bb[0] + bb[2]) / 2, (bb[1] + bb[3]) / 2],\n  min_lon = bb[0], min_lat = bb[1], \n  max_lon = bb[2], max_lat = bb[3]\n)\n\nfolium.TileLayer(\"CartoDB Positron\", overlay = True).add_to(m)\n\nisochrone_bubbles.explore(\n  m = m, name = \"Walk time circles\", tooltip = False,\n  style_kwds = dict(fill = False, color = \"black\", weight = 0.35))\n\nisochrone_approx.explore(\n  m = m, name = \"Approximate isochrone\", tooltip = False,\n  style_kwds = dict(color = \"red\", fill = False, weight = 3))\n\nisochrone_streets.explore(\n  m = m, name = \"Streets isochrone\", tooltip = False,\n  style_kwds = dict(color = \"orange\", weight = 1))\n\nroute_stops.explore(\n  m = m, name = \"Stops\", tooltip = \"stop_name\",\n  style_kwds = dict(color = \"red\", fill = False, radius = 6))\n\nfolium.LayerControl().add_to(m)\nm\n\n\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFigure 3: A web map composite of the number 7 bus route\n\n\n\nThis map shows, in a number of different ways the places reachable by way of the number 7 bus and walking, within 30 minutes starting from the moment at which you choose to board the 07:35 starting at the Kingston end of the route or not.8\nAt the Kingston end, if you were able to fly, you could get a long way in 30 minutes (as shown by the large circles). In practice, on the ground, the options are more limited because, in common with many places in Wellington, the roads just come to an end.\nIf you alight from the bus halfway into town you have time to walk a reasonable distance from your stop and this is reflected in the ‘widening’ out of the street based isochrone.\nIf you don’t get off the bus until near the terminus at Wellington Station, then your options on foot narrow down to only a small area close to the station.\nIn my thesis I attempted to model transfers between buses, and doing that here would get complicated pretty quickly, but not outrageously so. And of course, in my thesis I only modelled the off-peak timetable, and approximately at that. With the detailed data available from GTFS I could have been a lot more ambitious.9"
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#addendum-more-city2graph-gtfs-goodness",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#addendum-more-city2graph-gtfs-goodness",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "Addendum: more city2graph GTFS goodness",
    "text": "Addendum: more city2graph GTFS goodness\nHere’s an example of something you can do with GTFS using city2graph that would be excruciatingly painful to do manually working with individual GTFS files. The travel_summary_graph() method takes a stack of GTFS files and subjects them to further analysis, determining the average time taken across all trips to get between consecutive stops on the network.\n\n\nCode\npts, segments = c2g.travel_summary_graph(gtfs)\npts = pts.reset_index()\nsegments_m = segments.to_crs(2193)\nsegments_m.head(),\n\n\n(                         mean_travel_time  frequency  \\\n from_stop_id to_stop_id                                \n 0001         1800              257.000000         20   \n 0002         1806               70.800000         20   \n              1905               56.000000         20   \n 0003         1817               27.400000         20   \n              1916              127.238095         21   \n \n                                                                   geometry  \n from_stop_id to_stop_id                                                     \n 0001         1800        LINESTRING (1804586.846 5436469.543, 1806080 5...  \n 0002         1806        LINESTRING (1798074.727 5442644.393, 1797469.9...  \n              1905        LINESTRING (1798074.727 5442644.393, 1798720.0...  \n 0003         1817        LINESTRING (1799389.472 5445379.483, 1799729.9...  \n              1916        LINESTRING (1799389.472 5445379.483, 1797980.0...  ,)\n\n\nThis allows me to explore one of my local niggles with the number 7 bus, which is the seemingly stupidly high density of stops in Brooklyn. These involve the bus stopping at less than 60 second intervals three times in the so-called ‘village’, which is really barely more than a hundred metres or so of shops and take aways. I suspect this annoys drivers at least as much as it annoys me.10\nQuerying the analysis results for stops less than 200 metres apart which are also less than 60 seconds apart allows us to map the offenders. Suffice to say, Brooklyn and the No. 7 are not alone in this.\n\nCode\nsegments_m = (\n  segments_m\n  .assign(length  = segments_m.geometry.length.round(0),\n          mean_travel_time = segments_m.mean_travel_time.round(0))\n  .query(\"length &lt; 200 & mean_travel_time &lt; 60\")\n)\n(segments_m\n  .explore(tiles = \"CartoDB Positron\",\n           style_kwds = dict(color = \"red\", weight = 3))\n)\n\n\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFigure 4: Very short duration segments on Wellington’s public transport network"
  },
  {
    "objectID": "posts/2025-08-20-street-level-isochrones/index.html#footnotes",
    "href": "posts/2025-08-20-street-level-isochrones/index.html#footnotes",
    "title": "Street based transit isochrones using city2graph and osmnx",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO’Sullivan D, A Morrison and J Shearer. 2000. Using desktop GIS for the investigation of accessibility by public transport: an isochrone approach. International Journal of Geographical Information Science 14(1) 85-104.↩︎\nThis was a time when such things still existed. I guess they must have assumed I was a public transport timetable nut enthusiast.↩︎\nSurprisingly, you can still get one of these.↩︎\nIf you’re interested in the horrors I reserved colour printing for in the thesis itself, then take a look here.↩︎\nSato Y. 2025. city2graph: Transform geospatial relations into graphs for spatial network analysis and Graph Neural Networks doi:10.5281/zenodo.15858845.↩︎\nBoeing G. 2025. Modeling and Analyzing Urban Networks and Amenities with OSMnx. Geographical Analysis.↩︎\nIf you are familiar with Wellington’s rugged terrain, you’ll know this is a generous assumption. In principle, it wouldn’t be difficult to include street slope, but that’s another question, for another day.↩︎\nThat’s where the metaphorical sliding doors come in, even if buses don’t generally have actual sliding doors.↩︎\nSaying that, GTFS timetable data is theoretical, not actual, and the difference may matter greatly, a topic explored in this paper: Wessel N and S Farber. 2019. On the accuracy of schedule-based GTFS for measuring accessibility. Journal of Transport and Land Use. 12(1) 475-500.↩︎\nSaying which, it’s a collective action problem. Riders could choose collectively, somehow, not to use one or more of the pretty much redundant stops, but we don’t. And I include myself in that number. And we are probably all equally annoyed about it.↩︎"
  },
  {
    "objectID": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "href": "posts/2022-03-09-ok-covid-you-win/ok-covid-you-win.html",
    "title": "OK COVID, you win",
    "section": "",
    "text": "From some time in March 2020 for two years I downloaded the latest reported COVID data for New Zealand, added them to my spreadsheet of the various numbers, and updated a timeline I was keeping in R. The download process got a lot easier when I was introduced to the data downloader at University of Auckland eResearch.\nAs you can see, by the time I stopped things had gone pretty badly off the rails, and even a log scale wasn’t helping much.\nI can honestly say this was when I started to become competent with the ggplot2 package, and for that, as well as the reassurance the daily ritual provided for about a year and a half (we were doing so well…), I am grateful.\n\nAddendum\nI also mapped the progress of the vaccination program for a (much shorter) time. At least these numbers were released weekly and in a much more accessible form. Here’s how the critical ‘second dose’ went:"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "",
    "text": "One of the joys (ahem) of R spatial is moving data around between formats so you can use the best packages for particular jobs. Here’s an example using IDW interpolation in spatstat."
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#libraries",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Libraries",
    "text": "Libraries\nLibraries are the usual suspects plus spatstat (duh) and maptools for some extra conversions. We also need terra for the data prep.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#introduction",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Introduction",
    "text": "Introduction\nAs is often the case there is useful functionality in a package that doesn’t play nice with the core R-spatial packages. spatstat is really great for lots of things, but does not support sf and even needs a bit of persuading to handle sp data. Its implementation of IDW interpolation is nice however, so it’s nice to know how to use it. Whether or not you should ever use IDW is another question altogether, but we can worry about that some other time.\n\nData\nFirst we need a set of values to interpolate. I made a projected version of the R core dataset volcano which is a nice place to start.\n\nmaungawhau &lt;-  rast(\"maungawhau.tif\")\n\n\n\nSome random control points and a study area\nWe can get a dataframe of random points on the surface using terra::spatSample. We’ll make this into a sf object as a starting point because that’s the most likely situation when you want to interpolate data (you will have an sf source).\n\npts &lt;- maungawhau |&gt;\n  spatSample(500, xy = TRUE) |&gt;\n  st_as_sf(coords = c(\"x\", \"y\")) |&gt;\n  st_set_crs(2193) |&gt;\n  st_jitter(5)\n\nWe also need a spatial extent for the interpolation, so let’s just make a convex hull of the points\n\nspatial_extent &lt;- pts |&gt;\n  st_union() |&gt;\n  st_convex_hull() |&gt;\n  st_sf()\n\nAnd just to see where we are at\n\ntm_shape(maungawhau) + \n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\")) +\n  tm_shape(pts) + \n  tm_dots() + \n  tm_shape(spatial_extent) + \n  tm_borders() + \n  tm_layout(legend.frame = FALSE)"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#make-the-data-into-a-spatstat-point-pattern",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Make the data into a spatstat point pattern",
    "text": "Make the data into a spatstat point pattern\nspatstat has its own format for point patterns, including coordinates, marks (the values) and a window or owin (the spatial extent). It’s best to make the window first and then we can make the whole thing all at once. spatstat prefers sp objects, so we go via ‘Spatial’ to get a spatstat::owin object. maptools provides the conversion to an owin.\n\nW &lt;- spatial_extent |&gt;\n  as.owin()\n\nWe also need the control point coordinates\n\nxy &lt;- pts |&gt;\n  st_coordinates() |&gt;\n  as_tibble()\n\nNow we can make a spatstat::ppp point pattern\n\npp &lt;- ppp(x = xy$X, xy$Y, marks = pts$maungawhau, window = W)\nplot(pp)\n\n\n\n\n\n\n\n\nSuccess!\nA previous notebook showed an even quicker way to do this, but where the window will be formed from a bounding box (and where’s the fun in that?)\npts |&gt;\n  as(\"Spatial\") |&gt;\n  as.ppp()"
  },
  {
    "objectID": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "href": "posts/2021-10-22-spatstat-idw/spatstat-idw.html#interpolation",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "section": "Interpolation",
    "text": "Interpolation\nIt’s easy from here. power is the inverse power applied to distances, and eps is the resolution in units of the coordinate system.\n\nresult &lt;- idw(pp, power = 2, eps = 10)\nplot(result)\n\n\n\n\n\n\n\n\nThis can be converted back to a terra raster for comparison with the original surface.\n\n# stack the layers so we can 'facet' plot them\ncomparison_raster &lt;- rast(\n  list(maungawhau = maungawhau, \n       interpolation = rast(result) |&gt; resample(maungawhau)\n  ))\n\ntm_shape(comparison_raster) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\"),\n            col.free = FALSE,\n            col.legend = tm_legend(\n              title = \"Elevation\", \n              position = tm_pos_out(\"right\", \"center\"), \n              frame = FALSE))\n\n\n\n\n\n\n\n\nLike I said, IDW is not necessarily a great interpolation method!"
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html",
    "title": "30 Day Map Challenge 2025",
    "section": "",
    "text": "Have you ever flattened out a piece of crumpled paper and thought to yourself, “hmm, that looks quite like a hillshaded map”? If you don’t believe me, here’s an example.\nSo, that’s the motivation here: to make a crumpled hillshade map of a real place—unlike the image above which is just crumpled paper.1 I tried this the last time I engaged with the 30 Day Map Challenge in 2023. There’s some more background on the idea there. Herewith the steps I went through this time around."
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#drawing-crease-lines",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#drawing-crease-lines",
    "title": "30 Day Map Challenge 2025",
    "section": "Drawing (crease) lines",
    "text": "Drawing (crease) lines\nNext, I imported those layers into Inkscape and drew some red and blue lines. Red lines for ridges, blue ones for courses (or thalwegs as I’ve come to enjoy calling them). From there, I print the lines on a piece of A4 and fold the lines ‘backwards’. What I mean by that is that I pinch the blue valley folds towards me on the printed side of the paper, so they come out as valleys on the other side of the sheet; and I also pinch the red mountain folds (but looking through the sheet of paper to find them) so that they come out as ridges. The reason for pinching the folds will become clear to you if you try this yourself. It’s just much easier to accurately follow an irregular crease line by sliding along it, pinching as you go.\nBecause I’m folding ‘backwards’ like this I have to mirror the fold pattern before printing it.\nAnyway, at first I thought I’d try using just selected edges in the TIN. The image on the left is printed on the back of the sheet, and the image on the right is the resulting map on the other side. Hopefully you can see it reflects some of the landscape structure.\n\n\n\n\n\n\n\nNot terrible, but it’s not a great idea to have opposite creases run right up to one another. There’s a whole mathematics of flat-foldability in origami that tells us what combinations of what kinds of creases meeting at a point can be collapsed flat, but flat isn’t what we are aiming for here. It also turns out that _the paper itself will do some of the work for you if you let it, by leaving gaps between the creases."
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#curved-lines",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#curved-lines",
    "title": "30 Day Map Challenge 2025",
    "section": "Curved lines",
    "text": "Curved lines\nAnyway, next, I allowed myself leeway to follow the hillshade more than the TIN, and also made the crease lines curved. Curved folds on paper do interesting things all on their own.\n\n\n\n\n\n\n\nI thought I’d lean into that in my ‘map’.\nIn practical terms, the process of pinch-folding curved lines creates extra indentations and marks, unless you are impossibly precise at manually folding this kind of thing. and those actually help4 as much as any curved fold side-effects do.\nSo, here’s the curved creasline pattern I wound up with."
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#what-about-using-just-one-set-of-creases",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#what-about-using-just-one-set-of-creases",
    "title": "30 Day Map Challenge 2025",
    "section": "What about using just one set of creases?",
    "text": "What about using just one set of creases?\nAnd here’s what I got folding only the valleys (on the left), and only the ridges (on the right).\n\n\n\n\n\n\n\nI was interested to see if either set of folds alone, but assisted by the paper’s curved-crease induced preferences would do a better job than the other. I was surprised to find, at least for the set of creases I am using, that the ridge lines seem more effective. The valleys in this terrain are more incised than the ridgelines hence my expectation that they would work better. It may be that the ridgelines, as a discontinuous set of more ‘gestural’ lines do a better job of carrying the overall landscape structure.\nIn any case, it’s pretty clear we need both. Below is the result of making both sets of creases next to a GIS hillshade.\n\n\n\n\n\n\n\nClearly, I’d need a much bigger sheet of paper5 to come close to capturing the detail that the GIS hillshade includes, but as a generalised view reflecting the largest structural features of the terrain I’m reasonably happy with this. With the addition of some other elements it can become a bona fide map."
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#and-finally",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#and-finally",
    "title": "30 Day Map Challenge 2025",
    "section": "And finally…",
    "text": "And finally…\nFor a final map, I also printed the Zealandia fenceline on the back of the sheet and traced it through in pencil to give at least a little context. For the purists I’ve added a north arrow, a title, and an (approximate) scalebar.\n\nFor the record I printed the crease pattern on day 8, and did all the drawing and folding on day 9, so I didn’t use a computer to make this map… so, notwithstanding all the digital prep, this is an analogue map.6"
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#next-time",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#next-time",
    "title": "30 Day Map Challenge 2025",
    "section": "Next time",
    "text": "Next time\nIf there’s a next time I may explore the possibilities of using a heavier paper and wetting it. So-called wet-folding allows for the use of heavier paper and fixes the final shape more permanently, a bit like papier-mâché. The pinch-folding I do to make these terrains is already quite ‘sculptural’, and heavier paper that could eventually hold the shape permanently feels like a good next step."
  },
  {
    "objectID": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#footnotes",
    "href": "posts/2025-11-09-origami-terrain-30-day-maps-2025-day-9/index.html#footnotes",
    "title": "30 Day Map Challenge 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn an infinite universe it must be a map of somewhere, right?↩︎\nAnd after abandoning surface networks as just too intricate.↩︎\nIf slightly arcane.↩︎\nAt least I think they do↩︎\nAnd a lot more time.↩︎\nSetting to one side the fact that the only way I can show you it is using a photograph taken with a digital camera and posted on the inter… wait, wut?! Doh!↩︎"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "",
    "text": "Warning: sensitive souls who care about geodetic accuracy should probably stop reading now. Liberal use of affine transformation of geometries throughout. You have been warned.\nBefore I get going, huge appreciation to Randall Munroe for the generous terms of use for the XKCD comics.\nFor a long time I thought about teaching a geography / GIS class using XKCD comics as the primary source material, or at the very least as a jumping off point for every lecture. Recently I actually went through every XKCD from the very beginning to the present to assemble a list of comics that would actually be usable in some semi-serious2 way for this purpose. Among the things I learned is that Randall Munroe has become a lot less fixated on sex than he was when he started, and also somewhat less worried about velociraptors.\nAlso, his interest in map projections only seems to have kicked in about halfway through the XKCD time series. Possibly this is related to an ongoing interest in GPS, which will likely be a topic for a future post. #977 Map projections posted in late 2011, is the first sign of an enduring interest in projections, but things don’t really take off until #1500 almost 4 years later, which is the first of several ‘bad map projections’, although not actually called out as such in its title.\nHerewith, my ranking from worst bad projection to best bad projection of the nine examples I found,3 accompanied in a few cases by attempts at recreating them in R (click on the code-fold links to see how). Eventually, perhaps, ongoing work on arbitrary map projections might enable me to automate reverse-engineering any XKCD projection. For now, a combination of perseverance, guess-work, and sheer bloody mindedness will have to suffice.\nCode\nlibrary(sf)\n1library(tmap)\n2library(usmap)\nlibrary(dplyr)\nlibrary(ggplot2)\n3library(stringr)\n4library(smoothr)\n5library(xkcd)\n\n\n\n1\n\nFor its World dataset.\n\n2\n\nFor its US states dataset from usmap::us_map().\n\n3\n\nFor forming proj strings that include values from data.\n\n4\n\nTo densify some shapes.\n\n5\n\nThe less said about this the better.\nFor what it’s worth, by worst bad projection I mean something like ‘least interesting’ and by best, I mean something like ‘most thought-provoking’.4"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-liquid-resize",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-liquid-resize",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#9 Bad Map Projection: Liquid Resize",
    "text": "#9 Bad Map Projection: Liquid Resize\n\n\n\nAlt-text: This map preserves the shapes of Tissot’s indicatrices pretty well, as long as you draw them in before running the resize.\n\n\nI don’t use Adobe Photoshop so XKCD 1784 is just a bit too inside baseball for me. It feels like there might be a missed opportunity here to have said something about continental drift and Pangaea and supercontinents, which is a topic that has come up in other XKCDs (see # 1449 Red Rover). Something about how Africa and South America fitting together so well is evidence for plate tectonics liquid resize."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-united-stralia",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-united-stralia",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#8 Bad Map Projection: The United Stralia",
    "text": "#8 Bad Map Projection: The United Stralia\n\n\n\nAlt-text: This projection distorts both area and direction, but preserves Melbourne.\n\n\nXKCD 2999 just doesn’t work for me.5 I appreciate the attempt at many to one projection, that is to say more than one place on Earth mapping onto a single location in the map. And it’s certainly a relief to know that Melbourne is preserved.6\nI didn’t even try to replicate this one as it seems self-evidently something you’d have to do in a drawing package. Overall, I think the idea of two places into one is much better realised elsewhere in this list."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-greenland-special",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-the-greenland-special",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#7 Bad Map Projection: The Greenland Special",
    "text": "#7 Bad Map Projection: The Greenland Special\n\n\n\nAlt-text: The projection for those who think the Mercator projection gives people a distorted idea of how big Greenland is, but a very accurate idea of how big it SHOULD be.\n\n\nThe whole Greenland thing in map projections is a bit played out. I realise it continues to blow minds how exaggerated its size is in the Mercator projection.7 Anyway, it’s not too hard to remake XKCD 2489 using R, provided you aren’t too respectful of, you know, actual projections. Apparently Mercator Greenland is too big even for this comedy projection, as I had to downscale it to 85% of its ‘raw’ Mercator size to get a good match to the published map.\n\n\nCode\ngreenland &lt;- World |&gt;\n  filter(name == \"Greenland\") |&gt;\n  st_transform(\"+proj=merc\") |&gt;\n  mutate(geometry = \n1    (geometry + c(2e6, -3.1e6)) * diag(1, 2, 2) * 0.85) |&gt;\n2  st_set_crs(\"+proj=merc\")\n\n3greenland_buffer &lt;- greenland |&gt;\n  st_buffer(1e5)\n\nw &lt;- World |&gt;\n4  st_transform(\"+proj=moll\") |&gt;\n5  st_make_valid() |&gt;\n  st_set_crs(\"+proj=merc\") |&gt;\n6  st_difference(greenland_buffer) |&gt;\n7  bind_rows(greenland)\n\nggplot(w) +\n  geom_sf() +\n  coord_sf(xlim = c(-1.2e7, 1.7e7), expand = FALSE) +\n  theme_void()\n\n\n\n1\n\nExperimentation suggested that some rescaling of Greenland as it is projected ‘raw’ is required to get the outcome in the comic.\n\n2\n\nAfter rescaling and shifting Greenland we have to tell a white lie and tell R the data are still Mercator projected.\n\n3\n\nA buffered Greenland clears a passage between Greenland and Canada.\n\n4\n\nFairly confident the rest of the world is Mollweide projected.\n\n5\n\nIt’s not uncommon for world data sets to have invalid polygons after projection and that’s the case here.\n\n6\n\nThis erases the existing Greenland and removes some of Canada’s offshore islands.\n\n7\n\nAdd Mercator Greenland into the data."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-madagascator",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-madagascator",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#6 Bad Map Projection: Madagascator",
    "text": "#6 Bad Map Projection: Madagascator\n\n\n\nAlt-text: The projection’s north pole is in a small lake on the island of Mahé in the Seychelles, which is off the top of the map and larger than the rest of the Earth’s land area combined.\n\n\nXKCD 2613 went up in my estimation after I figured out how to make it… the alt-text provides enough information to find the central coordinates, which are in La Gogue Lake on Mahé in the Seychelles.\nArmed with the ludicrously precise lat-lon coordinates8 Google provided me for this spot, I figured out how to approximate this map.\n\n\nCode\n1ll &lt;- c(-4.595750619515433, 55.43837198904654)\n\nmadagascator &lt;- World |&gt;\n  st_transform(str_glue(\"+proj=laea +lon_0={ll[2]} +lat_0={ll[1]}\")) |&gt;\n2  st_set_crs(\"+proj=laea +lon_0=150 +lat_0=90\") |&gt;\n3  st_transform(\"+proj=merc\")\n\nggplot(madagascator) +\n  geom_sf(fill = \"lightgrey\", \n          color = \"darkgrey\") +\n  geom_sf_text(aes(label = name), \n               check_overlap = TRUE, \n               size = 3) +\n  theme_void()\n\n\n\n1\n\nThose ludicrously precise coordinates from Google.\n\n2\n\nTell sf that the data are centred on somewhere different than they really are! Some experimentation was required here to find a central ‘meridian’ that didn’t cut Antarctica in half.\n\n3\n\nApply Mercator to the recentred data. There’s probably a setting of the Oblique Mercator (+proj=omerc) that can do this sequence of transforms in one go, but since what I have works, I am leaving well alone."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-south-america",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-south-america",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#5 Bad Map Projection: South America",
    "text": "#5 Bad Map Projection: South America\n\n\n\nAlt-text: The projection does a good job preserving both distance and azimuth, at the cost of really exaggerating how many South Americas there are.\n\n\nXKCD 2256 is just delightfully dumb really. Truly a bad map projection. It’s also hard (for me) not to love a map projection that will throw everyone doing Worldle off their game."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-exterior-kansas",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-exterior-kansas",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#4 Bad Map Projection: Exterior Kansas",
    "text": "#4 Bad Map Projection: Exterior Kansas\n\n\n\nAlt-text: Although Kansas is widely thought to contain the geographic center of the contiguous 48 states, topologists now believe that it’s actually their outer edge.\n\n\nThe ‘comical’ claim about topologists in the alt-text for XKCD 2951 is actually… well… true on a spherical surface.\nI figured out a way to approximate this projection. First, we project the contiguous 48 states centred on the geographic center and enlarge them so they ostensibly cover a greater part of Earth’s surface. Any projection should work here; I’ve used an azimuthal equidistant one.\n\n\nCode\n1ctr &lt;- c(-98.583333, 39.833333)\n2anti &lt;- c(ctr[1] + 180, -ctr[2])\n3proj1 &lt;- str_glue(\"+proj=aeqd +lon_0={ctr[1]} +lat_0={ctr[2]}\")\n4proj2 &lt;- str_glue(\"+proj=aeqd +lon_0={anti[1]} +lat_0={anti[2]}\")\n\nstates_big &lt;- usmap::us_map() |&gt;\n  rename(geometry = geom) |&gt;\n  smoothr::densify(20) |&gt;\n  filter(!(abbr %in% c(\"AK\", \"HI\"))) |&gt;\n  st_transform(proj1) |&gt;\n5  mutate(geometry = geometry * matrix(c(5, 0, 0, 5), 2, 2)) |&gt;\n6  st_set_crs(proj1)\n\nggplot(states_big) + geom_sf() + theme_minimal()\n\n\n\n1\n\nThe ostensible centre of the contiguous US.\n\n2\n\nThe antipode of the central location.\n\n3\n\nForward projection: this can likely be any sensible projection with a central coordinate pair.\n\n4\n\nThe distance projection from the antipode, that will ‘invert’ the space.\n\n5\n\nThis multiplication expands the extent of the states.\n\n6\n\nAfter the multiplication we have to reset the projection.\n\n\n\n\n\n\n\n\n\n\n\nNext we apply an azimuthal equidistant projection centred on the antipode of the geographical centre. At this point we have to ‘turn Kansas inside out’, then remove and add it back into the inverted states.\n\n\nCode\nstates_inv &lt;- states_big |&gt;\n  st_transform(proj2)\n\n1kansas &lt;- states_inv |&gt;\n  st_bbox() |&gt;                        \n  st_as_sfc() |&gt;\n  as.data.frame() |&gt;\n  st_as_sf() |&gt;\n  st_difference(states_inv |&gt; \n                  filter(abbr == \"KS\")) |&gt;\n  st_as_sf()\n\nstates_inv &lt;- states_inv |&gt;\n2  filter(abbr != \"KS\") |&gt;\n3  bind_rows(kansas)\n\n\n\n1\n\nMake ‘inverse’ Kansas by subtracting Kansas from the bounding box of the data.\n\n2\n\nRemove Kansas.\n\n3\n\nAdd back inverted Kansas.\n\n\n\n\nAnd now we can make the final map. Note that it seems like an adjustment to the aspect ratio has been applied in the comic. I’ve not bothered with that step here. I assume that at some point Randall Munroe exports map data to a drawing package and does final tweaks there.\n\n\nCode\nggplot(states_inv) +\n  geom_sf(fill = \"white\") +\n  geom_sf_text(\n    data = states_inv |&gt; st_point_on_surface(), \n    aes(label = full), check_overlap = TRUE, size = 3) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"grey\"),\n        panel.border = element_rect(fill = NA, colour = \"black\",\n                                    linewidth = 1))"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#upside-down-map",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#upside-down-map",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#3 Upside-down Map",
    "text": "#3 Upside-down Map\n\n\n\nAlt-text: Due to their proximity across the channel, there’s long been tension between North Korea and the United Kingdom of Great Britain and Southern Ireland.\n\n\nPersonally, living in Aotearoa New Zealand I enjoy the genre of ‘upside down’ maps, but have always felt that the notion they change your whole perspective on the world is overstated, so XKCD 1500 is a nice gentle undermining of that, which I appreciate.9\nI briefly contemplated an attempt at making this one, but thought better of it. Turning a whole map through 180° is not difficult:\n\n\nCode\nw180 &lt;- World |&gt;  \n  mutate(geometry = geometry * diag(-1, 2, 2)) |&gt;\n  st_set_crs(4326) |&gt;\n  st_transform(\"+proj=eqearth\")\n\nggplot(w180) +\n  geom_sf() +\n  coord_sf(expand = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLikely the easiest way to make this into a map like the one in the comic is to export to a drawing format and move things around by hand."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-abslongitude",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-abslongitude",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#2 Bad Map Projection: ABS(Longitude)",
    "text": "#2 Bad Map Projection: ABS(Longitude)\n\n\n\nAlt-text: Positive vibes/longitudes only\n\n\nIf only rendering XKCD 2807 was as easy as doing longitude = abs(longitude).10 In R things get a bit more involved. This would really be very straightforward in a drawing package.\n\n\nCode\n1get_hemisphere &lt;- function(central_meridian = 0,\n2                           density = 1) {\n  lons &lt;- c( 1,  1, -1, -1,  1) * 90 + central_meridian\n  lats &lt;- c(-1,  1,  1, -1, -1) * 90\n  st_polygon(list(matrix(c(lons, lats), ncol = 2))) |&gt;\n    st_sfc() |&gt;\n    as.data.frame() |&gt;\n    st_as_sf(crs = 4326) |&gt;\n    smoothr::densify(density)\n}\n\n3proj = \"+proj=eqearth\"\nhemi_w &lt;- get_hemisphere(-90)\nhemi_e &lt;- get_hemisphere(90)\n\n4world_w &lt;- World |&gt;\n  st_intersection(hemi_w) |&gt;\n  mutate(geometry = geometry * matrix(c(-1, 0, 0, 1), 2, 2)) |&gt;\n  st_set_crs(4326)\n\nworld_e &lt;- World |&gt;\n  st_intersection(hemi_e)\n\n5world_abs &lt;- world_w |&gt;\n  bind_rows(world_e) |&gt;\n  mutate(id = 1) |&gt;\n  group_by(id) |&gt;\n  summarise() |&gt;\n  mutate(geometry = geometry - c(90, 0)) |&gt;\n  st_set_crs(4326) |&gt;\n  st_transform(proj)\n\nhalf_globe &lt;- get_hemisphere(density = 50) |&gt; \n  st_transform(proj)\n\nggplot() + \n  geom_sf(data = half_globe, lwd = 0) +\n  geom_sf(data = world_abs, fill = \"white\") +\n  geom_sf(data = half_globe, fill = NA, lwd = 1) +\n  coord_sf(expand = FALSE) +\n  theme_void()\n\n\n\n1\n\nIt’s convenient to have a function for making hemispheres centred on a specified meridian.\n\n2\n\nThe density parameter gives us hemispheres that still look right after transformation to a non-rectangular projection.\n\n3\n\nAn educated guess that the map is in the Equal Earth projection.\n\n4\n\nThis is where we make all longitudes positive.\n\n5\n\nCombine the hemispheres and shift them so they are centred on the prime meridian and transform to Equal Earth"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-time-zones",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#bad-map-projection-time-zones",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "#1 Bad Map Projection: Time Zones",
    "text": "#1 Bad Map Projection: Time Zones\n\n\n\nAlt-text: This is probably the first projection in cartographic history that can be criticized for its disproportionate focus on Finland, Mongolia, and the Democratic Republic of the Congo.\n\n\nI’m really not convinced XKCD 1799 is a bad map projection. It looks wonky as hell, but it’s certainly useful, which is all we can ask of any projection.11 I’m fairly confident Waldo Tobler would have approved, although I never had an opportunity to ask.\nIt’s interesting to contemplate what kind of automated process might be used to produce this map, but… I’m not even going to try. Happy to hear from anyone who gives it a go!"
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#in-conclusion",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#in-conclusion",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "In conclusion",
    "text": "In conclusion\nClose analysis of the data confirms no clear long term trend in the badness of XKCD’s bad map projections, although more recent examples may be getting worse.12\n\n\n\n\n\n\n\n\n\nAnd that’s a wrap (or fold, or cut, or… some transformation anyway)."
  },
  {
    "objectID": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#footnotes",
    "href": "posts/2025-05-16-xkcd-bad-projections-ranked/xkcd-bad-projections-ranked.html#footnotes",
    "title": "Nine XKCD bad map projections: ranked!1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInsert ‘You won’t believe how bad they are’ or similar clickbait headline here.↩︎\nWe’re working with a loose definition of the word ‘serious’ here.↩︎\nAlong with #1500 which was not called out as such↩︎\nYMMV. Don’t @ me.↩︎\nI’d obviously rank it higher if it was called United Straya.↩︎\nI have relatives there.↩︎\nMaybe someone should have a word with a recently elected president on this matter.↩︎\nSee XKCD 2170 on that subject↩︎\nI’m not sure how I feel about ‘The United Kingdom of Great Britain and Southern Ireland’, but that’s a whole other question…↩︎\nProbably in d3 it is.↩︎\nOr for that matter, model…↩︎\nThe same analysis suggests that while negative ranks are a possibility they remain unlikely.↩︎"
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html",
    "title": "30 Day Map Challenge 2025",
    "section": "",
    "text": "I did all of the 30 Day Map Challenge in 2023 and I won’t be repeating that experience again in a hurry. Last year I skipped it completely. This year I’ll do a few here and there as the mood takes me, especially if as in this example, I already have a lot of the material to hand.\nUnlike some of the more ‘tutorial’ posts on the website, I’ll leave the explanations to the code comments, and just get on with making the map.\nCode\n# package imports\nlibrary(sf)\nlibrary(rmapshaper)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cols4all)"
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#getting-points-along-contour-lines",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#getting-points-along-contour-lines",
    "title": "30 Day Map Challenge 2025",
    "section": "Getting points along contour lines",
    "text": "Getting points along contour lines\nThe map that got me thinking I could do ‘lines’ quickly was a vector field ‘hachure map’ of a DEM, which you’ll find here where the lines representing the gradient of a DEM were centred on the raster cells. Work by others on hachures1 suggests that it’s better to position the hachures along contour lines.\nFirst a couple of convenience functions.\n\n\nCode\n# round x to nearest y\nround_to_nearest &lt;- function(x, y) round(x / y) * y\n\n# get contour levels for supplied dem at specified interval\ncontour_levels &lt;- function(dem, interval) {\n  range_z &lt;- range(values(dem)) |&gt; round_to_nearest(interval)\n  seq(range_z[1], range_z[2], interval)\n}\n\n\nThese support a function to get points at equal intervals along contour lines from a DEM specified by spacing on contours contour_interval apart.\n\n\nCode\nget_contour_pts &lt;- function(dem, spacing = 10, contour_interval = 5) {\n  contour_lines &lt;- dem |&gt; \n    as.contour(levels = contour_levels(dem, contour_interval)) |&gt;\n    st_as_sf() |&gt;\n1    ms_explode() |&gt;\n    mutate(length = st_length(geometry) |&gt; units::drop_units())\n  \n  lines &lt;- contour_lines |&gt; pull(geometry) |&gt; lapply(st_sfc)\n  line_lengths &lt;- contour_lines |&gt; pull(length)\n  \n2  mapply(st_line_interpolate,\n         lines,\n         mapply(seq, 0, line_lengths, spacing)) |&gt;\n3    do.call(what = c) |&gt;\n    data.frame() |&gt;\n    st_sf(crs = 2193)\n}\n\n\n\n1\n\nIf I use st_cast(\"LINESTRING\") here it discards all but the first line in each MULTILINESTRING. rmapshaper::explode() does what I want, which is to break the MULTILINESTRINGs into individual LINESTRINGs.\n\n2\n\nProbably over-using mapply here, but it’s too much fun not to. The outer one applies st_line_interpolate to the linestrings, and the inner one makes sequences of steps at the required spacing along each contour.\n\n3\n\nThen concatenate (c) to convert to a points dataframe."
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#assembling-geomorphometric-data",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#assembling-geomorphometric-data",
    "title": "30 Day Map Challenge 2025",
    "section": "Assembling geomorphometric data",
    "text": "Assembling geomorphometric data\nThe next function takes a set of points, and assigns slope, aspect, and hillshade values from rasters to them to return a dataframe with all the information needed to make hachures (at least for my purposes).\n\n\nCode\nget_geomorph_df &lt;- function(pts, slope, aspect, hillshade) { \n1  dx &lt;- sin(aspect) * tan(slope)\n  names(dx) &lt;- \"dx\"\n  dy &lt;- cos(aspect) * tan(slope)\n  names(dy) &lt;- \"dy\"\n  pts |&gt; \n    st_coordinates() |&gt;\n    data.frame() |&gt; \n    rename(x = X, y = Y) |&gt;\n    bind_cols(extract(slope, pts, ID = FALSE)) |&gt;\n    bind_cols(extract(dx, pts, ID = FALSE)) |&gt;\n    bind_cols(extract(dy, pts, ID = FALSE)) |&gt;\n    bind_cols(extract(hillshade, pts, ID = FALSE))\n}\n\n\n\n1\n\nWe decompose aspect into its x and y components because extracting values to points involves interpolation and you can interpolate distances, but not angles. Also note that the usual trigonometric relation of cosine and sine to x and y directions is reversed here because aspects is expressed as an azimuth clockwise from north."
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#make-hachures-from-geomorphometric-data",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#make-hachures-from-geomorphometric-data",
    "title": "30 Day Map Challenge 2025",
    "section": "Make hachures from geomorphometric data",
    "text": "Make hachures from geomorphometric data\nThe function below takes the data compiled by the previous function and assembles line segments to be used as hachures. It scales the hachures to a length specified by the scale argument, starting from the contour and running uphill. I found this orientation gave me nicer results on this terrain than running them downhill from the contours. The function could be modified to displace the hachures in relation to the contour lines, perhaps starting a little uphill and extending downhill across them without too much difficulty.2 In one version of this code, I had that option, but it was too many things to think about, so in the end I left it out.3\n\n\nCode\nget_hachure_vectors &lt;- function(df, scale = 25) {\n  df |&gt; mutate(x0 = x, x1 = x - dx * scale,\n               y0 = y, y1 = y - dy * scale) |&gt;\n    select(x0, x1, y0, y1) |&gt;\n1    apply(1, matrix, ncol = 2, simplify = FALSE) |&gt;\n    lapply(st_linestring) |&gt;\n    st_sfc() |&gt;\n    as.data.frame() |&gt;\n    st_sf(crs = 2193) |&gt;\n2    bind_cols(df)\n}\n\n\n\n1\n\nThis forms the matrix required to make line segments from (x0 y0) to (x1 y1) pairs.\n\n2\n\nAdd the geomorphometric data back to the hachures so that is available for styling."
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#putting-it-all-together",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#putting-it-all-together",
    "title": "30 Day Map Challenge 2025",
    "section": "Putting it all together",
    "text": "Putting it all together\nFirst we read in and process the DEM.\n\n\nCode\ndem &lt;- rast(\"zealandia-5m.tif\")\ncellsize &lt;- res(dem)[1]\n# this DEM has a row of NAs so get rid of them\ndem &lt;- dem |&gt; crop(ext(dem) + c(0, 0, -cellsize, 0))\n\ngauss &lt;- focalMat(dem, 4, \"Gauss\")\ndem &lt;- dem |&gt; focal(gauss, mean, expand = TRUE)\n\nslope  &lt;- dem |&gt; terrain(unit = \"radians\")\naspect &lt;- dem |&gt; terrain(v = \"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, direction = 135, normalize = TRUE)\n\n1dem &lt;- dem |&gt; crop(ext(dem) + rep(-cellsize, 4))\nslope &lt;- slope |&gt; crop(dem)\naspect &lt;- aspect |&gt; crop(dem)\nhillshade &lt;- hillshade |&gt; crop(dem)\n\n2cxy &lt;- dem |&gt;\n  ext() |&gt;\n  matrix(ncol = 2) |&gt;\n  apply(2, mean)\n\n\n\n1\n\nAfter smoothing and terrain analysis we lose rows and columns around the edge, so we crop all the layers to the same extent here so they have no NA values.\n\n2\n\nThe centre of the DEM is useful for trimming maps to restricted extents.\n\n\n\n\nAnd now we can run the functions from above to get a set of hachures. I’ve experimented with settings, and a contour interval of 5m with points at 10m spacing along them is quite nice for this DEM.\n\n\nCode\npoints &lt;- get_contour_pts(dem, spacing = 10, contour_interval = 5)\npoints |&gt; head()\n\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1747094 ymin: 5426809 xmax: 1747096 ymax: 5426857\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n                 geometry\n1 POINT (1747096 5426809)\n2 POINT (1747095 5426819)\n3 POINT (1747095 5426829)\n4 POINT (1747095 5426839)\n5 POINT (1747094 5426849)\n6 POINT (1747095 5426857)\n\n\nNext make the geomorphometric data.\n\n\nCode\ndf &lt;- get_geomorph_df(points, slope, aspect, hillshade)\ndf |&gt; glimpse()\n\n\nRows: 79,646\nColumns: 6\n$ x         &lt;dbl&gt; 1747096, 1747095, 1747095, 1747095, 1747094, 1747095, 174709…\n$ y         &lt;dbl&gt; 5426809, 5426819, 5426829, 5426839, 5426849, 5426857, 542678…\n$ slope     &lt;dbl&gt; 0.07065191, 0.35487763, 0.36189409, 0.37607592, 0.31732243, …\n$ dx        &lt;dbl&gt; 0.04492523, 0.36862739, 0.37601937, 0.39448706, 0.31067836, …\n$ dy        &lt;dbl&gt; 0.054681579, 0.037854596, -0.043843913, -0.017379723, 0.1064…\n$ hillshade &lt;dbl&gt; 178.62155, 208.62245, 218.69818, 216.55379, 196.04494, 205.8…\n\n\nAnd form hachures.\n\n\nCode\nhachures &lt;- get_hachure_vectors(df, 20)\nhachures |&gt; glimpse()\n\n\nRows: 79,646\nColumns: 7\n$ x         &lt;dbl&gt; 1747096, 1747095, 1747095, 1747095, 1747094, 1747095, 174709…\n$ y         &lt;dbl&gt; 5426809, 5426819, 5426829, 5426839, 5426849, 5426857, 542678…\n$ slope     &lt;dbl&gt; 0.07065191, 0.35487763, 0.36189409, 0.37607592, 0.31732243, …\n$ dx        &lt;dbl&gt; 0.04492523, 0.36862739, 0.37601937, 0.39448706, 0.31067836, …\n$ dy        &lt;dbl&gt; 0.054681579, 0.037854596, -0.043843913, -0.017379723, 0.1064…\n$ hillshade &lt;dbl&gt; 178.62155, 208.62245, 218.69818, 216.55379, 196.04494, 205.8…\n$ geometry  &lt;LINESTRING [m]&gt; LINESTRING (1747096 5426809..., LINESTRING (17470…"
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#make-some-maps",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#make-some-maps",
    "title": "30 Day Map Challenge 2025",
    "section": "Make some maps",
    "text": "Make some maps\nHere’s a map of a 1km square part of the area showing the relation of the hachures to the contour lines.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = as.contour(dem, levels = contour_levels(dem, 5)) |&gt; st_as_sf(),\n    aes(colour = level), linewidth = 1) +\n  scale_colour_continuous_c4a_seq(\n    palette = \"-tableau.green_gold\", limits = c(50, 500)) +\n  geom_sf(data = hachures) +\n  coord_sf(xlim = cxy[1] + c(-500, 500),\n           ylim = cxy[2] + c(-500, 500), expand = FALSE) +\n  guides(colour = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nHere’s a final (R-generated) map, using the hillshade attribute to lighten or darken the hachures based on that attribute of the data, and making the line thickness of hachures dependent on the slope. I’ve added streams and lakes to show that the hachures ‘make sense’.\n\n\nCode\nggplot() +\n  geom_sf(data = hachures, aes(linewidth = slope ^ 2, colour = sqrt(hillshade))) +\n  scale_linewidth_identity() +\n  scale_colour_distiller(palette = \"Greys\", direction = 1) +\n  geom_sf(data = lake, fill = \"skyblue\", colour = NA) +\n  geom_sf(data = streams, colour = \"dodgerblue\", linewidth = 0.5) +\n  guides(colour = \"none\", linewidth = \"none\") +\n  coord_sf(xlim = ext(dem)[1:2], ylim = ext(dem)[3:4], expand = FALSE) +\n  theme_void()"
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#refining-the-hachures-in-qgis",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#refining-the-hachures-in-qgis",
    "title": "30 Day Map Challenge 2025",
    "section": "Refining the hachures in QGIS",
    "text": "Refining the hachures in QGIS\nBy exporting the data to points, and working with geometry generators in QGIS we can make hachures that are elongated triangles, so that the hachures are thicker at their lower elevation ends.\nHere’s a map I made that way.\n\nIf you zoom in (click on the image) you’ll see that the hachures are elongated triangles. I don’t know if that means they are still ‘lines’ or not.4 I tried to get this effect using the ggarrow and ggquiver packages, but the former was memory-limited and slow for data this size, and the latter is not flexible enough.\nThis used the following geometry generator expression to generate polygon geometries:\nmake_triangle(\n    make_point($x - 25 * \"dx\", $y - 25 * \"dy\"),\n    make_point($x - 2 * \"dy\", $y + 2 * \"dx\"),\n    make_point($x + 2 * \"dy\", $y - 2 * \"dx\")\n)\nand the following expression for the colour fill\ncolor_rgba(\n    0, 0, 0, \n    255 * sqrt(scale_linear(\"hillshade\", 0, 255, 0, 1))\n)\nA more complete hachure map solution would require that we consider relationships between the hachures and other features (streams, lakes, etc.). The kind of thing a cartographer would do, in fact.\nOverall this took a bit more work to refine the map from the rough cut of a few days ago than I hoped, but I’m reasonably happy with this version, even as it leaves plenty of scope for improvement."
  },
  {
    "objectID": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#footnotes",
    "href": "posts/2025-11-03-hachures-30-day-maps-2025-day-2/index.html#footnotes",
    "title": "30 Day Map Challenge 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post by Daniel Huffman links to several others and is a good place to start.↩︎\nA nice refinement for someone to try, perhaps.↩︎\nSometimes less is more.↩︎\nPerhaps I can double dip and count this one against day 2 (lines) and day 3 (polygons).↩︎"
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html",
    "title": "A population based binary space partition",
    "section": "",
    "text": "In a recent post I developed some R code to quadrisect the population of Aotearoa New Zealand. At the end of that post I airily suggested\nThe thought was something like, bisect the population data (by population), then bisect each half, then bisect each quarter and so on. The bisection procedure using weighted medians was thoroughly explored in the earlier post.\nNow, as promised, here’s that iterative bisection process, which yields something like a population quadtree. Each leaf of the tree accounts for some power-of-two fraction of the population (in the example below 1/1024th). If we had the location of every individual in the data as a point, the result is roughly what we would get if we were adding those points to a binary space partition data structure.\nIt would be nice to do this as a quad-tree where at each step we subdivide the population into 4 more or less equal groups, but that would require allowing the lines subdividing the space to be at arbitrary angles at each step and would demand much more complicated coding than what follows. Sticking to halving the population at each step means we can work with only east-west or north-south bisectors and simplifies matters greatly. I’ll set the more complicated quad-tree approach to one side for now.\nAs an indication of where we are going, here’s the end result, showing approximately equal population rectangular areas around Christchurch."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#preliminaries",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#preliminaries",
    "title": "A population based binary space partition",
    "section": "Preliminaries",
    "text": "Preliminaries\nAs usual, we need some libraries and data.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(spatstat)  # for weighted median function\nlibrary(data.tree) # to build a tree\n\npop &lt;- read.csv(\"nz-pop-grid-250m.csv\") |&gt;\n1  dplyr::filter(x &lt; 2.2e6)\nnz &lt;- st_read(\"nz.gpkg\")\n\n\n\n1\n\nThis is to remove the far-flung Chatham Islands, which in this case detract from the pattern of the quadtree.\n\n\n\n\nA useful helper functions is get_rectangle() to return a sf polygon based on x and y limits.\n\n\nCode\n1get_rectangle &lt;- function(xmin, ymin, xmax, ymax) {\n  st_polygon(list(matrix(c(xmin, xmin, xmax, xmax, xmin,\n                           ymin, ymax, ymax, ymin, ymin), nc = 2)))\n}\n\n\n\n1\n\nA convenience function to get an sf polygon given x and y limits."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#iterative-population-bisection",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#iterative-population-bisection",
    "title": "A population based binary space partition",
    "section": "Iterative population bisection",
    "text": "Iterative population bisection\nCentral to this whole exercise is a function that given a dataframe of x, y, and population data, along with a bounding extent, returns two rectangles within the extent, each containing half the population. This is accomplished using a weighted median as in the previous post. Because we are working only with north-south or east-west subdivisions there isn’t any especially complicated geometry needed to handle this.\n\n\nCode\nsplit_population &lt;- function(pop_pts, bounds) {\n1  w &lt;- bounds[1]; e &lt;- bounds[3];\n  s &lt;- bounds[2]; n &lt;- bounds[4];\n  width &lt;- e - w; height &lt;- n - s;\n2  xy &lt;- pop_pts |&gt; filter(x &gt; w, x &lt;= e, y &gt; s, y &lt;= n)\n  bbs &lt;- list()\n3  if (height &gt; width) { # cut east-west at median y\n    my &lt;- weighted.median(xy$y, xy$pop_250m_grid)\n    south &lt;- xy |&gt; filter(y &lt;= my)\n4    x1 &lt;- min(south$x) - 125; x2 &lt;- max(south$x) + 125;\n    y1 &lt;- min(south$y) - 125; y2 &lt;- my;\n    bbs[[1]] &lt;- get_rectangle(x1, y1, x2, my)\n    north &lt;- xy |&gt; filter(y &gt; my)\n    x1 &lt;- min(north$x) - 125; x2 &lt;- max(north$x) + 125;\n    y1 &lt;- my; y2 &lt;- max(north$y) + 125; \n    bbs[[2]] &lt;- get_rectangle(x1, my, x2, y2)\n  } else { # cut north-south at median x\n    mx &lt;- weighted.median(xy$x, xy$pop_250m_grid)\n    west &lt;- xy |&gt; filter(x &lt;= mx)\n    x1 &lt;- min(west$x) - 125; x2 &lt;- mx;\n    y1 &lt;- min(west$y) - 125; y2 &lt;- max(west$y) + 125;\n    bbs[[1]] &lt;- get_rectangle(x1, y1, mx, y2)\n    east &lt;- xy |&gt; filter(x &gt; mx)\n    x1 &lt;- mx; x2 &lt;- max(east$x) + 125;\n    y1 &lt;- min(east$y) - 125; y2 &lt;- max(east$y) + 125;\n    bbs[[2]] &lt;- get_rectangle(mx, y1, x2, y2)\n  }\n  bbs\n}\n\n\n\n1\n\nUnpack the bounding box to west, east, bottom, and top values, and use these to get width w and height h.\n\n2\n\nWe are only interested in the population inside the bounding box.\n\n3\n\nIf the area is taller than it is wide, then cut east-west at the weighted median y coordinate, otherwise cut north-south at the weighted median x coordinate.\n\n4\n\nHere, and elsewhere the 125 offsets account for the fact that the grid cells are 250m, but we are using their central coordinates.\n\n\n\n\nThe bisection runs east-west if the extent of the bounding area is longer north-south than east-west, and will run north-south otherwise. This is to promote ‘squarer’ rectangles as outputs, although as we’ll see the vagaries of population distribution mean that plenty of long skinny rectangles make it through the process.\nUsing this splitter function it is straightforward to iteratively subdivide the data into halves by population to some requested depth. I found an R package data.tree that means I can store the results of an iterative splitting process like this as a tree, or more accurately as a linked set of ‘nodes’ with associated attributes. As is often the case in R, when a data structure that is not tabular appears the syntax is a little weird1, so figuring out how to use it took some experimentation. The Node$new() and using $ to invoke methods on objects was new to me, but I figured it out in the end.\nHaving struggled a bit to get data.tree working,2 the reward is nice compact code, which much more clearly expresses the intent to create a tree structure than some earlier sprawling and rather kludge-y list-based code.\n\n\nCode\nbsp &lt;- Node$new(\n  \"X\", bb = get_rectangle(min(pop$x), min(pop$y), \n1                          max(pop$x), max(pop$y)))\n\nfor (level in 1:10) {\n2  leaves &lt;- bsp$leaves\n  for (i in 1:length(leaves)) {\n    leaf &lt;- leaves[[i]]\n    id &lt;- leaf$name\n    bbs &lt;- split_population(pop, leaf$bb |&gt; st_bbox())\n    leaf$AddChildNode(Node$new(str_c(id, 1), bb = bbs[[1]]))\n    leaf$AddChildNode(Node$new(str_c(id, 2), bb = bbs[[2]]))\n  }\n}\n\n\n\n1\n\nThe ‘root’ node which we call ‘X’ which has as its ‘bb’ attribute, a bounding box of all the population data.\n\n2\n\nThen, we repeatedly split the ‘leaf’ nodes of the tree in two, adding them to each leaf as child nodes.\n\n\n\n\nA minor difficulty once the tree has been constructed is to retrieve the sf geometries intact. Because sf geometries are thinly wrapped R matrices and lists, they are prone to ‘reverting to type’ when you include them in R lists or other containers. Using data.tree the polygons did indeed revert to matrices, but it’s not difficult to retrieve them as polygons, which we do in the code below to assemble final simple feature datasets for mapping.\n\n\nCode\n1all_levels &lt;- bsp$Get(\"bb\") |&gt;\n2  lapply(list) |&gt;\n3  lapply(st_polygon) |&gt;\n4  st_as_sfc() |&gt;\n  as.data.frame() |&gt; \n  st_sf(crs = 2193) |&gt; \n5  mutate(level = bsp$Get(\"level\"),\n         id = bsp$Get(\"name\"))\n\n6top_level &lt;- all_levels |&gt; filter(level == 11)\n\n\n\n1\n\nThis is data.tree‘s syntax for retrieving the ’bb’ attribute of each node in the tree.\n\n2\n\nWrap the matrix we get back in a list.\n\n3\n\nAnd convert to an sf polygon.\n\n4\n\nThen apply the well-worn st_as_sfc() |&gt; as.data.frame() |&gt; st_sf() sequence to convert to a simple features dataset.\n\n5\n\nThen we can append the binary space partition’s level and id attributes to our dataframe.\n\n6\n\nIt’s convenient to separate out the highest level of detail rectangles as a separate layer."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#some-maps-of-the-hierarchy",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#some-maps-of-the-hierarchy",
    "title": "A population based binary space partition",
    "section": "Some maps of the hierarchy",
    "text": "Some maps of the hierarchy\nSubdivision completed we can make a map of the final level of the tree, with 1024 rectangles. These are of roughly equal populations of around 5000.\n\n\nCode\nggplot(nz) + \n  geom_sf(fill = \"grey\", lwd = 0) +\n  geom_sf(data = top_level, fill = \"#ff101040\", \n          colour = \"white\", lwd = 0.15) +\n  guides(fill = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 2: All 1024 cells at level 11 of the partition.\n\n\n\n\n\nIt’s also interesting to see the stages along the way to the final subdivision, which is conveniently done using a facet_wrap based on the level attribute in the all_levels dataframe\n\n\nCode\nggplot(nz) + \n  geom_sf(fill = \"grey\", lwd = 0) +\n  geom_sf(data = all_levels |&gt; filter(level &gt; 1), \n          fill = \"#ff101040\", colour = \"white\", lwd = 0.1) + \n  facet_wrap( ~ level, ncol = 5) +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 3: The sequence of bisections along the way to 1024 rectangles."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#a-close-up-look",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#a-close-up-look",
    "title": "A population based binary space partition",
    "section": "A close up look",
    "text": "A close up look\nIt’s also useful to look at a zoomed in local areas. As I’ve noted previously a big chunk of the country’s population is in Auckland, so it’s perhaps most interesting to look there. What emerges is a picture over an extended area3 of relatively evenly distributed population everywhere that is inhabitated (all the pink areas of the map), except for a concentration in a tiny area of the downtown centre. The result in terms of the space partition is relatively equal-sized areas across much of the populated suburban sprawl.4\n\n\nCode\nbb &lt;- c(1.735e6, 5.895e6, 1.785e6, 5.935e6)\nggplot() +\n  geom_sf(data = nz, fill = \"white\", lwd = 0) + \n  geom_tile(data = pop, aes(x = x, y = y, fill = pop_250m_grid)) +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +\n  geom_sf(data = nz, fill = NA, colour = \"#1e90ff80\", lwd = 0.3) + \n  geom_sf(data = all_levels |&gt; filter(level == 11), \n          fill = \"#00000030\", colour = \"white\", lwd = 0.2) +\n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)], expand = FALSE) +\n  guides(fill = \"none\") +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA, linewidth = 0.5), \n        panel.background = element_rect(fill = \"#1e90ff30\"))\n\n\n\n\n\n\n\n\nFigure 4: A zoomed in view around Auckland of the final layer of the partition."
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#and-a-zoomable-view",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#and-a-zoomable-view",
    "title": "A population based binary space partition",
    "section": "And a zoomable view",
    "text": "And a zoomable view\nThis doesn’t really need any further comment. The hover shows the id variable which encodes the sequence of west-east or north-south choices by which any given rectangle was arrived at. See if you can find X1111111111 and X2222222222. It’s trickier to find X1212121212.5\n\n\nCode\ntmap_mode(\"view\")\ntm_shape(top_level) +\n  tm_polygons(fill = \"#ff101040\", border.col = \"white\", \n              popup.vars = FALSE, id = \"id\") +\n  tm_view(use_WebGL = FALSE)"
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#but-what-is-it-good-for",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#but-what-is-it-good-for",
    "title": "A population based binary space partition",
    "section": "But what is it good for?",
    "text": "But what is it good for?\nI’m honestly not sure about this! I learned about data.tree while making it, which is good.\nFor me, the next back-burner project in this line of thought is to think about how to equalize the size of the rectangles perhaps en route to cartograms. I find the rectilinear cartograms in early editions of Kidron and Segal’s The State of the World Atlas quite compelling. See this page for some spreads.6 In any case, perhaps this iterative bisection technique could be used as part of an automated workflow to make them.\nOr perhaps there is a population-centric web map zooming mechanism somewhere to be found in all this?"
  },
  {
    "objectID": "posts/2025-06-20-population-binary-space-partition/index.html#footnotes",
    "href": "posts/2025-06-20-population-binary-space-partition/index.html#footnotes",
    "title": "A population based binary space partition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI should probably say weirder, because, let’s face it, R’s syntax is weird from the get-go.↩︎\nWhile using data.tree was a little confusing, it was not as bad as second guessing what R lists will do when you append sf polygons to them.↩︎\nThe extent of the mapped area is 50 by 40km.↩︎\nEchoes here of Austin Mithchell’s phrase ‘the quarter acre paradise’, which sounds rather belittling to me, but was apparently well received in New Zealand at the time.↩︎\nTo save it taking too much of your time: it’s near Levin, not far from where my Tararuas adventure unfolded.↩︎\nThe internet tells me that edition of the atlas is from 1981: Kidron M and R Segal. 1981. The State of the World Atlas. Pan Books, London. Many more recent editions have come and gone. The latest seems to be this one, although rectilinear cartograms are no longer quite so heavily featured.↩︎"
  },
  {
    "objectID": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html",
    "href": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html",
    "title": "GIS, a transformational approach",
    "section": "",
    "text": "Earlier posts in this series, wherein I exhaustively implement many of the GIS transformations possible among the four main data types, point, lines, areas, and fields, are linked below:\nAs noted in the last of these, “area data are complicated”, so I gave up chose to split the post on area transformations halfway through and now I’m back to slay the dragon of area → area transformations. Yep… there will be yet another post after this one dealing with area → field transformations sometime soon.\nCode\nlibrary(sf)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(tmap)\n\ntheme_set(theme_void())"
  },
  {
    "objectID": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#from-areas",
    "href": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#from-areas",
    "title": "GIS, a transformational approach",
    "section": "From areas…",
    "text": "From areas…\nAs always, we need some data.\n\n\nCode\npolygons &lt;- st_read(\"sa2-generalised.gpkg\") |&gt; \n  mutate(\n    area_km2 = (st_area(geom) |&gt; units::drop_units()) / 1e6,\n    pop_density_km2 = population / area_km2)\n\n\nThese are Statistics New Zealand census data from 2023. I have augmented the data with a population density attribute to make a point later on. Before I get to that, a personal bug-bear here is how annoying I find it that I have to suppress sf’s default addition of units to calculated areas to do something as simple as create a population density, without getting caught up in endlessly assigning units to simple scaling factors.\nAnyway, here are what those look like.\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_sf(data = polygons, aes(fill = population), colour = NA) +\n  scale_fill_distiller(\"Population\", palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title.position = \"top\",\n        legend.key.width = unit(1, \"cm\"),\n        legend.text = element_text(angle = 45, hjust = 1))\n\ng2 &lt;- ggplot() +\n  geom_sf(data = polygons, aes(fill = pop_density_km2), colour = NA) +\n  scale_fill_distiller(\"Population per sq.km\", \n                       palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title.position = \"top\",\n        legend.key.width = unit(1, \"cm\"),\n        legend.text = element_text(angle = 45, hjust = 1))\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 1: The SA2 population data, by count, and by density\n\n\n\n\n\nA year or two ago, a nice alternative here would have been to use tmap, which had a convert2density option for choropleth maps, which seamlessly did all the work required to make your map a density map. In version 4, you get a warning that the convert2density option is deprecated, and a recommendation to perform the calculation I performed above anyway. Oh well. Progress, gotta love it!\ntmap does make it easier to classify the map, which emphasises the essential lack of much in the way of population density in New Zealand. I promise, there are some census areas with those higher population densities, you just can’t see them at this scale.\n\n\nCode\ntm_shape(polygons) +\n  tm_fill(\n    \"pop_density_km2\",\n    fill.scale = tm_scale_intervals(n = 10, values = \"brewer.reds\"),\n    fill.legend = tm_legend(title = \"Pop density sq.km\", frame = FALSE)) +\n  tm_options(frame = FALSE)\n\n\n\n\n\n\n\n\nFigure 2: A classed choropleth population density map done right using tmap. Is there anybody home? Anybody at all?!\n\n\n\n\n\nOK, on to the transformations."
  },
  {
    "objectID": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#to-areas",
    "href": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#to-areas",
    "title": "GIS, a transformational approach",
    "section": "… to areas",
    "text": "… to areas\n\nPolygon buffer\nWell… we can dispense with this one quickly. Rather than apply it to all the polygons, I’ll just grab one and apply it to that, to demonstrate a couple of things.\n\n\nCode\npoly &lt;- polygons$geom[100]\nbb &lt;- st_bbox(poly) \nxr &lt;- bb[c(1, 3)] + c(-400, 400)\nyr &lt;- bb[c(2, 4)] + c(-400, 400)\n\npoly_b_400 &lt;- poly |&gt; st_buffer(400)\npoly_b_400sq &lt;- poly |&gt; st_buffer(400, nQuadSegs = 0, joinStyle = \"MITRE\")\n\npar(mfrow = c(1, 2), mai = rep(0.1, 4))\nplot(poly, col = \"grey\", xlim = xr, ylim = yr)\nplot(poly_b_400, add = TRUE)\nplot(poly, col = \"grey\", xlim = xr, ylim = yr)\nplot(poly_b_400sq, add = TRUE)\n\n\n\n\n\n\n\n\nFigure 3: A single polygon buffered with the default settings and with nQuadSegs = 0 and joinStyle = \"MITRE\".\n\n\n\n\n\nThe first of these with the default settings is what we tend to think of buffering as doing. Why would you want to use the other options?\n\nNot so fast\nSo maybe we can’t dispense with this quite so quickly.\nJust to clarify what’s going on it’s worth zooming in on the figure above.\n\n\nCode\npar(mfrow = c(1, 2), mai = rep(0.1, 4))\nbb &lt;- poly |&gt; st_bbox()\nxr &lt;- bb[c(1, 3)] + c(2000, -200)\nyr &lt;- bb[c(2, 4)] + c(1500, -500)\nplot(poly, col = \"grey\", xlim = xr, ylim = yr)\nplot(poly_b_400, add = TRUE)\nplot(poly_b_400 |&gt; st_cast(\"POINT\"), add = TRUE, cex = 0.5)\nplot(poly, col = \"grey\", xlim = xr, ylim = yr)\nplot(poly_b_400sq, add = TRUE)\nplot(poly_b_400sq |&gt; st_cast(\"POINT\"), add = TRUE, cex = 0.5)\n\n\n\n\n\n\n\n\nFigure 4: The details of buffering up close.\n\n\n\n\n\nSo default buffering adds a lot of points to create the curve associated with each corner. Setting nQuadSegs to 0 reduces this explosion in points.\nGenerally speaking, the st_buffer defaults are fine, and limiting the number of additional corners inserted using nQuadSegs is unnecessary. You want nice rounded corners. But it’s good to know the option to limit the number of added corners is available, and there is at least one case I can think of where it can be useful.\nSometimes if you encounter polygon topology errors, either as a result of a series of transformations, or if those errors are present in supplied data, and repair methods like st_make_valid aren’t working, a recommended ‘fix’ is to ‘buffer up then buffer down’1 the offending polygons, by a small amount. Now, really, I recommend that you figure out where the topology error came from in the first place. Sometimes however, if you’re working on some problem, fixing topology is something you’d prefer to come back to later, and you just need a quick fix for the time being, and buffer-up buffer-down might be that quick fix.\nBut beware: here be dragons! To see this, make a square, and see how many points it has.\n\n\nCode\nsquare &lt;- st_polygon(\n  list(matrix(c(0, 0, 1, 1, 0,\n                0, 1, 1, 0, 0) * 100,\n              ncol = 2))) |&gt; st_sfc()\nsquare |&gt; \n  st_cast(\"POINT\") |&gt;\n  length()\n\n\n[1] 5\n\n\nAllowing for sf’s requirement that polygons be explicitly closed so that the first corner is repeated, there are four corners, and five is the correct answer. Now, buffer up and down, per the quick topology fix, and what do we get?\n\n\nCode\nsquare1 &lt;- square |&gt; \n  st_buffer( 1) |&gt; \n  st_buffer(-1)\nsquare1 |&gt; \n  st_cast(\"POINT\") |&gt; \n  length()\n\n\n[1] 13\n\n\nWe can repair things, although repairing a repair seems like it’s missing the point a bit!\n\n\nCode\nsquare1 |&gt; \n  st_simplify(preserveTopology = TRUE, dTolerance = 2) |&gt;\n  st_cast(\"POINT\") |&gt; \n  length()\n\n\n[1] 5\n\n\nIf you are doing a lot of this kind of thing, it can get very ugly. If I sound like I’ve been burned by this problem, it’s because I have. When developing the weavingspace module in both its early R and more recent python incarnations, I am working with polygons at a low-level all the time and before I had realised this was a problem it wasn’t unusual for me to be working with ‘rectangles’ with dozens or even hundreds of corners!\nUsing nQuadSegs we can avoid the problem (usually!):\n\n\nCode\nsquare2 &lt;- square |&gt; \n  st_buffer( 1, nQuadSegs = 0, joinStyle = \"MITRE\") |&gt; \n  st_buffer(-1, nQuadSegs = 0, joinStyle = \"MITRE\")\nsquare2 |&gt; \n  st_cast(\"POINT\") |&gt; \n  length()\n\n\n[1] 5\n\n\nHaving said that, the vagaries of floating point might still have messed up your data anyway:\n\n\nCode\nsquare2\n\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: -4.524159e-15 xmax: 100 ymax: 100\nCRS:           NA\n\n\nOh well. The lesson here is that it’s probably better to figure out where the problem with your data came from than to attempt running repairs with quick fixes from the internet.2 The other lesson is that if you care deeply about geometries and their integrity as opposed to using them as a step along the way to perform analyses, then one or more of the following is true:\n\nYou should read up on floating point, or\nYou need to look into sf’s precision options, or\nYou should look into topology aware formats, such as topojson, or\nYou should look into libraries that can do precise geometry such as euclid3, or\nIf this sort of thing is going to drive you nuts, then you should consider a career in something other than geospatial, because this is not going to be fixed any time soon, if ever.\n\n\n\n\nPolygon overlay\nAh… polygon overlay. If there’s a more iconic GIS transformation, I’m not sure what it is.4\nHaving said that it’s a little hard to be entirely clear what ‘polygon overlay’ even is. I am going to interpret it here to mean any transformation that moves data from one set of polygon units to a different set of polygon units, in some more or less principled way.\n\n‘Dissolve’ or group_by\nA dissolve operation allows us to combine polygons into larger polygons based on some shared attribute, and is a cornerstone of many administrative geospatial data sets, such as, for example, census data.5\nFor mysterious reasons, although Aoteroa New Zealand in common with other countries organises census data hierarchically, it does not assign nice hierarchical IDs to the census units. So the smallest units in the hierarchy, meshblocks, can’t be conveniently merged based on IDs into Statistical Area 1 (SA1), or Statistical Area 2 (SA2), or anything else for that matter without access to a concordance table, which is a front runner in the race to be the most boring dataset on Earth, recording as it does, for every meshblock, the SA1, SA2, and the many other administrative and statistical areas of which it is a part.\nTo add to the fun, all the meshblock IDs are strings composed of digits… with leading zeros. Let that sink in.6 Anyway, the data we loaded, which are SA2s7 have an additional attribute from the concordance file included, designating which Territorial Authority (TA) each SA2 belongs to.\n\n\nCode\nglimpse(polygons)\n\n\nRows: 2,155\nColumns: 7\n$ SA22018_V1_00      &lt;chr&gt; \"100100\", \"100200\", \"100400\", \"100500\", \"100600\", \"…\n$ SA22018_V1_00_NAME &lt;chr&gt; \"North Cape\", \"Rangaunu Harbour\", \"Karikari Peninsu…\n$ population         &lt;int&gt; 1602, 2310, 1251, 1134, 1230, 2388, 3483, 936, 846,…\n$ TA2018_V1_00_NAME  &lt;chr&gt; \"Far North District\", \"Far North District\", \"Far No…\n$ geom               &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((1613644 614..., MULTIP…\n$ area_km2           &lt;dbl&gt; 826.726679, 272.222494, 174.138272, 177.231255, 4.5…\n$ pop_density_km2    &lt;dbl&gt; 1.937763, 8.485706, 7.183946, 6.398420, 272.296015,…\n\n\nWe can use this column as a basis for a dissolve operation, which in R is actually a tidyverse standard group_by operation.\n\n\nCode\ntas &lt;- polygons |&gt;\n  group_by(TA2018_V1_00_NAME) |&gt;\n  summarise(\n    population = sum(population), \n    area_km2 = sum(area_km2),\n    pop_density_km2 = population / area_km2)\n\n\nThe key point to notice here is that different variables may need to be recalculated in different ways when we move data from one set of polygons to another. In this case population and area sum, but we can’t add densities together and instead ‘combine’ the population densities by recalculating it from the new attributes of the dissolved dataset.\n\n\nCode\ng1 &lt;- ggplot() +\n  geom_sf(data = tas, aes(fill = population), colour = NA) +\n  scale_fill_distiller(\"Population\", palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title.position = \"top\",\n        legend.key.width = unit(1, \"cm\"),\n        legend.text = element_text(angle = 45, hjust = 1))\n\ng2 &lt;- ggplot() +\n  geom_sf(data = tas, aes(fill = pop_density_km2), colour = NA) +\n  scale_fill_distiller(\"Population per sq.km\", \n                       palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title.position = \"top\",\n        legend.key.width = unit(1, \"cm\"),\n        legend.text = element_text(angle = 45, hjust = 1))\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 5: New Zealand population by Territorial Authority.\n\n\n\n\n\nThere are now a few glimmers of life outside of Auckland, but further confirmation that to a first order approximation, everyone in New Zealand is crammed into the cities.8\n\n\n\nZooming in\nRegular readers will be surprised by how long I have tolerated the Chatham Islands in this post. I will now get rid of them focus on a more manageable subset of the data, and zoom in on Wellington.\n\n\nCode\nwellington &lt;- polygons |&gt;\n  filter(TA2018_V1_00_NAME == \"Wellington City\")\nbb &lt;- st_bbox(wellington)\nxr &lt;- bb[c(1, 3)]\nyr &lt;- bb[c(2, 4)]\n\nbasemap &lt;- ggplot() + \n  geom_sf(data = tas, fill = \"lightgrey\", colour = NA) +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA))\n\nbasemap +\n  geom_sf(data = wellington, aes(fill = pop_density_km2)) +\n  scale_fill_distiller(\"Pop density sq.km\", palette = \"Reds\", direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\n\n\n\n\n\n\n\nFigure 6: The population density data restricted to the Wellington City TA.\n\n\n\n\n\nLike the country as a whole, Wellington has a big low population area (Makara-Ohariu), which is that huge zone along the west coast.\n\n\nReal overlay\nA lot of folks won’t consider dissolve to really be an overlay operation, although I don’t see why not. But for the overlay enthusiasts, here’s another polygon dataset, not aligned with the SA2 data.\n\n\nCode\nschoolzones &lt;- st_read(\"school-zones.gpkg\") |&gt;\n  mutate(School_name = str_replace(School_name, \"\\x20*School\", \"\"))\n\n\n\n\nCode\nglimpse(schoolzones)\n\n\nRows: 19\nColumns: 3\n$ School_ID   &lt;int&gt; 660, 2816, 2826, 2827, 2854, 2874, 2875, 2876, 2883, 2894,…\n$ School_name &lt;chr&gt; \"Kahurangi\", \"Brooklyn (Wellington)\", \"Clifton Terrace Mod…\n$ geom        &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((1751570 542..., MULTIPOLYGON …\n\n\nThese are primary school zones for the region. Unfortunately, they overlap one another, because school enrolment zones in New Zealand are not centrally administered exclusive areas9—in GIS terms they are not a coverage. That makes mapping them a bit of a pain, but that’s not our central concern for now.\n\n\nCode\nbasemap +\n  geom_sf(data = wellington, fill = \"grey\", colour = \"white\") +\n  geom_sf(data = schoolzones, fill = NA, colour = \"black\") +\n  geom_sf_text(data = schoolzones, aes(label = School_name),\n               size = 3, check_overlap = TRUE) +\n  coord_sf(xlim = xr, ylim = yr)\n\n\n\n\n\n\n\n\nFigure 7: School zones in the Wellington City TA.\n\n\n\n\n\nA classic polygon overlay operation is to estimate population for the school zones based on populations in the SA2s.\nThis is where one of sf’s best kept secrets st_interpolate_aw comes into its own. We could go through the tedious business of spatially joining polygons and calculating areas of overlap and using those to assign fractions of the population from the source SA2s to the overlap areas, and so on, but st_interpolate_aw allows us to do all of this in a single step.\n\n\nCode\nschoolzones_pop &lt;- wellington |&gt; \n  select(population, pop_density_km2) |&gt;\n  st_interpolate_aw(schoolzones, extensive = TRUE) |&gt;\n  bind_cols(schoolzones |&gt; st_drop_geometry())\n\nglimpse(schoolzones_pop)\n\n\nRows: 19\nColumns: 5\n$ population      &lt;dbl&gt; 8686.39072, 5134.15226, 2021.22267, 2899.79282, 5923.1…\n$ pop_density_km2 &lt;dbl&gt; 7816.40278, 5008.60164, 4262.77765, 4940.75692, 9242.2…\n$ School_ID       &lt;int&gt; 660, 2816, 2826, 2827, 2854, 2874, 2875, 2876, 2883, 2…\n$ School_name     &lt;chr&gt; \"Kahurangi\", \"Brooklyn (Wellington)\", \"Clifton Terrace…\n$ geometry        &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((1751570 542..., MULTIPOLY…\n\n\nAnd now we can map them:\n\n\nCode\ng1 &lt;- basemap +\n  geom_sf(data = schoolzones_pop, aes(fill = population)) +\n  scale_fill_distiller(\"Population\", palette = \"Reds\", direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\ng2 &lt;- basemap +\n  geom_sf(data = schoolzones_pop, aes(fill = pop_density_km2)) +\n  scale_fill_distiller(\"NOT REALLY\\npop density\\nsq.km\", palette = \"Reds\", \n                       direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 8: Maps of estimated school zone population and (incorrect) school zone population density\n\n\n\n\n\nBUT (and it’s big but10) we have to be careful here. While population can be interpolated this way, population density cannot. That’s what the extensive option in st_interpolate_aw is for. As the documentation points out\n\nextensive logical; if TRUE, the attribute variables are assumed to be spatially extensive (like population) and the sum is preserved, otherwise, spatially intensive (like population density) and the mean is preserved.\n\nSo, the proper way to proceed is.\n\n\nCode\nsz_pop &lt;- wellington |&gt; \n  select(population) |&gt;\n  st_interpolate_aw(schoolzones, extensive = TRUE)\n\nsz_pop_density &lt;- wellington |&gt; \n  select(pop_density_km2) |&gt;\n  st_interpolate_aw(schoolzones, extensive = FALSE)\n\n\nWhich gives us the correct result below.\n\n\nCode\ng1 &lt;- basemap +\n  geom_sf(data = sz_pop, aes(fill = population)) +\n  scale_fill_distiller(\"Population\", palette = \"Reds\", direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\ng2 &lt;- basemap +\n  geom_sf(data = sz_pop_density, aes(fill = pop_density_km2)) +\n  scale_fill_distiller(\"Pop density\\nsq.km\", palette = \"Reds\", \n                       direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\ng1 | g2\n\n\n\n\n\n\n\n\nFigure 9: Maps of estimated school zone population and correct school zone population density\n\n\n\n\n\nProvided we keep this aspect in mind st_interpolate_aw is a great tool. You can even use it to assign polygon data to ‘raster’ layers, if you are so inclined. Below, I make a ‘raster’ layer, which is actually a set of grid cell polygons, and assign population to them.\n\n\nCode\ncells &lt;- wellington |&gt; \n  st_make_grid(cellsize = 100) |&gt;\n  st_sf() |&gt;\n  st_filter(wellington) |&gt;\n  st_interpolate_aw(x = wellington |&gt; select(population),\n                    extensive = TRUE)\n\nbasemap + \n  geom_sf(data = cells, aes(fill = population), colour = NA) +\n  scale_fill_distiller(\"Population\\n(per ha.)\",\n                       palette = \"Reds\", direction = 1) +\n  coord_sf(xlim = xr, ylim = yr)\n\n\n\n\n\n\n\n\nFigure 10: Population per ha. mapped as vector polygon cells.\n\n\n\n\n\nWhat’s (potentially) interesting about this idea, is that at polygon boundaries this process assigns different populations to the ‘raster’ cells. We can see this if we zoom in a bit.\n\n\nCode\nbasemap + \n  geom_sf(data = cells, aes(fill = population), colour = NA) +\n  scale_fill_distiller(\"Population\\n(per ha.)\",\n                       palette = \"Reds\", direction = 1) +\n  geom_sf(data = wellington, fill = NA) +\n  coord_sf(xlim = c(1.746e6, 1.749e6), ylim = c(5.425e6, 5.428e6))\n\n\n\n\n\n\n\n\nFigure 11: A closer look at the population per ha. map.\n\n\n\n\n\nAnd there we have it, at least for now. This is a good point to stop before the next post which will be about area → field transformations, because, after all, raster data sets are actually large numbers of small, albeit identical polygons…"
  },
  {
    "objectID": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#footnotes",
    "href": "posts/2025-10-05-gia-chapter-1B-part-3B/index.html#footnotes",
    "title": "GIS, a transformational approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr down then up…↩︎\nSays the person offering advice on the internet…↩︎\nI have not been able to install this on a Mac↩︎\nIt’s buffering, you fool.↩︎\nShall we ever see their like again?↩︎\nSeriously, some of the locked-in decisions around the New Zealand census almost make me glad to see the back of it. Almost.↩︎\nPreviously known by the more descriptive if equally anodyne name Census Area Unit.↩︎\nSee also this post.↩︎\nMany schools don’t even have an enrolment zone.↩︎\n©Stan Openshaw.↩︎"
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html",
    "title": "Synthetic urban networks",
    "section": "",
    "text": "A just published paper1 by Marc Barthelemy and Geoff Boeing proposes an interesting new model for making synthetic urban street networks. Setting aside the slightly breathless language (‘universal’ model? really?!2) the model had me scurrying for my computer to make my own version, as these things tend to do when they (i) are interesting, and (ii) make cool pictures.\nThat usually means first NetLogo (it’s the tool I think with), then R (it does graphics well, even if the language itself is a bit wonky). Marc and Geoff have written their model in Python, which is probably what I would do too if my plan was to do lots of analysis. That’s not the goal here. I was just interesting in exploring the model a little. The question of what happens when you add ‘shortcuts’ to a two-dimensional graph in terms of the network structure is something I’ve explored to some degree or another since my PhD work when I used Duncan Watts’s small world rewiring method to make non-uniform 2D lattices. I returned to it at least in passing in Computing Geographically\nAlong the way, I changed the model a lot, probably because I’ve recently been updating the Spatial Simulation model-zoo and there are a horde of Eden growth and percolation models in that work, and those were front of mind. There’s also not a lot of point in just making the same model, right?\nAlthough I worked out some initial ideas in NetLogo, I present the code here in R. First, as usual imports.\nCode\nlibrary(igraph)\nlibrary(sfnetworks)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DescTools)\nlibrary(ggplot2)\nlibrary(cols4all)\nlibrary(patchwork)\n\nset.seed(1)\nMostly the usual suspects, with the addition of igraph and sfneworks, and DescTools, which I must have used before because it was already in my R library, but I have no idea why. The last of these provides a simple Gini function, which I don’t use, but which is important in Marc and Geoff’s analysis, along with a function for generating all pairwise combinations of elements from a vector, a function it is surprising that base R does not offer."
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#useful-functions",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#useful-functions",
    "title": "Synthetic urban networks",
    "section": "Useful functions",
    "text": "Useful functions\nNext, some functions that find repeated use in what follows.\n\nEdges of a graph as a data frame\nA function for extracting the edges of a graph as a dataframe of from and to entries, along with any additional attribute data that might be associated with the edges is nice to have.\n\n\nCode\nget_edge_df &lt;- function(G) {\n  df &lt;- G |&gt;\n    as_edgelist() |&gt;\n    as.data.frame() |&gt;\n    rename(from = V1, to = V2)\n  # if there are edge attributes also include those\n  if (length(edge_attr_names(G)) &gt; 0) {\n    df &lt;- df |&gt; bind_cols(G |&gt; edge.attributes() |&gt; as.data.frame())\n  }\n  df\n}\n\n\n\n\nAdding the lengths of edges\nAlso useful is a function to add distances (i.e., edge lengths) to an edge dataset, given a table of vertices with xand y coordinate locations. This assumes that the edges dataframe has from and to attributes that correspond to the nameattribute in the vertex dataframe.\nCoordinates are added by joining based on the from and to columns and used to find distances. igraph treats any edge attribute called weightas special for use in path length calculations and the like, so we give the length that name as a convenience.\n\n\nCode\nadd_edge_lengths &lt;- function(edge_df, vertex_df) {\n  edge_df |&gt; \n    left_join(vertex_df, by = join_by(from == name)) |&gt;\n    rename(x1 = x, y1 = y) |&gt;\n    left_join(vertex_df, by = join_by(to == name)) |&gt;\n    rename(x2 = x, y2 = y) |&gt;\n    mutate(weight = sqrt((x2 - x1) ^ 2 + (y2 - y1) ^ 2)) |&gt;\n    select(-x1, -y1, -x2, -y2)\n}\n\n\n\n\nCollapsing chains of degree-2 vertices\nOnly used once, but good to give them names and not clutter up the main workflow with their complicated workings are two functions that combine to collapse ‘chains’ of vertices with only two neighbours into single edges.\nIf we have a local formation like A—B—C3 then the goal here is to collapse that to A—C. In the paper, Marc and Geoff avoid creating vertices with two neighhours by adding two neighbours to vertices with a single neighbour. Here, because I am making the ‘background’ lattice of the network by removing edges, the problem is to to remove all such vertices. Aesthetically, as we’ll see this has the nice side-effect of introducing diagonal edges and departing a little from the strict orthogonality of the networks in the original paper.\nThis code was by far the most complicated piece of this to get working. Hover over the numbered circles for explanations of what’s going on at each step.\n\n\nCode\ncollapse_chains &lt;- function(G) {\n  chains &lt;- G |&gt; \n1    subgraph(V(G)[which(degree(G) == 2)]) |&gt;\n2    components()\n3  vs_to_delete &lt;- c()\n  es_to_add &lt;- c()\n  for (i in 1:chains$no) {\n    vs &lt;- names(chains$membership)[which(chains$membership == i)]\n    vs_to_delete &lt;- c(vs_to_delete, vs)\n4    ext_chain &lt;- neighborhood(G, order = 1, nodes = vs) |&gt;\n      unlist() |&gt; names() |&gt; unique()\n5    end_vs &lt;- ext_chain[which(!(ext_chain %in% vs))]\n6    if (length(end_vs) == 2) {\n      es_to_add &lt;- c(es_to_add, end_vs)\n    }\n  }\n  G |&gt; add_edges(es_to_add) |&gt;\n    delete_vertices(vs_to_delete) |&gt;\n    largest_component() |&gt;\n    simplify()\n}\n\n7collapse_all_chains &lt;- function(G) {\n8  while (degree_distribution(G)[3] &gt; 0) {\n    G &lt;- G |&gt; collapse_chains()\n  }\n  G\n}\n\n\n\n1\n\nUse subgraph in combination with degree to get all vertices with only two neighbors.\n\n2\n\nThe componentsfunction provides us with information on which of the subgraphs of vertices of degree 2 are connected to one another.\n\n3\n\nvs_to_delete and es_to_add are lists of the degree-2 vertices we will remove, and edges that will be added back in to reconnect the graph across gaps left by removing vertices.\n\n4\n\next_chain extends the chain of degree-2 vertices to any neighbors by one-link. The added vertices in the extended chain are the ones that may have to be reconnected.\n\n5\n\nend_vs removes the chain itself from the extended chain leaving only end points.\n\n6\n\nA loop attached to the main network at one vertex, will yield only one additional vertex when extended and this can’t be used to create a new edge, so we only add to the es_to_add list when the number of additional vertices is the usual two.\n\n7\n\nIt may require more than one pass to remove all degree-2 vertices. For example a unit square with two adjacent corners connected to the rest of the network will see the other two corner vertices removed in a first pass, potentially making those two vertices into degree-2 vertices.\n\n8\n\nItem 3 in the vector returned by degree_distribution is the number of degree-2 vertices in the graph.\n\n\n\n\n\n\nA nice plotting function\nThe plot.igraph function is difficult to control particularly when it comes to imposing equal aspect ratios and layering graphs in the same space on top of one another, so the somewhat ad hoc function below converts graphs to sfnetwork graphs which can be plotted by ggplot.\n\n\nCode\nggplot_graph &lt;- function(G, plot = NULL) {\n  G_sf &lt;- sfnetwork(nodes = G |&gt; \n                      vertex.attributes() |&gt; \n                      as.data.frame() |&gt; \n                      st_as_sf(coords = c(\"x\", \"y\")),\n                    edges = G |&gt; get_edge_df(),\n                    directed = FALSE,\n                    edges_as_lines = TRUE)\n  edges &lt;- G_sf |&gt; activate(\"edges\") |&gt; st_as_sf()\n  if (is.null(plot)) {\n    plot &lt;- ggplot()\n    if (\"betweenness\" %in% names(edges)) {\n      plot &lt;- plot + \n        geom_sf(data = edges, aes(colour = betweenness, linewidth = betweenness)) +\n        scale_color_binned_c4a_seq(palette = \"tableau.classic_red\") +\n        scale_linewidth(range = c(0.1, 3)) +\n        guides(colour = \"none\", linewidth = \"none\")\n    } else {\n      plot &lt;- plot + \n        geom_sf(data = edges)\n    }\n  } else {\n    plot &lt;- plot + \n      geom_sf(data = edges, colour = \"red\", linewidth = 2)\n  }\n  plot + theme_void()\n}"
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#and-so-to-work",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#and-so-to-work",
    "title": "Synthetic urban networks",
    "section": "And so to work",
    "text": "And so to work\nIn this section I work through the process of creating a small network, assembling a series of functions along the way, so that at the end of this notebook it’s simple to chain them all together to make a larger ‘synthetic city network’.\nFirst we need some very basic parameters\n\n\nCode\n1radius &lt;- 10\n2n_mst &lt;- 25\n3prop_retained &lt;- 0.85\n\n\n\n1\n\nCoordinates in the x and y directions will range from -radius to +radius inclusive.\n\n2\n\nThe number of nodes in min spanning tree.\n\n3\n\nThe proportion of lattice edges to retain.\n\n\n\n\n\nMake a complete lattice\nWe start with a complete lattice, optionally trimmed to a ‘circle’. Astute observers will notice that trimming the lattice to a circle assumes that the coordinates are centred on \\((0, 0)\\).\n\n\nCode\nget_lattice &lt;- function(radius, circular = TRUE) {\n  lims &lt;- -radius:radius\n  extent &lt;- length(lims)\n  size &lt;- extent * extent\n  lattice &lt;- igraph::make_lattice(c(extent, extent), dim = 2) |&gt;\n    set_vertex_attr(\"name\", value = 1:size |&gt; as.character()) |&gt;\n    set_vertex_attr(\"x\", value = rep(lims, extent)) |&gt;\n    set_vertex_attr(\"y\", value = rep(lims, each = extent))\n  if (circular) {\n    disc &lt;- sqrt(vertex_attr(lattice, \"x\") ^ 2 + \n                 vertex_attr(lattice, \"y\") ^ 2) &lt;= radius\n    lattice &lt;- lattice |&gt; \n      subgraph(vids = V(lattice)$name[disc])    \n  }\n  lattice\n}\nlattice &lt;- get_lattice(radius)\n\n\nHaving made the lattice, let’s see what it looks like.\n\n\nCode\nggplot_graph(lattice)\n\n\n\n\n\n\n\n\nFigure 1: The starting lattice.\n\n\n\n\n\n\n\nThin the lattice\nThe lattice ‘thinning’ is a two stage process. First we randomly remove some edges, then we apply the degree-2 vertex collapsing process explained in the previous section.\n\n\nCode\n# get the lattice edge and vertex dataframes\nget_thinned_lattice &lt;- function(G, proportion_retained) {\n  vs &lt;- G |&gt;\n    vertex.attributes() |&gt;\n    as.data.frame()\n  G |&gt; \n    get_edge_df() |&gt;\n    slice_sample(prop = proportion_retained) |&gt;\n    graph_from_data_frame(directed = FALSE, vertices = vs) |&gt;\n    largest_component() |&gt;\n    collapse_all_chains() |&gt; \n    get_edge_df() |&gt; \n    add_edge_lengths(vs) |&gt;\n    graph_from_data_frame(directed = FALSE, vertices = vs) |&gt;\n    largest_component()\n}\nthinned_lattice &lt;- get_thinned_lattice(lattice, prop_retained)\n\n\nAnd again, we can see what this looks like.\n\n\nCode\nplot1 &lt;- ggplot_graph(thinned_lattice)\nplot1\n\n\n\n\n\n\n\n\nFigure 2: The thinned and degree-2 vertex collapsed lattice.\n\n\n\n\n\n\n\nMinimum spanning tree\nCentral to the Barthelemy and Boeing model is a network backbone derived from a minimum spanning tree. As we’ll see, the current model doesn’t seem to need this backbone to yield skewed distributions of the edge betweenness centrality, but it’s easy enough to make a minimum spanning tree and add it to the thinned lattice to see its effect on the overall structure.\n\n\nCode\nget_mst_from_random_subset &lt;- function(G, n) {\n  vs &lt;- G |&gt; \n    vertex.attributes() |&gt; \n    as.data.frame()\n  es &lt;- G |&gt; \n    get_edge_df() |&gt;\n    add_edge_lengths(vs)\n  mst_vs &lt;- vs |&gt;\n    slice_sample(n = n)\n  mst_all_es &lt;- CombPairs(mst_vs$name) |&gt;\n    mutate(from = as.character(X1), to = as.character(X2)) |&gt;\n    select(-X1, -X2) |&gt;\n    add_edge_lengths(vs)\n  mst_all_es |&gt;\n    graph_from_data_frame(directed = FALSE, vertices = mst_vs) %&gt;%\n    mst()\n}\nmin_ST &lt;- get_mst_from_random_subset(thinned_lattice, n_mst)\n\n\nAnd we can use our nice graph plotting function to overlay this on the thinned lattice, noting that so far the two graphs are entirely separate.\n\n\nCode\nplot2 &lt;- ggplot_graph(min_ST, plot = plot1)\nplot2\n\n\n\n\n\n\n\n\nFigure 3: The thinned lattice with minimum spanning tree system of ‘shortcuts’ overlaid in red.\n\n\n\n\n\nThat long east-west edge in the minimum spanning tree won’t make a lot of difference in this example, because when we calculate centrality based on shortest paths (see the next section) it is based on edge weights and it doesn’t provide a shortcut. This is a lot more likely to happen in a small network like this one, but it’s an important thing to keep in mind when considering overall characteristics of this approach. The Barthelemy and Boeing model grows the lattice part of the network out from the nodes of the minimum spanning tree, so that the MST edges are necessarily shortcuts. Adding the MST to an existing lattice, in the absence of different ‘speeds’ along edges (imagine the MST was the freeway network or transit) is unlikely to make as much difference as the backbone does in their model."
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#viewing-the-resulting-networks",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#viewing-the-resulting-networks",
    "title": "Synthetic urban networks",
    "section": "Viewing the resulting networks",
    "text": "Viewing the resulting networks\nOK. Here’s a betweenness centrality map of the thinned lattice without any included minimum spanning tree backbone.\n\n\nCode\neb1 &lt;- thinned_lattice |&gt; edge_betweenness(directed = FALSE)\nthinned_lattice &lt;- thinned_lattice |&gt; \n  set_edge_attr(name = \"betweenness\", value = eb1)\n\n\n\n\nCode\nplot3 &lt;- ggplot_graph(thinned_lattice) +\n  ggtitle(\"Thinned lattice\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot3\n\n\n\n\n\n\n\n\nFigure 4: Thinned lattice with edge betweenness shown by line weight and colour.\n\n\n\n\n\nIt seems like this process has resulted in a network structure that already has a fairly defined backbone of higher centrality edges. I’m not making any claims here about how congruent with empirical data on real city networks this structure is: that kind of thing is very much in Marc and Geoff’s realm of expertise. Nevertheless the structure is interesting.\n\nWith added backbone\nNow we can add the minimum spanning tree backbone into the thinned lattice.\n\n\nCode\n# merge the new edges into the existing\nadd_subgraph &lt;- function(G, x) {\n  vs &lt;- G |&gt;\n    vertex.attributes() |&gt; \n    as.data.frame()\n  G |&gt; get_edge_df() |&gt;\n    bind_rows(x |&gt; get_edge_df() |&gt; add_edge_lengths(vs)) |&gt;\n    graph_from_data_frame(directed = FALSE, vertices = vs)\n}\ncombined_lattice &lt;- add_subgraph(thinned_lattice, min_ST)\neb2 &lt;- combined_lattice |&gt; edge_betweenness(directed = FALSE)\ncombined_lattice &lt;- combined_lattice |&gt; \n  set_edge_attr(name = \"betweenness\", value = eb2)\n\n\nAnd map the two networks side-by-side. For the very little that it’s worth, the distribution of betweenness centrality doesn’t appear as different as I’d have expected, going into this process. But clearly much more extensive investigation would be required before jumping to any conclusions.\n\n\nCode\nplot4 &lt;- ggplot_graph(combined_lattice) +\n  ggtitle(\"Combined with MST\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot3 | plot4\n\n\n\n\n\n\n\n\nFigure 5: Comparison of the thinned lattice and the same lattice augmented with a minimum spanning tree."
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#and-finally-a-more-substantial-network",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#and-finally-a-more-substantial-network",
    "title": "Synthetic urban networks",
    "section": "And finally… a more substantial network",
    "text": "And finally… a more substantial network\nThe small network shown above is really to illustrate the process step by step. In the code below, a wrapper function let’s us run everything at once and generate a bigger network in one go.\n\n\nCode\nmake_city &lt;- function(radius, prop, n_mst = 0, circular = TRUE) {\n  L &lt;- get_lattice(radius, circular = circular) |&gt;\n    get_thinned_lattice(prop)\n  if (n_mst &gt; 1) {\n    M &lt;- get_mst_from_random_subset(L, n_mst)\n    L &lt;- add_subgraph(L, M)\n  }\n  L |&gt; set_edge_attr(\n    name = \"betweenness\", \n    value = edge_betweenness(L, directed = FALSE)\n  )\n}\ncity &lt;- make_city(50, 0.75, 0)\nplot5 &lt;- ggplot_graph(city)\nplot5\n\n\n\n\n\n\n\n\nFigure 6: A ‘city’ of greater extent.\n\n\n\n\n\nEven if this is all just a bit of messing around with network code, the pictures are satisfyingly ‘city-like’!"
  },
  {
    "objectID": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#footnotes",
    "href": "posts/2025-10-04-barthelemy-boeing-and-co/index.html#footnotes",
    "title": "Synthetic urban networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBarthelemy M and G Boeing. 2025. Universal Model of Urban Street Networks. Physical Review Letters 135(13): 137401.↩︎\nAs Geoff notes on his blog the apparently lost indefinite article in the title, which adds to the grandiosity of the claim, is apparently because the journal doesn’t allow titles to start with an indefinite article. There’s probably something fairly deep about the psyche of phyicists to learn from that rather odd rule. Saying that, it’s not clear that the notion of ‘a’ universal model is logically coherent. But I digress…↩︎\nYes, I’m using em-dashes, and I don’t care if it makes you think I am an AI.↩︎"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html",
    "title": "Into the (LiDAR) void",
    "section": "",
    "text": "Last weekend the tables were turned on me by my elder son. Back when we lived in California, I dragged my two boys out on day hikes around the Bay Area much to their disgust (they had video games they’d rather have spent that time playing).\nBut lo and behold, a decade on and son #1 is planning on doing the :Te Araroa Trail next year sometime. Towards that end he’s doing a lot of overnight tramps and steadily working up to longer and more difficult expeditions (he’s done the :Tongariro Crossing and :Routeburn Track in the last couple of months, with another one coming up soon).\nAnyway, back to his revenge. I haven’t done any serious tramping lately, but he took me on an overnight trip to the Waiopehu Hut in the :Tararua Range.\nTo cut a long story short and answer the question in my subtitle: I’m afraid of the Tararuas! OMG they’re gnarly. In all honesty, my son overestimated my abilities and although I made it in and back out, several days later I am still feeling it!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#so-where-was-this-tramp",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#so-where-was-this-tramp",
    "title": "Into the (LiDAR) void",
    "section": "So where was this tramp?",
    "text": "So where was this tramp?\nHaving admitted that I have a healthy respect for the Tararuas, I was disappointed to find in putting together this post, that they are a LiDAR deadzone for some reason. I’d hoped I could explore the fractal nature of our tramp from the comfort of my keyboard, but to do that well I’d need more precise elevation data than are available.\nI’ve shown the LiDAR coverage available from LINZ overlaid on a LINZ Topo 50 map below. You can get the map data from here and of course there are also useful apps based on that layer. We also found the Organic Maps app which I have on my phone for everyday use, and which is based solely on OpenStreetMap data, to be very good.1\n\ntopo_map &lt;- rast(\"_data/nz-topo50.png\")\ncrs(topo_map) &lt;- st_crs(2193)$wkt\nlidar_coverage &lt;- st_read(\"_data/linz-lidar-available.gpkg\")\npath &lt;- st_read(\"_data/path.gpkg\") |&gt; st_reverse()\n\n\ntm_shape(topo_map) +\n  tm_rgb() +\n  tm_shape(lidar_coverage) +\n  tm_fill(fill = \"forestgreen\", fill_alpha = 0.5) +\n  tm_shape(path) +\n  tm_lines(col = \"black\", lwd = 3) +\n  tm_scalebar(position = tm_pos_out(\"center\", \"bottom\"))\n\n\n\n\n\n\n\n\nWithout LiDAR data exploring the fractal nature of the ups and downs of the trail is a bit of a non-starter. The reason I wanted to look at it that way is that one of the most salient features of this track was just how much ‘down’ there is mixed in with the ‘up’. It’s a little bit ridiculous how much you end up resenting the downhill stretches on the way up, when you know you’re shortly going to have to pay for them with more uphill. Not only that, in terrain this rugged, going downhill might use less energy than going uphill, but it demands just as much concentration, if not more!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#a-transect-of-our-travails",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#a-transect-of-our-travails",
    "title": "Into the (LiDAR) void",
    "section": "A transect of our travails",
    "text": "A transect of our travails\n\ndem &lt;- rast(\"_data/waiopehu-NZDEM_SoS_v1-0_14.tif\")\nhillshade &lt;- rast(\"_data/hillshade.tif\")\nwaterways &lt;- st_read(\"_data/waterways.gpkg\")\n\nAnyway, here’s another map, assembled from data variously obtained from the Otago School of Surveying NZSoSDEMv1 15m digital elevation model and OpenStreetMap. I made the hillshade layer in QGIS. The School of Surveying DEM is the best available for ‘analysis’ purposes in this area that I’m aware of. LINZ offer an 8m product, but are careful to say it’s only useful for cartographic visualization. As it turned out, that’s all I ended up doing, but never mind, the 15m product is more than adequate for making a transect of the track.\n\ntm_shape(dem) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"hcl.terrain2\")) +\n  tm_shape(waterways) +\n  tm_lines(col = \"#89ddff\",\n           lwd = \"waterway\",\n           lwd.scale = tm_scale_categorical(\n             values = c(0.5, 1), levels = c(\"stream\", \"river\"))) +\n  tm_shape(hillshade) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.greys\"),\n            col_alpha = 0.15) +\n  tm_shape(path) +\n  tm_lines(col = \"black\", lwd = 3) +\n  tm_layout(legend.show = FALSE)\n\n\n\n\n\n\n\n\nSo here’s one approach to making a transect.\nFirst we have to take the path, convert it to a series of points and extract elevation values from the DEM at those points. I’ve used just the points along the linestring rather than interpolating along its length. How that would look at different resolutions were LiDAR data available would be interesting to explore.\n\n# function to return data for plotting a transect across a DEM along a line\nget_transect &lt;- function(dem, line) {\n  pts &lt;- line |&gt; \n1    st_cast(\"POINT\") |&gt;\n2    st_coordinates() |&gt;\n    as_tibble() |&gt;\n3    st_as_sf(coords = 1:2, crs = st_crs(line))\n4  names(dem) &lt;- \"z\"\n  transect &lt;- extract(dem, pts |&gt; as(\"SpatVector\"), \n5                      method = \"bilinear\", xy = TRUE) |&gt;\n    as_tibble() |&gt;\n6    mutate(dx = replace_na(x - lag(x), 0),\n           dy = replace_na(y - lag(y), 0),\n           dz = replace_na(z - lag(z), 0),\n           dxy = sqrt(dx ^ 2 + dy ^ 2),\n7           distance = cumsum(dxy),\n           Ascent = cumsum(if_else(dz &gt; 0, dz, 0)),\n           Descent = cumsum(if_else(dz &lt; 0, dz, 0)),\n8           downhill_start = dz &gt;= 0 & lead(dz) &lt; 0,\n           downhill_end = dz &gt;= 0 & lag(dz) &lt; 0) |&gt;\n    select(-dx, -dy, -dz, -dxy)\n}\n\nxyz &lt;- dem |&gt; get_transect(path)\n\n\n1\n\nFirst convert the LINESTRING to POINTs.\n\n2\n\nExtract the point coordinates.\n\n3\n\nConvert back to an sf dataset.\n\n4\n\nMake sure the height variable in the DEM is called z.\n\n5\n\nThe terra::extract function extracts values from the DEM at the supplied points (which have to be converted to a SpatVector layer for this to work). xy = TRUE ensures that the x, y coordinates are also retained in the output.\n\n6\n\nThe lag function allows us to calculate differences between consecutive values in the data frame. The first difference will come out as a NA result (since there is no value before the first row), so we use replace_na to set that result to 0.\n\n7\n\nUse cumsum to calculate a elapsed distance from the start, and also total ascent and descent determined separately.\n\n8\n\nDetecting when downhill sections of the trail start and end.\n\n\n\n\nFor plotting purposes it’s easier if we pull the data apart into two data frames, one with the cumulative ascent and descent data tagged as such, the other with them retained, and with a constant label added. This is mostly so that we can convince ggplot to give us two items in the plot legend!\n\nup_down &lt;- xyz |&gt; \n  select(distance, Ascent, Descent) |&gt;\n  pivot_longer(cols = c(Ascent, Descent))\n\ntransect &lt;- xyz |&gt;\n  select(distance, Ascent, Descent) |&gt;\n1  mutate(height = Ascent + Descent,\n2         gain = as.factor(c(\"Gain\")))\n\n\n1\n\nBecause descent values are negative we just add Ascent and Descent to get the net climb.\n\n2\n\nThe gain variable is constant, but used here so that it can be picked up by scale_colour_manual which persuades ggplot to produce a legend item for it.\n\n\n\n\nFinally we can make a plot.\n\ng &lt;- ggplot() + \n  geom_area(\n    data = up_down, \n    aes(x = distance, y = value, group = name, fill = name),\n        colour = NA, position = \"identity\", alpha = 0.35) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Up or down\") +\n  geom_line(\n    data = transect, \n    aes(x = distance, y = height, colour = gain)) +\n  scale_colour_manual(name = \"\", values = \"black\", labels = \"Net gain\") +\n  coord_cartesian(expand = FALSE) + \n  labs(x = \"Elapsed distance, m\", y = \"Height gain from start, m\")\n\ng + theme_minimal()\n\n\n\n\n\n\n\n\nI have split the climb out like this, because as I said above, some of the more emotionally uh… challenging parts of this trip were the sections going downhill when our overall course was up, and I wanted to see just how much of that there was.\nIt looks like a pretty relentless uphill tramp. But there really are quite a few sections of downhill. To emphasise this we can add to the plot, using the downhill start and end flags in the data.\n\ndownhill_starts &lt;- xyz |&gt;\n  filter(downhill_start) |&gt;\n  pull(distance)\n\ndownhill_ends &lt;- xyz |&gt;\n  filter(downhill_end) |&gt;\n  pull(distance)\n\ndownhill_sections &lt;- tibble(\n  dmin = downhill_starts,\n  dmax = downhill_ends)\n\ng + geom_rect(data = downhill_sections, \n              aes(xmin = dmin, xmax = dmax, \n                  ymin = min(xyz$Descent), \n                  ymax = max(xyz$Ascent)),\n              fill = \"#00000030\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnd now we can see that while many of those ‘downhill’ sections on the way up are really fairly level, there are a lot of them!\nAll in all, I’m not sure I’d recommend it as a first long hike back after a (too) long break. Maybe I’ll go back next year and see if I’ve gotten any fitter.\nMeanwhile, here’s a photograph looking further to the east from the hut when we finally got there. Lighting conditions were a bit challenging for my phone’s camera, but all in all… very much worth it, even if I won’t be hurrying back right away!"
  },
  {
    "objectID": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#footnotes",
    "href": "posts/2025-01-28-waiopehu/waiopehu-tramp.html#footnotes",
    "title": "Into the (LiDAR) void",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut take my advice: don’t take my advice on anything to do with tramping.↩︎"
  },
  {
    "objectID": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "href": "posts/2020-07-18-what-the-chord/what-3-chords.html",
    "title": "What three chords",
    "section": "",
    "text": "I don’t know if this needs or deserves any further explanation, but SERIOUSLY and I can’t emphasise this enough, turn down the volume on your device before clicking the button.\nWhere am I?"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html",
    "title": "Random points on the globe revisited",
    "section": "",
    "text": "NOTE: There were a few over-simplifications in the original version of this post, which I’ve done my best to correct. Turns out point patterns on the globe are even more complicated than I realised…\nThis holiday season I had reason to revisit a post from November 2021 about generating evenly distributed random points on the globe. This was in the course of writing (for fun) a NetLogo simulation of the boardgame Lacuna. You can find the simulation here. The simulation isn’t very developed and it turns out that the web version of NetLogo doesn’t implement some functions important to its operation, so pending developing my Javascript skills further that’s likely where it will remain.\nAnyway, the setup rules for the physical version of Lacuna stipulate that the players should\nWriting the code for this immediately had me reaching for a sequential spatial inhibition process using spatstat in R, or for that matter an implementation of it in NetLogo1 This also reminded me that spatial inhibition processes require an inhibition distance to be specified, and got me wondering if there is a spatial point process that generates randomly distributed points ‘without clumping’ but with no need to specify a spacing parameter.\nThis is what is known as a rabbit hole.\nFor whatever reason, there doesn’t seem to be a widely used spatial point process model with this property, but there are :low discrepancy sequences most often used for balanced sampling in operations like Monte Carlo simulation that can generate evenly spaced spatial patterns in two dimensions. In this post, I look at a couple of these along with some other alternatives, in the context set by my earlier post of generating random point patterns of even intensity on the globe."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#some-preliminaries",
    "title": "Random points on the globe revisited",
    "section": "Some preliminaries",
    "text": "Some preliminaries\nTo make things easier, I am going to generate patterns in latitude-longitude space, and then transform them to patterns on the globe. Before we get started here are all the libraries I’m using.\n\nlibrary(rnaturalearth)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\n1library(spbal)\n2library(spatstat)\nlibrary(ggplot2)\nlibrary(cols4all)\n\ntheme_set(theme_minimal())\ntheme_update(axis.title = element_blank())\n\n3set.seed(1)\n\nprojection &lt;- \"+proj=moll\"\naspect_ratio &lt;- 2\nR &lt;- 6371000\nn_points &lt;- 1152\n\n\n1\n\nspbal provides Halton sequences.\n\n2\n\nspatstat provides a wide range of spatial point processes.\n\n3\n\nFor replicability.\n\n\n\n\nAnd we’ll set up a globe polygon and a world map.\n\nangles &lt;- 0:719 / 720 * 2 * pi\nangles &lt;- c(angles, angles[1])\nx &lt;- R * cos(angles) * 2 * sqrt(aspect_ratio)\ny &lt;- R * sin(angles) * 2 / sqrt(aspect_ratio)\n\nglobe &lt;- c(x, y) |&gt; \n  matrix(ncol = 2) |&gt;\n  list() |&gt;\n  st_polygon() |&gt;\n  st_sfc() |&gt;\n  st_as_sf(crs = projection)\n\nworld &lt;- ne_countries() |&gt;\n  select() |&gt;\n  st_transform(projection)\n\nAnd make a quick map to make sure everything is in order.\n\n\n\n\nggplot() +\n  geom_sf(data = globe, fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0)\n\n\n\n\n\n\n\n\n\n\nFigure 1: World map in Mollweide"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#patterns-in-latitude-longitude-space",
    "title": "Random points on the globe revisited",
    "section": "Patterns in latitude-longitude space",
    "text": "Patterns in latitude-longitude space\n\nA simple random pattern\nWe can make this by drawing x and y coordinates from uniform distributions.\n1plot_pp &lt;- function(df) {\n  ggplot() +\n    geom_point(data = df, aes(x = x, y = y)) +\n    coord_equal(xlim = 180 * c (-1, 1),\n2                ylim = 90 * c(-1, 1), expand = FALSE) +\n3    theme(panel.background = element_rect(fill = NA))\n}\npattern1 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = runif(n_points) * 180 - 90,\n                   generator = \"Uniform random\")\nplot_pp(pattern1)\n\n\n1\n\nConvenient to have a single function for plotting point patterns.\n\n2\n\nexpand = FALSE stops ggplot adding a margin beyond the limits we’ve set.\n\n3\n\nIt’s good to be able to see the bounds, and theme_minimal would not normally display a frame.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simple uniform random pattern\n\n\n\n\n\nSimple random pattern with a cosine correction for latitude\nNext up the same pattern with the cosine correction from my earlier post.\n\n\n\n\npattern2 &lt;- tibble(x = runif(n_points) * 360 - 180,\n                   y = acos(runif(n_points, -1, 1)) / pi * 180 - 90,\n                   generator = \"Uniform random cosine-corrected\")\nplot_pp(pattern2)\n\n\n\n\n\n\n\n\n\n\nFigure 3: Uniform random pattern with cosine correction\n\n\n\nIn later patterns I have applied this cosine correction where the underlying process generates uniformly distributed y coordinates.\n\n\nA pattern from a spatial point process\nMy first thought here was to use a sequential spatial inhibition process. This is a spatial point process where points are generated at random locations, but rejected if they are closer than some inhibition distance to an existing point already in the pattern. This is easily done in Euclidean space using the spatstat::rSSI function. In latitude-longitude space this won’t work because distances are not calculable using the Pythagorean function on coordinates.\nInstead, I have opted for the uniform point process (just as above), but specifying a tiling (a tessellation) of the space with approximately equal-area tiles. Using the formula for the y coordinate of a cylindrical equal-area projection \\(y=sin\\phi\\), we can generate approximately equal-area rectangles in latitude-longitude space as below. I emphasise the approximation here, because it is only approximate. Rectangles in lat-lon space are not rectangles on the globe after all.\n1nx &lt;- sqrt(n_points / 2) * 2 + 1\nny &lt;- sqrt(n_points / 2) + 1\n2xg &lt;- seq(-1, 1, length.out = nx) * 180\n3yg &lt;- asin(seq(-1, 1, length.out = ny)) / pi * 180\n\npp &lt;- runifpoint(n = 1, win = tess(xgrid = xg, ygrid = yg))\npattern3 &lt;- pp |&gt;\n  as.data.frame() |&gt;\n  mutate(generator = \"Stratified point process\")\nplot_pp(pattern3) +\n  geom_point(data = expand_grid(xg, yg), aes(x = xg, y = yg), \n             colour = \"red\", shape = 3, size = 0.5)\n\n\n1\n\nThe number of gridlines we need in each direction is determined here.\n\n2\n\nx coordinates are trivially equally spaced.\n\n3\n\ny coordinates approximately equally spaced in the transformed space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Stratified random point process\n\n\n\nI’ve plotted the coordinates of the grid used to ‘thin’ the points at high latitudes for reference. The process generates only one point in each of these cells. However… the thinning is not continuous and there are a number of points very close to the poles, where of course the available area is zero!\n\n\nQuasi-random sequences\nThese are the processes that were new to me, which showed up when I started looking for R packages that could do even sampling in 2D spaces. The first package to show up was spacefillr. This implements a number of such sequences, but we’ll look here at the Halton sequence, because its workings are relatively easy to understand, and the implementation in the spbal package provides more options.\n:Halton sequences are generated using a pair of coprime numbers, i.e., two numbers with no common factors. The simplest example is 2 and 3. Each of the selected numbers specifies a sequence by repeated subdivision of the interval 0 to 1. So for 2 we get\n\\[\n\\frac{1}{2},\\frac{1}{4},\\frac{3}{4},\\frac{1}{8},\\frac{5}{8},\\frac{3}{8},\\frac{7}{8},\\ldots\n\\]\nand for 3 we get\n\\[\n\\frac{1}{3},\\frac{2}{3},\\frac{1}{9},\\frac{4}{9},\\frac{7}{9},\\frac{2}{9},\\frac{5}{9},\\ldots\n\\]\nPairing these sequences gives us coordinates of points in the unit square. Different generating numbers can be chosen (provided they are coprime) and different starting points in each sequence can be paired, to give a wide variety of deterministically generated quasi-random patterns. The spbal package provides a highly configurable interface to generate such sequences using the cppRSHalton_br function. We can see the procedures inner workings clearly by examining the first few elements in the sequence:\n\ncppRSHalton_br(10, bases = 2:3, seeds = 0)$pts\n\n        [,1]       [,2]\n [1,] 0.0000 0.00000000\n [2,] 0.5000 0.33333333\n [3,] 0.2500 0.66666667\n [4,] 0.7500 0.11111111\n [5,] 0.1250 0.44444444\n [6,] 0.6250 0.77777778\n [7,] 0.3750 0.22222222\n [8,] 0.8750 0.55555556\n [9,] 0.0625 0.88888889\n[10,] 0.5625 0.03703704\n\n\nThese appear entirely regular, and some regularity is evident in the patterns generated (see Figure 5), although it is less apparent than might be anticipated on considering the numerical values alone. The main interest in Halton sequences is their desirable evenness of distribution for sampling purposes, which is apparent in Figure 5.\npattern4 &lt;- cppRSHalton_br(n_points, \n1                           bases = c(2, 3),\n2                           seeds = c(14, 21))$pts |&gt;\n  as.data.frame() |&gt;\n  rename(x = V1, y = V2) |&gt; \n  mutate(x = x * 360 - 180, \n         y = acos(y * 2 - 1) / pi * 180 - 90,\n         generator = \"Halton\")\nplot_pp(pattern4)\n\n\n1\n\nThe coprime generating values.\n\n2\n\nThe starting positions in the sequence for each generating value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Points generated by a Halton sequence\n\n\n\nDetails concerning generation of Halton sequences and their statistical properties are provided by Faure and Lemieux.2\n\n\nA ‘home-grown’ parameter-free pattern generator\nHere I use a seemingly trivial (but not very efficient!) algorithm to generate some home-made evenly distributed points, without the need to specify any (spatial) parameter like the inhibition distance required by rSSI. I found the inspiration for this in this detailed blog post about generating :blue noise, which was in turn based on an algorithm described by Mitchell.3\nMatters are complicated by having to calculate distances in latitude-longitude space, which we do using the :Haversine formula\n\\[\nd=2 r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\varphi_2 - \\varphi_1}{2}\\right) + \\cos(\\varphi_1) \\cos(\\varphi_2)\\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\n\\]\nfor the distance between two lon-lat locations\\(\\left(\\lambda_1,\\varphi_1\\right)\\) and \\(\\left(\\lambda_2,\\varphi_2\\right)\\), although because we only need the relative distances we don’t use the \\(2r\\) scaling. The need to calculate toroidal distances described in the blogpost is obviated by calculating distances on the sphere which wraps in a similar way.\nThe simple idea of this algorithm is that each time we add a new point we generate a set of points (candidates) to choose from, and select the one with the largest minimum distance to an existing point in the pattern. Making the algorithm more efficient would involve only checking the distance to points known to be close to candidate points using some kind of spatial index or binning structure.\n\n1ll_distances &lt;- function(p1, p2) {\n  lon1 &lt;- p1[,1]; lat1 &lt;- p1[,2]\n  lon2 &lt;- p2[,1]; lat2 &lt;- p2[,2] \n  asin(sqrt(\n    sin(outer(lat1, lat2, \"-\") / 2) ^ 2 +\n    outer(cos(p1[, 2]), cos(p2[, 2]), \"*\") * \n      sin(outer(lon1, lon2, \"-\") / 2) ^ 2\n  ))\n}\n\nrescale &lt;- function(x, x1min, x2min, x1max, x2max) {\n  x2min + (x - x1min) / (x1max - x1min) * (x2max - x2min)\n}\n\nspaced_points &lt;- function(n = 50, choice_scaling = 1.5,\n                          input_bb = c(0, 0, 1, 1),\n                          output_bb = c(0, 0, 1, 1), dist_fn) {\n  points &lt;- c(runif(1, input_bb[1], input_bb[3]), \n              runif(1, input_bb[2], input_bb[4])) |&gt; \n    matrix(ncol = 2)\n  for (i in 1:(n-1)) {\n2    n_candidates &lt;- ceiling(log(i * exp(1) * choice_scaling))\n    candidates &lt;- c(runif(n_candidates, input_bb[1], input_bb[3]), \n                    runif(n_candidates, input_bb[2], input_bb[4])) |&gt; \n      matrix(ncol = 2)\n    r_max &lt;- dist_fn(candidates, points) |&gt; \n3      apply(1, min) |&gt;\n      which.max()\n    points &lt;- rbind(points, candidates[r_max, ])\n  }\n  points |&gt; \n    as.data.frame() |&gt;\n    rename(x = V1, y = V2) |&gt;\n    mutate(x = rescale(x, input_bb[1], output_bb[1], input_bb[3], output_bb[3]),\n           y = rescale(y, input_bb[2], output_bb[2], input_bb[4], output_bb[4]),\n           generator = \"Blue noise\")\n}\n\n\n1\n\nI use the outer function to do all pairwise differences and products of data in the two supplied matrices more efficiently than by nested loops.\n\n2\n\nThe choice_scaling parameter determines how rapidly the number of candidate points grows with the size of the existing data set. It should be strictly greater than 1 or no points will ever get added! Including a log factor stops the speed of the algorithm from falling too rapidly as points are added.\n\n3\n\nThe apply(1, min) operation finds the smallest distance in each row (i.e. distance to nearest neighbour in the existing set of points), and which.max() identifies the row with the largest minimum distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: A pattern generated using a ‘blue noise’ algorithm by iteratively choosing the most remote random additional point among a set of choices"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#putting-all-these-points-on-the-globe",
    "title": "Random points on the globe revisited",
    "section": "Putting all these points on the globe",
    "text": "Putting all these points on the globe\nWe now apply a transformation from the lat-lon Cartesian coordinates to the globe that is equal-area as shown in the previous post. We can combine all the points into a single data set and use facet_wrap for side-by-side comparison.\n\n\n\n\ntransform_to_globe &lt;- function(df, proj) {\n  df |&gt;\n    st_as_sf(coords = c(\"x\", \"y\"), crs = 4326) |&gt; \n    st_transform(proj)  \n}\n\nall_points &lt;- bind_rows(pattern1, pattern2, pattern3, pattern4, pattern5) |&gt;\n  transform_to_globe(projection) |&gt;\n  mutate(generator = ordered(generator, c(\"Uniform random\", \n                                          \"Uniform random cosine-corrected\",\n                                          \"Stratified point process\",\n                                          \"Halton\", \"Blue noise\")))\nggplot(globe) +\n  geom_sf(fill = \"#cceeff\", linewidth = 0) +\n  geom_sf(data = world, fill = \"grey\", linewidth = 0) +\n  geom_sf(data = all_points, size = 0.35, colour = \"red\") +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nFigure 7: Side-by-side comparison of five patterns projected onto the globe\n\n\n\nWith the obvious exception of the uniform random pattern, which exhibits definite clumping, all of these seem to do a reasonable job of producing a random arrangement of points evenly distributed over the globe.\nFrom the perspective of avoiding clumping we can try to make the comparison slightly more precise by making hexbin density plots of the points.\nxy &lt;- all_points |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  bind_cols(all_points)\n\nnx &lt;- sqrt(n_points / pi) * 2 * sqrt(aspect_ratio)\nny &lt;- nx * 2 / sqrt(3) / sqrt(aspect_ratio)\n\nggplot(xy) +\n  geom_hex(aes(x = X, y = Y, fill = as.factor(after_stat(count))), \n           bins = c(nx, ny), linewidth = 0.1, colour = \"#666666\") +\n1  scale_fill_manual(values = c4a(\"brewer.yl_gn_bu\", 11),\n                    breaks = 1:10, guide = \"legend\", name = \"#Points\") +\n  annotate(geom = \"path\", x = x, y = y, linewidth = 0.2) +\n  coord_equal() +\n  facet_wrap( ~ generator, ncol = 2) +\n  theme_void()\n\n\n1\n\nggplot2::geom_hex insists that the point counts are ‘continuous’ which is what forces me to convert at a factor, so that I can use a legend not a colour ramp here. Another instance of ggplot2’s strong preference for not classing data, see this recent post in the context of choropleth maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Side-by-side comparison of hexbin density plots of the five point patterns\n\n\n\nThe uniform random pattern clearly exhibits the most uneven distribution of points. The cosine-corrected version is better although there is still unevenness in the mid-latitudes, as we would expect because there is no interaction between points in the pattern: the presence of a previous point does not block another point showing up close by. The stratified point process and Halton patterns are better again. The latter of these is particularly surprising, given its completely deterministic generative process.\nSomewhat to my surprise my ‘homebrew’ blue noise compares well with the other four patterns for evenness of distribution. This is particularly nice given that it requires no spatial parameter to be supplied, only one that (perhaps only marginally) affects the quality of the patterns produced."
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#final-thoughts",
    "title": "Random points on the globe revisited",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt is intriguing to me that blue noise is seemingly not a standard spatial point process. Unlike sequential spatial inhibition to which it is similar it requires no spatial parameter to be tuned to get a desired result, and it cannot fail to produce the requested number of points. It is also a reasonable plausible process from a ‘mechanism’ perspective. Imagine for example retailers considering premises in which to set up shop and examining a number of different sites, then choosing the one farthest from any potential competitor. It’s at least as compelling in that respect as a model of the outcome of competition between event locations as SSI.\nSince it seems useful, I provided some ‘hooks’ in the implementation above to allow use of different distance functions and ‘windows’ for point generation. Here’s an example in a simple Euclidean space, using :Manhattan distance to determine the best new point among candidates.\n\n\n\n\nabs_diffs &lt;- function(v1, v2) {\n  outer(v1, v2, \"-\") |&gt; abs()\n}\n\nmanhattan_distances &lt;- function(p1, p2) {\n  dx &lt;- abs_diffs(p1[, 1], p2[, 1])\n  dy &lt;- abs_diffs(p1[, 2], p2[, 2])\n  dx + dy\n}\n\nspaced_points(n_points, dist_fn = manhattan_distances) |&gt;\n  ggplot() +\n    geom_point(aes(x = x, y = y)) +\n    coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 9: A blue noise pattern in Euclidean space\n\n\n\nAn interesting rabbit hole indeed!"
  },
  {
    "objectID": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "href": "posts/2025-01-13-random-points-on-globe-revisited/random-points-on-globe.html#footnotes",
    "title": "Random points on the globe revisited",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHaving said that, the circular game board eventually led me to generate the patterns in NetLogo by randomly displacing each new flower from the centre of the circle by a random distance in a random direction, and retrying until they were no closer to another flower than some set distance.↩︎\nFaure H and C Lemieux. 2009. Generalized Halton sequences in 2008: A comparative study. ACM Transactions on Modeling and Computer Simulation 19(4) 15:1-15:31.↩︎\nMitchell DP. 1991. Spectrally optimal sampling for distribution ray tracing. SIGGRAPH Computer Graphics. 25(4) 157–164.↩︎"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html",
    "title": "Uniform random points on the globe",
    "section": "",
    "text": "There isn’t as much land near the poles, so how do you make uniform randomly distributed points in lat-lng coordinate space. Here’s how!\nNeeded libraries are the usual suspects plus rnaturalearth for basemap data\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\nn &lt;- 2500\nThe key thing to realise here is that random uniform numbers in both latitude and longitude will not be evenly distributed on Earth’s surface, because the meridians converge toward the poles. We can make two datasets to show this. First a naive set of randomly located points:\npts_naive &lt;- data.frame(lon = runif(n) * 360 - 180,\n                        lat = runif(n) * 180 - 90,\n                        type = \"naive\")"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#here-comes-the-science",
    "title": "Uniform random points on the globe",
    "section": "Here comes the science…",
    "text": "Here comes the science…\nAnd now a set where inserting a cosine correction ensures that the distribution of latitudes is appropriately more dense close to the equator:\n\npts_even &lt;- data.frame(lon = runif(n) * 360 - 180,\n                       lat = acos(runif(n) * 2 - 1) * 180 / pi - 90,\n                       type = \"even\")\n\n\nCompare the latitude distributions\nWe can make up a combined data table and directly compare the distribution of the latitudes with a nice density plot. The increased representation of points in the mid-latitudes with the cosine correction is clear.\n\npts &lt;- bind_rows(pts_naive, pts_even)\nggplot(pts) +\n  geom_density(aes(y = lat, fill = type), alpha = 0.5, lwd = 0) +\n  scale_fill_viridis_d()"
  },
  {
    "objectID": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "href": "posts/2021-10-20-random-even-points-on-the-globe/random-even-points-on-the-globe.html#make-a-map",
    "title": "Uniform random points on the globe",
    "section": "Make a map",
    "text": "Make a map\nUse an equal-area projection to clearly see the problem geographically.\n\nw &lt;- ne_countries(returnclass = \"sf\") |&gt;\n  st_transform(\"+proj=hammer\")\n\npts_sf &lt;- pts |&gt;\n  st_as_sf(coords = 1:2, crs = 4326)\n\nggplot(w) + \n  geom_sf(fill = \"#cccccc\", colour = \"white\", lwd = 0.35) +\n  geom_sf(data = pts_sf, aes(colour = type), alpha = 0.35) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe naively distributed points are clearly denser at the poles than they should be, where the cosine term in the ‘even’ points generation method makes them evenly distributed over the globe."
  },
  {
    "objectID": "training/00-spatial-data-science.html",
    "href": "training/00-spatial-data-science.html",
    "title": "Introducing spatial data science",
    "section": "",
    "text": "View the slides\nThese materials aim to help GIS users (even those with limited experience) transition to a more data science-centric approach. There is a particular emphasis on open source tools and on doing geospatial analysis in code using R as both are becoming increasingly important across the industry.\nThe materials are organised by theme and short courses to tailored to any or a selection of the themes can easily be arranged.\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "training/01-spatial-analysis.html",
    "href": "training/01-spatial-analysis.html",
    "title": "Spatial analysis and modelling",
    "section": "",
    "text": "Go to the materials\nThese materials outline a one semester (36 contact hours) class in spatial analysis and modelling that was last taught at Victoria University of Wellington as GISC 422 in the second half of 2023. The materials cover many of the topics introduced in my book Geographic Information Analysis a recognised classic text in the field.\nI am still in the process of cleaning the materials up for conversion into training materials. For the time being the materials are provided gratis with no warrant as to their accuracy as a guide to spatial analysis in R but you may still find them useful all the same!\nFor more information contact me, especially if any of these or materials on similar themes is of interest to your organisation.\nNote Unfortunately, if you are not enrolled at a university or associated with a similar institution you might find links to some resources broken (e.g. DOI links to articles that are paywalled, or datasets that I do not have permission to share.)"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A similar list is available at my ORCiD profile. You can also see them in citational context at my google scholar page, which is reasonably accurate. Links below are generally to official publisher sources, and are often paywalled. Open access copies are linked from my University of Auckland profile page. If you would like a copy of something in particular and can’t access it, then get in touch.\nJump to books, papers, book chapters, dissertations"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books\nSila-Nowicka K, A Moore, D O’Sullivan, B Adams, and M Gahegan eds. 2025. 13th International Conference on Geographic Information Science (GIScience 2025). LIPIcs 346. Leibniz International Proceedings in Informatics (LIPIcs). Schloss Dagstuhl – Leibniz-Zentrum für Informatik.\nO’Sullivan D. 2024. Computing Geographically: Bridging Giscience and Geography (Guilford Press: New York).\nO’Sullivan D. 2017. Section Editor for ‘Fundamentals of GIScience’ (32 articles) in The International Encyclopedia of Geography: People, the Earth, Environment, and Technology. Richardson, D. (ed). New York: Wiley.\nMiller JA, D O’Sullivan and Wiegand N eds. 2016. Geographic Information Science: Proceedings of the 9th International Conference, GIScience 2016 Lecture Notes in Computer Science Vol. 9927 (Springer: Cham, Switzerland).\nO’Sullivan D and GLW Perry. 2013. Spatial Simulation: Exploring Pattern and Process (Wiley: Chichester, UK).\nO’Sullivan D and DJ Unwin. 2010. Geographic Information Analysis 2nd edn. (Wiley: Hoboken, NJ).\nO’Sullivan D and DJ Unwin. 2003. Geographic Information Analysis (Wiley: Hoboken, NJ)."
  },
  {
    "objectID": "publications.html#in-refereed-journals",
    "href": "publications.html#in-refereed-journals",
    "title": "Publications",
    "section": "In refereed journals",
    "text": "In refereed journals\nMahmoudi D, J Thatcher, LB Imaoka, and D O’Sullivan. Online first. From FOSS to profit: Digital spatial technologies and the mode of production. Digital Geography and Society. 100101. doi: 10.1016/j.diggeo.2024.100101.\nO’Sullivan D. 2024. Environment and Planning B and me; or what is lost in data. Environment and Planning B: Urban Analytics and City Science 51(5) 1045–1048. doi: 10.1177/23998083241246320.\nEtherington TR, D O’Sullivan, GLW Perry, DR Richards, and J Wainwright. 2024. A least-cost network neutral landscape model of human sites and routes. Landscape Ecology 39(3) 52. doi: 10.1007/s10980-024-01836-w.\nAntosz P, D Birks, B Edmonds, A Heppenstall, R Meyer, JG Polhill, D O’Sullivan and N Wijermans. 2023. What do you want theory for? A pragmatic analysis of the roles of “theory” in agent-based modelling. Environmental Modelling & Software 168 105802. doi: 10.1016/j.envsoft.2023.105802\nLester PJ, D O’Sullivan and GLW Perry. 2023. Gene drives for invasive wasp control: Extinction is unlikely, with suppression dependent on dispersal and growth rates. Ecological Applications 33(7) e2912. doi: 10.1002/eap.2912\nBergmann L, LF Chaves, D O’Sullivan and RG Wallace. 2023. Dominant Modes of Agricultural Production Helped Structure Initial COVID-19 Spread in the U.S. Midwest. ISPRS International Journal of Geo-Information 12(5) 195. doi: 10.3390/ijgi12050195\nEtherington T, F Morgan, D O’Sullivan. 2022, Binary space partitioning generates hierarchical and rectilinear neutral landscape models suitable for human-dominated landscapes. Landscape Ecology 37(7) 1761–1769. doi: 10.1007/s10980-022-01452-6\nChaves LF, MD Friberg, LA Hurtado, RM Rodríguez, D O’Sullivan and LR Bergmann. 2022. Trade, uneven development and people in motion: Used territories and the initial spread of COVID-19 in Mesoamerica and the Caribbean. Socio-Economic Planning Sciences 80 (March) 101161. doi: 10.1016/j.seps.2021.101161\nGibadullina A, LR Bergmann and D O’Sullivan. 2021. For Geographical Network Analysis. Tijdschrift voor Economische en Sociale Geografie 112(4) 482-487. doi: 10.1111/tesg.12489.\nO’Sullivan D. 2021. New mappings of GIScience and geography. A commentary on May Yuan’s ‘GIS research to address tensions in geography.’ Singapore Journal of Tropical Geography 42(1) 31–35. doi: 10.1111/sjtg.12345\nO’Sullivan D. 2021. Things are how they are because of how they got that way: Thoughts from the beach, on 50 years of Geographical Analysis. Geographical Analysis 53(1) 157–163. doi: 10.1111/gean.12225\nFranklin, RS, V Houlden, C Robinson, D Arribas-Bel, EC Delmelle, U Demšar, HJ Miller, and D O’Sullivan. 2021. Who counts? Gender, gatekeeping, and quantitative human geography. The Professional Geographer 73(1) 48–61. doi: 10.1080/00330124.2020.1828944\nO’Sullivan D, M Gahegan, DJ Exeter and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967–1000. doi: 10.1111/tgis.12660\nPayne WB and D O’Sullivan. 2020. Exploding the phone book: Spatial data arbitrage in the 1990s Internet boom. Annals of the American Association of Geographers 110(2) 391–398. doi: 10.1080/24694452.2019.1656999\nManson S, L An, KC Clarke, A Heppenstall, J Koch, B Krzyzanowski, F Morgan, D O’Sullivan, BC Runck, E Shook and L Tesfatsion. 2020. Methodological Issues of Spatial Agent-Based Models. Journal of Artificial Societies and Social Simulation 23(1) 3. doi: 10.18564/jasss.4174\nChristophers B and D O’Sullivan. 2019. Intersections of inequality in homeownership in Sweden. Housing Studies 34(6) 897-924. doi: 10.1080/02673037.2018.1495695\nMavoa S, N Bagheri, MJ Koohsari, AT Kaczynski, KE Lamb, K Oka, D O’Sullivan and K Witten. 2019. How do neighbourhood definitions influence the associations between built environment and physical activity? International Journal of Environmentalal Research and Public Health 16. doi: 10.3390/ijerph16091501\nO’Sullivan, D 2019. Untangling knots: Thoughts on Wilson’s New Lines. Transactions in GIS 32(1) 168-169. doi: 10.1111/tgis.12502\nLiu C, D O’Sullivan and GLW Perry. 2018. The rent gap revisited: gentrification in Point Chevalier, Auckland. Urban Geography 39(9) 1300-1325. doi: 10.1080/02723638.2018.1446883\nPerry GLW and D O’Sullivan. 2018. Identifying narrative descriptions in agent-based models representing past human-environment interactions. Journal of Archaeological Method and Theory, 25(3) 795-813. doi: 10.1007/s10816-017-9355-x\nMavoa S, K Lamb, D O’Sullivan, K Witten and M. Smith. 2018. Are disadvantaged children more likely to be excluded from analysis when applying global positioning systems inclusion criteria? BMC Research Notes, 11 578. doi: 10.1186/s13104-018-3681-2\nMahdavi Ardestani B, D O’Sullivan, and P Davis. 2018. A multi-scaled agent-based model of residential segregation applied to a real metropolitan area. Computers, Environment and Urban Systems, 69 1-16. doi: 10.1016/j.compenvurbsys.2017.11.002\nBergmann LR and D O’Sullivan. 2018. Reimagining GIScience for relational spaces. The Canadian Geographer / Le Géographe canadien. 62(1) 7-14. doi: 10.1111/cag.12405\nGetz WM, CR Marshall, CJ Carlson, L Giuggioli, SJ Ryan, SS Romañach, C Boettiger, SD Chamberlain, L Larsen, P D’Odorico, D O’Sullivan. 2018. Making ecological models adequate. Ecology Letters 21(2) 153-166. doi: 10.1111/ele.12893\nO’Sullivan D, LR Bergmann, and JE Thatcher. 2018. Spatiality, maps, and mathematics in critical human geography: toward a repetition with difference. The Professional Geographer 70(1) 129-139. doi: 10.1080/00330124.2017.1326081\nHarris R, D O’Sullivan, M Gahegan, M Charlton, L Comber, P Longley, C Brunsdon, N Malleson, A Heppenstall, A Singleton, D Arribas-Bel, and A Evans. 2017. More bark than bytes? Reflections on 21+ years of geocomputation. Environment and Planning B: Urban Analytics and City Science 44(4) 598-617. doi: 10.1177/2399808317710132.\nLiu C and O’Sullivan, D. 2016. An abstract model of gentrification as a spatially contagious succession process. Computers, Environment and Urban Systems 59 1-10. doi: 10.1016/j.compenvurbsys.2016.04.004\nThatcher JE, D O’Sullivan and D Mahmoudi. 2016. Data colonialism through accumulation by dispossession: new metaphors for everyday data. Environment and Planning D: Society and Space 34(6) 990-1006. doi: 10.1177/0263775816633195\nCheung AK-L, G Brierley, and D O’Sullivan. 2016. Landscape structure and dynamics on the Qinghai-Tibetan Plateau. Ecological Modelling 339 7-22. doi: 10.1016/j.ecolmodel.2016.07.015\nThatcher JE, LR Bergmann, B Ricker, R Rose-Redwood, D O’Sullivan, TJ Barnes, LR Barnes­moore, L Beltz Imaoka, R Burns, J Cinnamon, CM Dalton, C Davis, S Dunn, F Harvey, J-K Jung, E Kersten, L Knigge, N Lally, W Lin, D Mahmoudi, M Martin, W Payne, A Sheikh, T Shelton, E Sheppard, CW Strother, A Tarr, MW Wilson, and JC Young. 2016. Revisiting critical GIS. Environment and Planning A 48(5) 815-824. doi: 10.1177/0308518X15622208\nO’Sullivan D, T Evans, SM Manson, S Metcalf, A Ligmann-Zielinska, and C Bone. 2016. Strategic directions for agent-based modeling: avoiding the YAAWN syndrome. Journal of Land Use Science 11(2) 177-187. doi: 10.1080/1747423X.2015.1030463\nO’Sullivan D and SM Manson. 2015. Do physicists have geography envy? And what can geographers learn from it? Annals of the Association of American Geographers 105(4) 704–722. doi: 10.1080/00045608.2015.1039105\nCheung AK-L, D O’Sullivan and G Brierley. 2015. Graph-assisted landscape monitoring. International Journal of Geographical Information Science 29(4) 580-605. doi: 10.1080/13658816.2014.989856\nEtherington TR, EP Holland, and D O’Sullivan. 2015. NLMpy: a python software package for the creation of neutral landscape models within a general numerical framework. Methods in Ecology and Evolution 6(2) 164-168. doi: 10.1111/2041-210X.12308\nHong S-Y, D O’Sullivan and Y Sadahiro. 2014. Implementing Spatial Segregation Measures in R. PLoS ONE 9(11): e113767. doi: 10.1371/journal.pone.0113767\nO’Sullivan D. 2014. Don’t panic! The need for change and for curricular pluralism. Dialogues in Human Geography 4(1) 39-44. doi: 10.1177/2043820614525712\nMillington JDA, D O’Sullivan and GLW Perry. 2012. Model histories: Narrative explanation in generative simulation modelling. Geoforum 43(6) 1025-1034. doi: 10.1016/j.geoforum.2012.06.017\nMueller S, DJ Exeter, H Petousis-Harris, N Turner, D O’Sullivan and CD Buck. 2012. Measuring disparities in immunisation coverage among children in New Zealand. Health and Place 18(6) 1217-1223. doi: 10.1016/j.healthplace.2012.08.003\nHong S-Y and D O’Sullivan. 2012. Detecting ethnic residential clusters using an optimisation clustering method. International Journal of Geographical Information Science 26(8) 1257-1277. doi: 10.1080/13658816.2011.637045\nXue J, W Friesen and D O’Sullivan. 2012. Diversity in Chinese Auckland: Hypothesising multiple ethnoburbs. Population, Space and Place 18 579-595. doi: 10.1002/psp.688\nMavoa S, K Witten, T McCreanor, and D O’Sullivan. 2012. GIS based destination accessibility via public transit and walking in Auckland, New Zealand. Journal of Transport Geography 20(1) 15-22. doi: 10.1016/j.jtrangeo.2011.10.001\nMateos P, PA Longley, and D O’Sullivan. 2011. Ethnicity and Population Structure in Personal Naming Networks. PLoS ONE 6(9) e22943. doi: 10.1371/journal.pone.0022943\nPearson J, R Lay-Yee, P Davis, D O’Sullivan, M von Randow, N Kerse and S Pradhan. 2011. Primary care in an aging society: Building and testing a microsimulation model for policy purposes. Social Science Computer Review 29(1) 21-36. doi: 10.1177/0894439310370087\nO’Sullivan D. 2009. What’s critical about critical GIS? Cartographica 44(1) 7-8. doi: 10.3138/carto.44.1.5\nO’Sullivan D and G. L. W. Perry. 2009. A discrete space model for continuous space dispersal processes. Ecological Informatics 4(2) 57-68. doi: 10.1016/j.ecoinf.2009.03.001\nO’Sullivan D. 2009. Changing neighborhoods – neighborhoods changing: a framework for spatially explicit agent-based models of social systems. Sociological Methods and Research 37(4) 498-530. doi: 10.1177/0049124109334793\nReardon SF, CR Farrell, SA Matthews, D O’Sullivan, K Bischoff and G Firebaugh. 2009. Race and space in the 1990s: changes in the geographic scale of racial residential segregation, 1990-2000. Social Science Research 38 55-70. doi: 10.1016/j.ssresearch.2008.10.002\nLee BA, SF Reardon, G Firebaugh, CR Farrell, SA Matthews and D O’Sullivan. 2008. Beyond the census tract: patterns and determinants of racial segregation at multiple geographic scales. American Sociological Review 73(October) 766-791. doi: 10.1177/000312240807300504\nReardon SF, SA Matthews, D O’Sullivan, BA Lee, G Firebaugh, CR Farrell and K Bischoff. 2008. The geographic scale of metropolitan segregation. Demography, 45(3) 489-514. doi: 10.1353/dem.0.0019\nO’Sullivan D. 2008. Geographical information science: agent-based models. Progress in Human Geography 32(2) 541-550. doi: 10.1177/0309132507086879\nO’Sullivan D and DWS Wong. 2007. A surface-based approach to measuring spatial segregation. Geographical Analysis 39(2) 147-168. doi: 10.1111/j.1538-4632.2007.00699.x\nO’Sullivan D. 2006. Geographical information science: critical GIS. Progress in Human Geography 30(6) 783-791. doi: 10.1177/0309132506071528\nRygel L, D O’Sullivan and B Yarnal. 2006. A method for constructing a social vulnerability index: an application to hurricane storm surges in a developed country. Mitigation and Adaptation Strategies for Global Change 11(3) 741-764. doi: 10.1007/s11027-006-0265-6\nManson SM and D O’Sullivan. 2006. Complexity theory in the study of space and place. Environment and Planning A 38(4) 677-692. doi: 10.1068/a37100\nO’Sullivan D, JP Messina, SM Manson and TW Crawford. 2006. Space, place, and complexity science. Environment and Planning A 38(4) 611-617. doi: 10.1068/a3812\nO’Sullivan D. 2005. Geographical information science: time changes everything. Progress in Human Geography 29(6) 749-756. doi: 10.1191/0309132505ph581pr\nCrawford TW, JP Messina, SM Manson and D O’Sullivan. 2005. Complexity science, complex systems, and land-use research. Environment and Planning B: Planning & Design 32(5) 792-798. doi: 10.1068/b3206ed\nReardon SF and D O’Sullivan. 2004. Measures of Spatial Segregation. Sociological Methodology 34(1) 121-162. doi: 10.1111/j.0081-1750.2004.00150.x\nO’Sullivan D. 2004. Complexity science and human geography. Transactions of the Institute of British Geographers 29(3) 282-295. doi: 10.1111/j.0020-2754.2004.00321.x\nO’Sullivan D. 2002. Toward micro-scale spatial modelling of gentrification. Journal of Geographical Systems 4(3) 251-274. doi: 10.1007/s101090200086\nO’Sullivan D. 2001. Graph-cellular automata: a generalised discrete urban and regional model. Environment and Planning B: Planning & Design 28(5) 687-705. doi: 10.1068/b2707\nHaklay M, T Schelhorn, D O’Sullivan and M Thurstain-Goodwin. 2001. “So go down town”: Simulating pedestrian movement in town centres. Environment and Planning B: Planning & Design 28(3) 343-359. doi: 10.1068/b2758t\nTorrens PM and D O’Sullivan. 2001. Cellular automata and urban simulation: where do we go from here? Environment and Planning B: Planning & Design 28(2) 163-168. doi: 10.1068/b2802ed\nO’Sullivan D and A Turner. 2001. Visibility graphs and landscape visibility analysis. International Journal of Geographical Information Science 15(3) 221-237. doi: 10.1080/13658810151072859\nTurner A, M Doxa, D O’Sullivan and A Penn. 2001. From isovists to visibility graphs: a methodology for the analysis of architectural space. Environment and Planning B: Planning & Design 28(1) 103-121. doi: 10.1068/b2684\nO’Sullivan D. 2001. Exploring spatial process dynamics using irregular cellular automaton models. Geographical Analysis 33(1) 1-18. doi: 10.1111/j.1538-4632.2001.tb00433.x\nO’Sullivan D. and M Haklay. 2000. Agent-based models and individualism: is the world agent-based? Environment and Planning A 32(8) 1409-1425. doi: 10.1068%2Fa32140\nO’Sullivan D, A Morrison and J Shearer. 2000. Using desktop GIS for the investigation of accessibility by public transport: an isochrone approach. International Journal of Geographical Information Science 14(1) 85-104. doi: 10.1080/136588100240976"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters\nBergmann LR and D O’Sullivan. 2024. Space: towards a global sense of place. In A Research Agenda for Spatial Analysis (eds LJ Wolf, R Harris and AJ Heppenstall). Elgar Research Agendas. Edward Elgar Publishing. doi: 10.4337/9781802203233.00009.\nC Andris and D O’Sullivan. 2019. Spatial Networks. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer. doi: 10.1007/978-3-642-36203-3_67-1.\nD O’Sullivan. 2018. Cartography and geographic information systems. In J Ash, R Kitchin and A Leszczynski (eds), Digital Geographies, (Los Angeles: Sage), pages 118-128.\nD O’Sullivan. 2018. Big data … why (oh why?) this computational social science? In JE Thatcher, J Eckert and A Shears (eds), Thinking Big Data in Geography: New Regimes, New Research, (Lincoln: University of Nebraska Press), pages 21-38. doi: 10.2307/j.ctt21h4z6m.7\nLR Bergmann and D O’Sullivan. 2017. Computing with many spaces: Generalizing projections for the digital geohumanities and GIScience. In Proceedings of GeoHumanities’17: 1st ACM SIGSPATIAL Workshop on Geospatial Humanities, Redondo Beach, CA, November 7-10 (GeoHumanities’17), pages 31-38. doi: 10.1145/3149858.3149866\nThatcher JE, LR Bergmann and D O’Sullivan. 2016. Searching for common ground (again). In Short Paper Proceedings of 9th International Conference on Geographical Information Science, eds JA Miller, D O’Sullivan and N Wiegand, (Montreal, Canada), pages 304-307. doi: 10.21433/B3118nq409qz\nPfeffer K, J Martinez, D O’Sullivan and D Scott. 2015. Geo-Technologies for Spatial Knowledge: Challenges for Inclusive and Sustainable Urban Development. In J Gupta, K Pfeffer, H Verrest and M Ros-Tonen (eds), Geographies of Urban Governance (Cham: Springer International Publishing), pages 147-173. doi: 10.1007/978-3-319-21272-2_8\nO’Sullivan D. 2014. Spatial Network Analysis. In Handbook of Regional Science (eds MM Fischer and P Nijkamp). Springer, pages 1253-1273. doi: 10.1007/978-3-642-23430-9_67\nO’Sullivan D, JDA Millington, GLW Perry and J Wainwright. 2012. Agent-Based Models – Because They’re Worth It? In AJ Heppenstall, AJ Crooks, LM See, and M Batty (eds), Agent-Based Models of Geographical Systems (Springer: Dordrecht, Netherlands), pages 109-123. doi: 10.1007/978-90-481-8927-4_6\nHeppenstall AJ, AJ Evans, MH Birkin, JR Macgill and D O’Sullivan. 2005. The Use of Hybrid Agent-Based Systems to Model Petrol Markets. In T Terano, H Kita, H Kaneda, K Arai and H Deguchi (eds), Agent-Based Simulation: From Modelling Methodologies to Real-World Applications, Springer Series on Agent-Based Social Systems, pages 154-162. doi: 10.1007/4-431-26925-8_17\nO’Sullivan D. 2004. Too much of the wrong kind of data: implications for the practice of micro-scale spatial modelling. In MF Goodchild and D Janelle (eds), Spatially Integrated Social Science: Examples of Best Practice (Oxford University Press: Oxford), pages 95-107.\nO’Sullivan D, JR Macgill and C Yu. 2003. Agent-based residential segregation: a hierarchically structured spatial model. In C Macal, M North and D Sallach (eds), Agent 2003: Challenges in Social Simulation, pages 493-507 (Argonne National Laboratory: Chicago). www.agent2004.anl.gov/Agent2003.pdf\nO’Sullivan D. 2002. Understanding the difference that space can make: toward a geographical agent modeling environment. In C. Macal and D. Sallach (eds), Agent 2002: Ecology, Exchange, and Evolution, pages 13-25 (Argonne National Laboratory: Chicago). www.agent2003.anl.gov/proceedings/2002.pdf\nO’Sullivan D and PM Torrens. 2001. Cellular models of urban systems. In S Bandini and T Worsch (eds), Theoretical and Practical Issues on Cellular Automata, Proceedings of the Fourth International Conference on Cellular Automata for Research and Industry (ACRI 2000), October 4–6, Karlsruhe, Germany (Springer-Verlag: London), pages 108-116. [Also available as CASA Working Paper 22 at www.casa.ucl.ac.uk/cellularmodels.pdf"
  },
  {
    "objectID": "publications.html#dissertations",
    "href": "publications.html#dissertations",
    "title": "Publications",
    "section": "Dissertations",
    "text": "Dissertations\nPhD: O’Sullivan D. 2000. Graph-based Cellular Automaton Models of Urban Spatial Processes. University College London.\nMSc: O’Sullivan D. 1997. Using GIS to create public transport travel time isochrones for the Glasgow area. University of Glasgow."
  },
  {
    "objectID": "portfolio/01-oecd-rural-services.html",
    "href": "portfolio/01-oecd-rural-services.html",
    "title": "OECD report",
    "section": "",
    "text": "Not a lot to say here, just to note that I recently assisted with reviewing and commentary on an OECD Report entitled Getting to Services in Towns and Villages on the variations across the OECD in the accessibility of various services across the settlement hierarchy.\nAs a geographer it was interesting to see this work from a more economistic perspective, and also somewhat amusing to hear that getting maps into OECD publications is an uphill battle given some ongoing uh… debate about territorial claims across the organisation!\nThat minor issue notwithstanding, it was a pleasure working with the folks at OECD. There’s a lot of really great research going on outside the academy, something it is easy to lose site of when you’ve spent so long inside it."
  },
  {
    "objectID": "portfolio/05-jev.html",
    "href": "portfolio/05-jev.html",
    "title": "JEV risk mapping",
    "section": "",
    "text": "I’m excited to be doing the mapping work in a recently funded project investigating the risks for establishment in New Zealand of Japanese Encephalitis Virus (JEV), funded by the Ministry for Primary Industries - Manatū Ahu Matua, and led by Prof Phil Lester.\nIt’s early days, and my initial work is focused on marshalling data on the distribution of bird species using the recently compiled NZ Bird Atlas. Here’s a map of local favourite kākā sitings.\n\n\n\nSitings of kākā in the NZ Bird Atlas\n\n\nIf this map seems more optimistic than others you’ve seen that’s because it is derived from raw sightings. Translating that information into more reliable ‘range’ maps is one aspect of the work we need to consider.\nThe reason where the birds are matters in this project is that establishment of JEV in New Zealand is most likely to occur as a result of it circulating in some bird populations (likely not kākā) from which mosquitoes might then pass it on to other populations."
  },
  {
    "objectID": "portfolio/00-2-30-day-maps.html",
    "href": "portfolio/00-2-30-day-maps.html",
    "title": "The 30 day map challenge",
    "section": "",
    "text": "Finding myself unaccountably with time on my hands in November 2023, I thought I’d give the 30 Day Map Challenge a go. I am not sure I’d recommend the experience. I mean, it’s a kind of warped fun, but it’s hard. I might have restricted my focus too much for it to stay interesting for a whole month, perhaps. Anyway, you can see what I made here and a short slideshow reflecting on the experience here.\nHere are my two favourite maps from the month:\n\n\n\n#5 A hillshaded map made by crumpling paper. Yes… it’s a real place.\n\n\n\n\n\n#27 Dots within dots to show all seven components and the overall value of an index of deprivation."
  },
  {
    "objectID": "portfolio/00-1-nz-commute-viewer.html",
    "href": "portfolio/00-1-nz-commute-viewer.html",
    "title": "Aotearoa New Zealand commutes viewer",
    "section": "",
    "text": "I somehow convinced myself in the break between two exhausting semesters in 2020 (yes, that year) to enter Stats NZ’s There and back again visualization competition.\nI learned a lot about a bunch of things along the way, but probably needed to take a step back and learn more about web development frameworks, as there is altogether too much handcrafted javascript in there. Nevertheless, I had fun and I am reasonably happy with the end result. Also… it still works over four years later, which might not be true if I’d used something smarter than vanilla JS.\nGo to the viewer\nHere’s a screenshot for anyone not curious enough to click on the link.\n\nNo, I didn’t win the competition. That was this entry by Jono Cooper."
  },
  {
    "objectID": "portfolio/00-0-spatial-covid-model.html",
    "href": "portfolio/00-0-spatial-covid-model.html",
    "title": "A spatial COVID model",
    "section": "",
    "text": "During those ahem… heady days of the first COVID-19 lockdown in Aotearoa (7 or 8 weeks in April-May 2020) I worked intensely with colleagues on a simple spatial simulation model of the pandemic in New Zealand.\nInitially we hoped that we might get involved in the ongoing modelling efforts which the government was using to manage the national response given how self-evidently geographical a thing an epidemic is. But that clearly wasn’t a perspective shared by the public health professionals and others advising the government. Either that or they were just too damn busy dealing with events to welcome the additional distraction of thinking about the geography of the disease.\nOr maybe they thought it was just too much:\n\nWhatever the reason, we never did get involved in the national response, although it was clear by the time of the outbreak in later 2021 that they were, at least by then, thinking geographically about how to manage things. Anyway, you can play with the model by clicking below.\nGo to the model\nIt’s quite slow to load, and if you aren’t smart about the settings you use it’s likely to rapidly spin out of control (just like a uh… pandemic) and slow things down dramatically. For that reason I advise only using the go-one-day or go-one-week buttons until you get a feel for things.\nWe published a paper explaining the inner workings here:\n\nO’Sullivan D, M Gahegan, DJ Exeter, and B Adams. 2020. Spatially explicit models for exploring COVID 19 lockdown strategies. Transactions in GIS 24(4) 967-1000. doi: 10.1111/tgis.12660\n\nand I presented the work at the hard-to-believe-it-even-happened face-to-face-in-person 2020 meeting of the New Zealand Geographical Society in November that year.\nAll the materials are available at this repo."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "More information on projects current and completed. You’ll also find a lot of my work on my github.\n\n\n\n\n\n\n\n\n\n\n\n\nJEV risk mapping\n\n\nThe birds and the… mozzies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScience impacts in Antarctica\n\n\nTake only pictures, leave only footprints…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-space mapping\n\n\nMore generally: folding space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent modeling of land management\n\n\nMoving the middle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOECD report\n\n\nAccess to services on the rural to urban continuum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA model for gene drive control of wasps\n\n\nSpoilers: it wouldn’t work\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiled and woven maps\n\n\nA new approach to mapping multivariate data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe 30 day map challenge\n\n\nIt’s called ‘challenge’ for a reason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAotearoa New Zealand commutes viewer\n\n\nMy take on the 2018 Census travel to work and study data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA spatial COVID model\n\n\nFiddling (with code) while the world burns\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html",
    "href": "notebooks/2025-02-18-marimo.html",
    "title": "Marimo notebooks",
    "section": "",
    "text": "In an ideal world I’d write this notebook in the tool I’m talking about and share it on my website that way. Unfortunately I haven’t been able to find an extension for Quarto (which I use to build the website) that handles marimo notebooks1, which is the tool in question.\nSo what’s a marimo notebook? The easiest way to think about it is as a reactive Python notebook. I’ve been using Jupyter notebooks since forever. I first taught with them in 2016 at Berkeley as a contribution to the then nascent major in Data Science there. Since then I’ve taught python programming using Jupyter notebooks as the platform.\nThat’s a good choice in some ways, but a bad one in others. The good is that you can introduce code in the context of working code for students to modify and get comfortable with, in a managed, predictable environment (unlike the wild west that is most student laptops). The bad reason is that Jupyter notebooks can get in a terrible tangle if you run the code cells out of sequence. Almost invariably when a student runs into problems (or for that matter I run into a problem) using a notebook, it’s because they I have been jumping back and forward running cells, forgetting that this will put the notebook into a weird state and break things.\nIt’s not unreasonable for students to expect a change anywhere in their code anywhere in a notebook to affect every other part of the notebook that uses affected variables and their values. In other words, for the notebook to ‘react’ to all changes. This enhanced interactivity is what marimo notebooks offer."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#jupyter-notebooks",
    "href": "notebooks/2025-02-18-marimo.html#jupyter-notebooks",
    "title": "Marimo notebooks",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nAnyway, if you are unfamiliar, this page is built from a Jupyter notebook. More accurately, most of it is, but I’ll get to that.\nThis being a notebook, I can mix markdown text such as this text you are reading, with code cells like this:\n\n%matplotlib inline\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nThat cell imports some python modules that I have installed locally, and now I can write python code using them, and run it interactively (locally) in a browser (or suitable coding environment like Visual Studio Code). For example, I can define a random point class RandomPoint and a function for generating a collection of points using the ‘blue noise’ method I described in a previous post.\n\nclass RandomPoint():\n\n    def __init__(self):\n        self.x = random.random()\n        self.y = random.random()\n\n    def distance2(self, pt):\n        dx = min(self.x - pt.x, 1 - self.x + pt.x)\n        dy = min(self.y - pt.y, 1 - self.y + pt.y)\n        return dx**2 + dy**2\n        \ndef spaced_points(n=100):\n    pts = [RandomPoint()]\n    for i in range(1, n):\n        n_candidates = math.ceil(math.log(1.1 * i * math.e))\n        p_new = [RandomPoint() for _ in range(n_candidates)]\n        d_mins = [min([p.distance2(p0) for p0 in pts]) for p in p_new]\n        i_max = max(enumerate(d_mins), key=lambda x: x[1])[0]\n        pts.append(p_new[i_max])\n    return pd.DataFrame(data = {'x': [p.x for p in pts], \n                                'y': [p.y for p in pts]})\n\nAnd I can make a point pattern and plot it.\n\npp = spaced_points(1000)\nscatter = plt.scatter(x=pp.x, y=pp.y)\nscatter.axes.set_aspect(1)\nscatter.axes.set_xlim(0, 1)\nscatter.axes.set_ylim(0, 1)\nplt.show()\n\n\n\n\n\n\n\n\nWhat you are seeing here is the output of my code parsed to produce a static web page in HTML. You can also parse notebooks to make PDFs, Word documents, and so on."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#interactive-jupyter",
    "href": "notebooks/2025-02-18-marimo.html#interactive-jupyter",
    "title": "Marimo notebooks",
    "section": "Interactive Jupyter",
    "text": "Interactive Jupyter\nThe experience of using Jupyter notebooks is highly interactive for the author of the notebook. That quickly leads to wanting to put together notebooks that are interactive for the end user. That’s a little bit more complicated to code, but not by much, using the ipywidgets module.\nIf I wrap the plotting code above in a function that takes as inputs a point pattern pp and the number of points n to plot, then using components from the ipywidgets module, I can control how many points in the pattern are displayed, so that I can see the progression of the generating function as points are added.\n\nimport ipywidgets as widgets\nfrom ipywidgets import interactive, fixed\n\ndef plot_spaced_points(pp:pd.DataFrame, n:int):\n  scatter = plt.scatter(x=pp.x[:n], y=pp.y[:n])\n  scatter.axes.set_aspect(1)\n  scatter.axes.set_xlim(0, 1)\n  scatter.axes.set_ylim(0, 1)\n  plt.show()\n  return None\n\npp = spaced_points(1000)\nnum_pts = widgets.IntSlider(value=100, min=10, max=pp.shape[0], step=5)\ninteractive(plot_spaced_points, pp=fixed(pp), n=num_pts)\n\n\n\n\nAnd it works. For me. Locally. Here’s a video to prove it!\n\nWhat’s much more difficult to arrange is hosting this interactive notebook in an online environment so that end users can interact with it like I can.2 For that I need to set up some kind of server infrastructure, or use an environment such as Google Colab, or CoCalc, Azure Notebooks, or SaturnCloud.\nSetting up your own infrastructure is technically demanding involving scary sounding tools like Kubernetes. It’s by no means impossible, but you need good people to set things up, and security can be a concern.\nThe hosted environments can be a useful alternative, but they can also be limited by low power cores, or limited storage (on free tiers), or can get quite pricey quickly if you need more oomph, or want multiple users at a time (this is a common problem getting Jupyter notebooks into the lab in New Zealand universities)."
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#enter-marimo",
    "href": "notebooks/2025-02-18-marimo.html#enter-marimo",
    "title": "Marimo notebooks",
    "section": "Enter marimo",
    "text": "Enter marimo\nThis is where marimo comes in. Below is a marimo notebook embedded in a HTML iframe.3\nYou can drag the slider to see the number of points in the plot change. You can even edit the code blocks to see the effect of changes to the code! If you make it crash, you’ll have to relaunch the Restart kernel option in the controls dropdown menu in the upper right.\n\n\nPutting it in an iframe like this isn’t ideal. If you’d just like to visit the page, then go to southosullivan.com/misc/meet-marimo/\nThe magic here is :WebAssembly (WASM) which my python ‘source code’ has been compiled to via the Pyodide project. Because the resulting HTML page is in WebAssembly it runs in the browser, not on a python kernel on a remote server, so it is effectively a static web page and I can even serve it on github pages. Here’s a more ambitious example from my work on tiled maps of multivariate data. Be warned it takes quite a while to download (about a minute to get up and running) because it has to install a bunch of relatively chunky python modules before it will work.\nAs a way to build relatively lightweight dashboards or nice interactive web-app ‘explainers’ without complicated backend hosting, this seems to hold a lot of potential!"
  },
  {
    "objectID": "notebooks/2025-02-18-marimo.html#footnotes",
    "href": "notebooks/2025-02-18-marimo.html#footnotes",
    "title": "Marimo notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYet… and it may never happen, given the investment of Posit in Shiny↩︎\nNote from future self: actually not that difficult. See this post.↩︎\nOne wrinkle here is that I have to host this notebook on another server because I can’t show a local HTML file in an iframe because of a disallowed MIME type (“text/html”) according to my browser console.↩︎"
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html",
    "href": "posts/2025-11-26-city-to-sea/index.html",
    "title": "30 Day Map Challenge 2025",
    "section": "",
    "text": "Among the better things about living in Wellington, a city with its fair share of challenges, is the proximity of green space almost everywhere. And threaded through all that green space are numerous paths and biking trails. I got to know a lot of paths in my neck of the woods, around Brooklyn, during the 2020 Covid lockdown when I’d wander far and wide for the allowed one hour of outdoors exercise each day. Since then I’ve walked pretty much all sections of the City to Sea Walkway at some point but never all in one go.\nWith an impending Great Walk coming up in early December, I’ve been getting tramping fit by doing a bit more walking, and the time had come one sunny Saturday a couple of weeks ago to finally do the whole of the City to Sea in one go.\nAnd of course, after finishing it I was keen to map it, specifically to assemble an elevation profile of the route.\nCode\nlibrary(dplyr)\nlibrary(terra)\nlibrary(sf)\nlibrary(units)\nlibrary(igraph)\nlibrary(ggplot2)\nlibrary(cols4all)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#when-a-route-is-not-a-linestring",
    "href": "posts/2025-11-26-city-to-sea/index.html#when-a-route-is-not-a-linestring",
    "title": "30 Day Map Challenge 2025",
    "section": "When a route is not a linestring",
    "text": "When a route is not a linestring\nTurns out this was a little more complicated than I expected. If I’d been smart I’d have tracked my tramp on my phone and thus have had a single sequence of points that could be made into a single linestring which is what’s needed to make an elevation profile. I didn’t think to do that and was instead faced with the multilinestring available via OpenStreetMap (downloaded using the excellent QuickOSM plugin for QGIS).\nThat’s what’s shown in the thrown together map above. And that’s fine if all you want to do is put a line on the screen. If you want to interpolate points along a line then you need a single linestring, and it’s not always the case that multilinestrings can be trivially converted into linestrings.\nIt turns out that this particular multilinestring is well behaved and can be combined into a single linestring using st_line_merge. I’ve shown an example at the end of this post where this is not the case.\n\ncity_to_sea &lt;- st_read(\"city-to-sea-all.gpkg\") |&gt;\n  st_transform(2193) |&gt;\n  st_line_merge()"
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#making-an-elevation-profile",
    "href": "posts/2025-11-26-city-to-sea/index.html#making-an-elevation-profile",
    "title": "30 Day Map Challenge 2025",
    "section": "Making an elevation profile",
    "text": "Making an elevation profile\nNow that we have a single linestring, we also need elevation data. This is extracted from Wellington LiDAR imagery available here.\n\ndem &lt;- rast(\"c2s.tif\")\nnames(dem) &lt;- \"Elevation\"\n\nWe can make a profile by sampling at equally spaced intervals along the linestring usingst_line_interpolate and terra::extract. The walkway is about 14.5 km, so 3000 steps is roughly every 3m. That seems about right when the elevation data is at 1m resolution.\n\nsteps &lt;- 0:3000 / 3000\n\npoints &lt;- city_to_sea$geom |&gt;\n1  st_line_interpolate(steps, normalized = TRUE)\n\nprofile &lt;- terra::extract(dem, points |&gt; as(\"SpatVector\")) |&gt;\n  mutate(Distance = (city_to_sea |&gt; st_length()) * steps) |&gt;\n2  mutate(Distance = drop_units(Distance))\n\n\n1\n\nnormalized = TRUE means that 0 is the start of the line and 1 is the end of it.\n\n2\n\nIt’s easier to work with distances presented as plain numbers not units.\n\n\n\n\nAnd now we can make a plot of elevation by distance along the walk. It’s instructive here to show the elevation and distance at the same scale, and also with the elevation exaggerated eight-fold. The at-scale version is that barely visible horizontal sliver. Clearly, in some sense, this walk is not as hilly as it feels!\n\n\nCode\ng_profile_1 &lt;- ggplot() +\n  geom_area(data = profile, aes(x = Distance, y = Elevation), fill = \"grey\") +\n  coord_fixed() +\n  theme_minimal()\n\ng_profile &lt;- ggplot() +\n  geom_area(data = profile, aes(x = Distance, y = Elevation), fill = \"grey\") +\n  coord_fixed(ratio = 8) +\n  theme_minimal()\n\n(g_profile_1 / g_profile & labs(x = NULL, y = NULL)) + \n  labs(x = \"Distance\", y = \"Elevation\")\n\n\n\n\n\n\n\n\nFigure 2: Elevation profile of the City to Sea Walkway shown at scale, and with elevation exaggerated eight-fold."
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#adding-some-names-to-the-walkway-segments",
    "href": "posts/2025-11-26-city-to-sea/index.html#adding-some-names-to-the-walkway-segments",
    "title": "30 Day Map Challenge 2025",
    "section": "Adding some names to the walkway segments",
    "text": "Adding some names to the walkway segments\nNamed City to Sea segments are also available from OpenStreetMap. We can use these to add some contextual information to our elevation profile.\n\nnamed_segments &lt;- st_read(\"city-to-sea-segments.gpkg\") |&gt;\n  st_transform(2193) |&gt;\n  select(name) |&gt;\n1  filter(!is.na(name) & name != \"City to Sea Walkway\") |&gt;\n2  group_by(name) |&gt;\n  filter(row_number() == 1)\n\n\n1\n\nAny name attribute that is ‘City to Sea Walkway’ is not very useful in this context, so we drop those.\n\n2\n\nThe last two steps remove duplicate names retaining only the first instance of any name that appears more than once.\n\n\n\n\nNext we convert the line segments to points half way along the segment length and add to each point its distance along the city_to_sea linestring.\n\nnamed_points &lt;- named_segments |&gt;\n  mutate(geom = st_line_interpolate(geom, 0.5, normalized = TRUE),\n         Distance = st_line_project(city_to_sea$geom, geom))\n\nAnd now, as before we find the elevation of each of these points. I also make a simple dataframe version of the data so we can plot the data based on the distance and elevation rather than the planimetric map coordinates.\n\nnamed_points &lt;- bind_cols(\n  named_points, \n  named_points |&gt;\n    as(\"SpatVector\") |&gt;\n    terra::extract(x = dem, method = \"bilinear\", ID = FALSE))\n\nname_xys &lt;- named_points |&gt; \n  st_coordinates() |&gt;\n  data.frame() |&gt; \n  rename(x = X, y = Y) |&gt;\n  bind_cols(named_points |&gt; st_drop_geometry())\n\nAnd now we can make a labelled version of the elevation profile.\n\n\nCode\ng_profile_labelled &lt;- g_profile + \n  geom_text(\n    data = name_xys, \n    aes(x = Distance, y = Elevation, label = name),\n    angle = 90, size = 2.5, hjust = 0, vjust = 0.5, nudge_y = 5, \n    check_overlap = TRUE) +\n  scale_y_continuous(breaks = seq(0, 150, 50)) +\n  coord_fixed(ratio = 8, ylim = c(0, 350)) +\n  theme(axis.title.y = element_text(hjust = 0.15))\ng_profile_labelled\n\n\n\n\n\n\n\n\nFigure 3: The profile with labelled locations based on names of segments."
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#making-an-actual-map",
    "href": "posts/2025-11-26-city-to-sea/index.html#making-an-actual-map",
    "title": "30 Day Map Challenge 2025",
    "section": "Making an actual map",
    "text": "Making an actual map\nI could argue that this elevation profile is a map, but my heart wouldn’t really be in it. Instead, it’s fun to make a parallel map of the path, and roughly line the two up next to one another. Because the elevation profile is ‘landscape’ mode, I want the map to also be oriented with its long axis horizontal. Given the very strong north-south orientation of the walkway, it’s convenient to do this by a coordinate swap, so here is a function for that.\n\nswitch_xy &lt;- function(df) {\n  df |&gt;\n1    mutate(z = -y, y = x, x = z) |&gt;\n2    select(-z)\n}\n\n\n1\n\nTo swap the x and y coordinates we need a temporary third attribute, which I’ve imaginatively called z.\n\n2\n\nAnd since it’s temporary, we throw it away.\n\n\n\n\nI explored doing this with an actual map projection or by faking a projection using matrix multiplication1, but in the end decided it was easier just to convert my data to plain dataframes with x and y attributes and alter those as needed.\nThe 1m resolution DEM is more detail than we really need, so I aggregate it and make a hillshade, which, because I’m using ggplot for mapping I also convert into a simple dataframe, and switch the coordinates.\n\ndem_x5 &lt;- dem |&gt; aggregate(5)\nslope &lt;- dem_x5 |&gt; terrain(unit = \"radians\")\naspect &lt;- dem_x5 |&gt; terrain(v = \"aspect\", unit = \"radians\")\nhillshade &lt;- shade(slope, aspect, direction = 225) \nhillshade_yx &lt;- hillshade |&gt;\n  as.data.frame(xy = TRUE) |&gt;\n  switch_xy()\n\nWe also need the route and named points as dataframes, with coordinates swapped.\n\nname_yxs &lt;- name_xys |&gt;\n  switch_xy()\n\ncity_to_sea_yx &lt;- city_to_sea |&gt; \n  st_coordinates() |&gt; \n  data.frame() |&gt;\n  select(X, Y) |&gt; \n  rename(x = X, y = Y) |&gt;\n  switch_xy()\n\nxmin &lt;- min(hillshade_yx$x)\nymax &lt;- max(hillshade_yx$y)\n\nAnd finally we can make the map with the elevation profile underneath. There’s a bit of fiddle here with positioning of annotations, and to make a north-arrow, something I am usually reluctant to provide, but which in this case, given the wilfull reorientation by 90° seems warranted.\n\ng_map &lt;- ggplot() +\n  geom_raster(\n    data = hillshade_yx,\n    aes(x = x, y = y, fill = hillshade), alpha = 0.5) +\n  scale_fill_continuous_c4a_seq(palette = \"brewer.greens\") +\n  guides(fill = \"none\") +\n  geom_path(data = city_to_sea_yx, aes(x = x, y = y),\n            linetype = \"dashed\") +\n  geom_text(\n    data = name_yxs, aes(x = x, y = y, label = name),\n    angle = 90, size = 2.5, hjust = c(rep(0:1, 15), 0), vjust = 0.5, \n    nudge_y = c(rep(c(25, -25), 15), 25), check_overlap = TRUE) +\n1  annotate(\"segment\",\n    x = xmin + 250, y = ymax + 100,\n    xend = xmin + 150, yend = ymax + 100,\n    arrow = arrow(type = \"closed\",\n                  length = unit(0.1, \"npc\"), angle = 20)) +\n2  annotate(\"text\",\n    x = xmin + 80, y = ymax + 100, label = \"N\", size = 5) +\n  coord_equal(ylim = range(hillshade_yx$y) + c(-200, 250)) +\n  ggtitle(\"Wellington's City to Sea Walkway\") +\n  theme_void()\n\n\n1\n\nA home-made north arrow.\n\n2\n\nAnd the necessary ‘N’.\n\n\n\n\nFinally put the two together with patchwork.\n\n\nCode\n(g_map / g_profile_labelled) + \n  plot_layout(heights = c(5, 4))\n\n\n\n\n\n\n\n\nFigure 4: Roughly aligned map and elevation profile of the route.\n\n\n\n\n\nThey’re not entirely aligned, but then again, a precise alignment is impossible since the path is twisty and therefore non-uniform in terms of the relationship between progress from city (in the north) to sea (in the south) versus elapsed distance along the path.\nA more annoying issue is that because the labels are placed differently in each representation the check_overlap filter in each plot has chosen a different set of labels to keep. A better approach might would likely be to hand pick the labels to include, and ensure that all of the selected labels appear in both plots. That wouldn’t be too hard to arrange, but I’m happy enough with this version and will leave it as is."
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#addendum-making-a-linestring-from-messier-data",
    "href": "posts/2025-11-26-city-to-sea/index.html#addendum-making-a-linestring-from-messier-data",
    "title": "30 Day Map Challenge 2025",
    "section": "Addendum: making a linestring from messier data",
    "text": "Addendum: making a linestring from messier data\nAbove I made the unified linestring representation of the City to Sea Walkway from an input file containing a single multilinestring. The version of the data with named segments on the walk turns out to be less cooperative than that.\n\n\nCode\nsegments_2 &lt;- st_read(\"city-to-sea-segments.gpkg\") |&gt;\n  st_transform(2193) |&gt;\n  st_cast(\"LINESTRING\")\n\n\nThis dataset doesn’t play ball when I try to combine it into a single linestring using st_line_merge:\n\nsegments_2 |&gt; \n  st_union() |&gt;\n  st_line_merge() |&gt;\n  st_cast(\"LINESTRING\")\n\nGeometry set for 5 features \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 1747444 ymin: 5421285 xmax: 1748604 ymax: 5428814\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\n\nWe don’t get a single linestring; instead we get five. The reason is that this dataset has a couple of small branch sections. We can detect these by counting how many other segments each segment touches.\n\nconnections &lt;- segments_2 |&gt;\n  st_touches(remove_self = TRUE)\n\nn_connections &lt;- connections |&gt; \n  lengths()\n\ntable(n_connections)\n\nn_connections\n  1   2   3 \n  3 137   3 \n\n\nIf the collection of linestrings formed a single linear chain, we’d expect there to be no linestrings that touch three others, and only two (one at the start, and one at the end) that touch only one other. We can map this (although you’ll have to squint to see the ‘spurs’ or ‘on-ramp’ segments, that are causing the problem—click on the map for a closer look).\n\nsegments &lt;- segments_2 |&gt;\n  st_coordinates() |&gt;\n  data.frame() |&gt; \n  left_join(data.frame(L1 = 1:143, n = as.factor(n_connections)))\n\nggplot(segments) +\n  geom_path(aes(x = -Y, y = X, group = L1, colour = n)) +\n  scale_colour_manual(values = c(\"red\", \"grey\", \"dodgerblue\")) +\n  theme_void()\n\n\n\n\n\n\n\nFigure 5: A map showing the ‘rogue’ segments\n\n\n\n\n\nTo piece together a maximal single linear route in this case we can proceed as follows. First, determine which segments in the route touch, and use those relations to build a graph.\n\nG &lt;- segments_2 |&gt; \n  st_touches(sparse = FALSE) |&gt;\n  graph_from_adjacency_matrix(mode = \"undirected\")\n\nNow, find the longest shortest path in the graph (in graph terms, its diameter). This connects the most remote ends of the graph, which we assume will give us appropriate start and end points for the whole of the City to Sea Walkway.\n\nD &lt;- distances(G)\nstart_finish &lt;- which(D == max(D), arr.ind = TRUE)[1, ]\nstart_finish\n\nrow col \n140  21 \n\n\nNow, we get the shortest path, i.e. the list of segments between those two end segments…\n\nroute &lt;- shortest_paths(\n    G, start_finish[1], start_finish[2])$vpath |&gt;\n  unlist()\n\n… and then use that list of segments to slice the named_segments dataset and assemble the route using st_line_merge.\n\ncity_to_sea_2 &lt;- segments_2 |&gt;\n  slice(route) |&gt;\n  st_union() |&gt;\n  st_line_merge()\n\nAnd here’s a very quick and dirty map to check we successfully extracted the ‘trunk’ of the walkway.\n\n\nCode\nggplot() +\n  geom_path(data = city_to_sea_2 |&gt; st_coordinates() |&gt; data.frame(),\n            aes(y = X, x = -Y)) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nFigure 6: A quick and dirty map of the main trunk route of the walkway."
  },
  {
    "objectID": "posts/2025-11-26-city-to-sea/index.html#footnotes",
    "href": "posts/2025-11-26-city-to-sea/index.html#footnotes",
    "title": "30 Day Map Challenge 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have previous on that score as regular readers if I have any will know…↩︎"
  }
]